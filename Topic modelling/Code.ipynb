{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.001*\"dopamine\" + 0.001*\"kaffine\" + 0.001*\"fat\" + 0.000*\"muscle\"')\n",
      "(1, '0.000*\"rem\" + 0.000*\"visualization\" + 0.000*\"apo\" + 0.000*\"polar\"')\n",
      "(2, '0.000*\"aggression\" + 0.000*\"aggressive\" + 0.000*\"mounting\" + 0.000*\"mating\"')\n",
      "(3, '0.000*\"headache\" + 0.000*\"cooling\" + 0.000*\"fermented\" + 0.000*\"microbiota\"')\n",
      "(4, '0.000*\"dopamine\" + 0.000*\"carbon\" + 0.000*\"psychodelic\" + 0.000*\"apo\"')\n",
      "(5, '0.000*\"nicotine\" + 0.000*\"vaping\" + 0.000*\"smoking\" + 0.000*\"adaptation\"')\n",
      "(6, '0.000*\"salt\" + 0.000*\"salty\" + 0.000*\"noise\" + 0.000*\"ear\"')\n",
      "(7, '0.000*\"sperm\" + 0.000*\"trough\" + 0.000*\"fertilization\" + 0.000*\"sandwich\"')\n",
      "(8, '0.000*\"adaptogens\" + 0.000*\"premium\" + 0.000*\"chaga\" + 0.000*\"adaptagens\"')\n",
      "(9, '0.000*\"lens\" + 0.000*\"myopia\" + 0.000*\"premium\" + 0.000*\"eyesight\"')\n",
      "(10, '0.000*\"coling\" + 0.000*\"error\" + 0.000*\"visual\" + 0.000*\"anerexia\"')\n",
      "(11, '0.000*\"carbon\" + 0.000*\"smell\" + 0.000*\"oxygen\" + 0.000*\"inhale\"')\n",
      "(12, '0.000*\"sylicibon\" + 0.000*\"sylisibon\" + 0.000*\"journey\" + 0.000*\"mushroom\"')\n",
      "(13, '0.000*\"canabas\" + 0.000*\"microbe\" + 0.000*\"indica\" + 0.000*\"fermented\"')\n",
      "(14, '0.000*\"estrogen\" + 0.000*\"stretching\" + 0.000*\"testostrone\" + 0.000*\"grief\"')\n",
      "(15, '0.000*\"iodine\" + 0.000*\"argenine\" + 0.000*\"growth\" + 0.000*\"thiroid\"')\n",
      "(16, '0.000*\"hair\" + 0.000*\"premium\" + 0.000*\"alcohol\" + 0.000*\"alcol\"')\n",
      "(17, '0.000*\"gratitude\" + 0.000*\"speech\" + 0.000*\"attachment\" + 0.000*\"premium\"')\n",
      "(18, '0.000*\"habit\" + 0.000*\"psychodelics\" + 0.000*\"happiness\" + 0.000*\"psychodelic\"')\n",
      "(19, '0.000*\"intrusive\" + 0.000*\"bond\" + 0.000*\"premium\" + 0.000*\"bonding\"')\n"
     ]
    }
   ],
   "source": [
    "# For the implementation of topic modelling by using the gensim and LDA library for deciding the major important  topics and get the details\n",
    "\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "from gensim.models import TfidfModel\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Directory containing your text files\n",
    "#directory = '/path/to/text/files'\n",
    "directory = 'C:/Users/singh/OneDrive/Desktop/NLP Project/NLPproject1/all/h14'\n",
    "\n",
    "# Read and preprocess data\n",
    "texts = []\n",
    "index_names = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename), 'r') as file:\n",
    "            content = file.read().lower()\n",
    "            # Remove punctuation and numbers\n",
    "            translator = str.maketrans('', '', string.punctuation + string.digits)\n",
    "            content = content.translate(translator)\n",
    "            # Tokenization and removing stopwords\n",
    "            stop = set(stopwords.words('english'))\n",
    "            tokens = [word for word in content.split() if word not in stop]\n",
    "            # Lemmatization\n",
    "            lemma = WordNetLemmatizer()\n",
    "            tokens = [lemma.lemmatize(word) for word in tokens]\n",
    "            texts.append(tokens)\n",
    "            index_names.append(filename.replace('.txt', ''))\n",
    "\n",
    "# Create dictionary and corpus for topic modeling\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Create TF-IDF model based on the corpus\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# Perform LDA using the TF-IDF corpus\n",
    "num_topics = 20  # Adjust the number of topics as needed\n",
    "lda = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=20, random_state=42)\n",
    "\n",
    "# Extract top words for each topic\n",
    "top_words_per_topic = []\n",
    "for t in range(num_topics):\n",
    "    top_words = lda.show_topic(t, topn=2)  # Get top 3 words for each topic\n",
    "    top_words_per_topic.append(\", \".join([word for word, _ in top_words]))\n",
    "\n",
    "# Collect and save the results\n",
    "results = []\n",
    "for i, bow in enumerate(corpus):\n",
    "    topic_probs = lda.get_document_topics(bow, minimum_probability=0)\n",
    "    # Sort topics by probability\n",
    "    sorted_topics = sorted(topic_probs, key=lambda x: x[1], reverse=True)\n",
    "    # Pick the top 3 topics\n",
    "    top_topics = sorted_topics[:3]\n",
    "    #top_topic_words = [f\"Topic {t[0]+1}: \" + top_words_per_topic[t[0]] for t in top_topics]\n",
    "    top_topic_words = [f\"Topic {t[0]+1}: \" + top_words_per_topic[t[0]] + \";\" for t in top_topics]\n",
    "\n",
    "    combined_topics = \"\\n\".join(top_topic_words)\n",
    "    results.append([index_names[i], combined_topics])\n",
    "\n",
    "# Column labels for CSV\n",
    "columns = ['Index', 'Top Topics']\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "#df.to_csv('/path/to/output/topics_distribution_combined.csv', index=False)\n",
    "df.to_csv('C:/Users/singh/OneDrive/Desktop/NLP Project/NLPproject1/all/h14/h14.csv'\n",
    ", index=False)\n",
    "# Optionally print topics\n",
    "topics = lda.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficientPS_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
