0 (0s):
Welcome to the Huberman lab podcast, where we discuss science and science-based tools for everyday life.

1 (9s):
I'm Andrew Huberman, and I'm a professor of neurobiology and ophthalmology at Stanford school of medicine. Today, I have the pleasure of introducing Dr. Lex Friedman as our guest on the Huberman lab podcast. Dr. Friedman is a researcher at MIT specializing in machine learning, artificial intelligence and human robot interactions. I must say that the conversation with Lex was without question one of the most fascinating conversations that I've ever had, not just in my career, but in my lifetime, I knew that Lex worked on these topics. And I think many of you are probably familiar with Lex and his interest in these topics from his incredible podcast, the Lex Friedman podcast. If you're not already watching that podcast, please subscribe to it.

1 (51s):
It is absolutely fantastic. But in holding this conversation with Lex, I realized something far more important. He revealed to us a bit of his dream, his dream about humans and robots about humans and machines, and about how those interactions can change the way that we perceive ourselves and that we interact with the world. We discuss relationships of all kinds, relationships with animals, relationships, with friends, relationships, with family and romantic relationships. And we discuss relationships with machines, machines that move and machines that don't move and machines that come to understand us in ways that we could never understand for ourselves and how those machines can educate us about ourselves before this conversation.

1 (1m 37s):
I had no concept of the ways in which machines could inform me or anyone about themselves by the end, I was absolutely taken with the idea and I'm still taken with the idea that interactions with machines have a very particular kind, a kind that Lex understands and wants to bring to the world can not only transform the self, but may very well transform humanity. So whether or not you're familiar with Dr. Lex Friedman or not, I'm certain you're going to learn a tremendous amount from him during the course of our discussion and that it will transform the way that you think about yourself and about the world.

0 (2m 12s):
Before we begin, I want to mention that this podcast is separate from my teaching and research roles at Stanford. It is however, part of my desire and effort to bring zero cost to consumer information about science and science related tools to the general public and keeping with that theme. I'd like to thank the sponsors of today's podcast.

ROKA (2m 30s):
Our first sponsor is Roca. Rocha makes sunglasses and eyeglasses that are of absolutely phenomenal quality. The company was founded by two All-American swimmers from Stanford and everything about the sunglasses and eyeglasses they've designed had performance in mind, I've spent a career working on the visual system. And one of the fundamental issues that your visual system has to deal with is how to adjust what you see when it gets darker or brighter in your environment with Roca, sunglasses, and eyeglasses, whether or not it's dim in the room or outside whether or not there's cloud Cover or whether or not you walk into a shadow, you can always see the world with absolute clarity. And that just tells me that they really understand the way that the visual system works processes like habituation and attenuation. All these things that work at a real mechanistic level have been built into these glasses. In addition, the glasses are very lightweight. You don't even notice really that they're on your face and the quality of the lenses is terrific. Now the glasses were also designed so that you could use them, not just while working or at dinner, et cetera, but while exercising, they don't fall off your face or slip off your face. If you're sweating. And as I mentioned, they're extremely lightweight. So you can use them while running. You can use them while cycling and so forth. Also the aesthetic of Roca glasses is terrific. Unlike a lot of performance glasses out there, which frankly make people look like cyborgs, these glasses look great. You can wear them out to dinner. You can wear them for essentially any occasion. If you'd like to try Roca glasses, you can go to roca.com. That's R O k.com and enter the code Huberman to save 20% off your first order. That's Rocha, R O k.com and enter the code Huberman at checkout

Inside Tracker (4m 3s):
today's episode is also brought to us by inside tracker. Inside tracker is a personalized nutrition platform that analyzes data from your blood and DNA to help you better understand your body and help you reach your health goals. I am a big believer in getting regular blood work done for the simple reason that many of the factors that impact our immediate and long-term health can only be assessed from a quality blood test. And now with the advent of quality DNA tests, we can also get insight into some of our genetic underpinnings of our current and long-term health. The problem with a lot of blood and DNA tests out there, however, is you get the data back and you don't know what to do with those data. You see that certain things are high or certain things are low, but you really don't know what the actionable items are, what to do with all that information with inside tracker, they make it very easy to act in the appropriate ways on the information that you get back from those blood and DNA tests. And that's through the use of their online platform. They have a really easy to use dashboard that tells you what sorts of things can bring the numbers for your metabolic factors, endocrine factors, et cetera, into the ranges that you want and need for immediate and longterm health. In fact, I know one individual just by way of example, that was feeling good, but decided to go with an inside tracker test and discovered that they had high levels of what's called C-reactive protein. They would never detected that otherwise C reactive protein is associated with a number of deleterious health conditions, some heart issues, eye issues, et cetera. And so they were able to take immediate action to try and resolve those CRP levels. And so with inside tracker, you get that sort of insight. And as I mentioned before, without a blood or DNA test, there's no way you're going to get that sort of insight until symptoms start to show up. If you'd like to try inside tracker, you can go to inside tracker.com/huberman to get 25% off any of inside trackers plans. You just use the code Huberman at checkout that's inside tracker.com/ <inaudible> to get 25% off any of inside trackers Plans.

AG1 (5m 59s):
Today's podcast is brought to us by athletic greens. Athletic greens is an all-in-one vitamin mineral probiotic drink. I started taking athletic greens way back in 2012. And so I'm delighted that they're sponsoring the podcast. The reason I started taking athletic greens and the reason I still take athletic greens is that it covers all of my vitamin mineral probiotic basis. In fact, when people ask me, what should I take? I always suggest that the first supplement people take is athletic greens for the simple reason, is that the things it contains covers your bases for metabolic health, endocrine health, and all sorts of other systems in the body. And the inclusion of probiotics are essential for a healthy gut microbiome. There are now tons of data showing that we have neurons in our gut and keeping those neurons healthy requires that they are exposed to what are called the correct microbiota little microorganisms that live in our gut and keep us healthy. And those neurons in turn help keep our brain healthy. They influence things like mood, our ability to focus, and many, many other factors related to health with athletic greens. That's terrific because it also tastes really good. I drink it once or twice a day. I mix mine with water and I add a little lemon juice, or sometimes a little bit of lime juice. If you want to try athletic greens, you can go to athletic greens.com/huberman. And if you do that, you can claim their special offer. They're giving away five free travel packs, a little packs that make it easy to mix up athletic greens while you're on the road. And they'll give you a year supply of vitamin D three and K2, again, go to athleticgreens.com/huberman to claim that special offer.

0 (7m 31s):
And now my conversation with Dr. Lex Friedman, we meet again, we meet again, thanks so much for sitting down with me. I have a question that I think is on a lot of people's minds or ought to be on a lot of people's minds because we hear these terms a lot these days, but I think most people, including most scientists and including me don't know really what is artificial intelligence and how is it different from things like machine learning and robotics. So if you would be so kind as to explain to us what is artificial intelligence and what is machine learning?

0 (8m 14s):
Well, I think that question is as complicated and as fascinating as the question of what is intelligence. So I think of artificial intelligence first as a big philosophical thing, Pamela McCormick said AI was a AI was the ancient wish to forge the gods or was born as an ancient wish to forge the gods. So I think at the big philosophical level, it's our longing to create other intelligence systems, perhaps systems more powerful than us at the more narrow level.

0 (8m 54s):
I think it's also set of

4 (8m 56s):
Tools that are computational mathematical tools to automate different tasks. And then also it's our attempt to understand our own mind. So build systems that exhibit some intelligent behavior in order to understand what is intelligence in our own selves. So all of those things are true. Of course, what AI really means as a community, as a set of researchers and engineers, it's a set of tools, a set of computational techniques that allow you to solve various problems. The there's a long history that approaches the problem from different perspectives. What's always been throughout one of the threads.

4 (9m 38s):
One of the communities goes under the flag of machine learning, which is emphasizing in the AI space, the, the task of learning. How do you make a machine that knows very little in the beginning, follow some kind of process and learns to become better and better at a particular task what's been most very effective in the recent about 15 years is a set of techniques that fall under the flag of deep learning that utilize neural networks when you will networks, are, are these fascinating things inspired by the structure of the human brain, very loosely, but they have a, it's a network of these little basic computational units called neurons artificial neurons.

4 (10m 24s):
And they have these architectures have an input and output. They know nothing in the beginning and their task would learning something interesting. What that's something interesting is usually involves a particular task. The there's a lot of ways to talk about this and break this down. Like one of them is how much human supervision is required to teach this thing. So supervised learning is broad category is the, the neural network knows nothing in the beginning. And then it's given a bunch of examples of in computer vision that will be examples of cats, dogs, cars, traffic signs, and then you're given the image and you're given the ground truth of what's in that image.

4 (11m 10s):
And when you get a large database of such image, examples where, you know, the truth, the, the neural network is able to learn by example, that's called supervised learning the question. There's a lot of fascinating questions within that, which is how do you provide the truth? When you given an image of a cat, how do you provide to the computer that this image contains a cat? Do you just say the entire image is a picture of a cat? Do you do what's very commonly been done, which is a bounding box. You have a very crude box around the cat's face saying, this is a cat. Do you do semantic segmentation, mind you, this is a 2d image of a cat.

4 (11m 51s):
So it's not the computer knows nothing about our three-dimensional world is just looking at a set of pixels. So a semantic segmentation is drawing a nice, very crisp outline around the cat and saying, that's a cat. That's really difficult to provide that truth. And the one of the fundamental open questions and computer vision is is that even a good representation of the truth. Now, there is another contrasting set of ideas that our attention they're overlapping is what's used to be called unsupervised learning. What's commonly now called self supervised learning, which is trying to get less and less and less human supervision into the, into, into the task.

4 (12m 34s):
So self supervised learning is more, has been very successful in the domain of language models, natural English processing, and now more and more as being successful in computer vision tasks. And was the idea there is let the machine without any ground truth. Annotation, just look at pictures on the internet, or look at texts on the internet and try to learn something generalizable about the ideas that are at the core of language or at the core of vision. Yeah.

3 (13m 7s):
And based on that, we humans at its best.

4 (13m 12s):
I call that common sense. So with this, we have this giant base of knowledge on top of which we build more sophisticated knowledge. We have this kind of common sense knowledge. And so the idea of self supervised learning is to build this common sense knowledge about what are the fundamental visual ideas that make up a cat and a dog and all those kinds of things without ever having human supervision. The dream there is the, you just, you just let an AI system, that's a self supervised run around the internet for awhile. Watch YouTube videos for millions and millions of hours and without any supervision be primed and ready to actually learn with very few examples.

4 (13m 55s):
Once the human is able to show up, we think of children in this way, human children is your parents only give one or two examples to teach a concept. The, the dream was self supervised learning is that would be the same with, with machines that they would watch millions of hours of YouTube videos, and then come to a human and be able to understand when the human shows them, this is a cat, like, remember this a cat, they will understand that a cat is not just a thing with pointy ears or a CA cat is a thing that's orange, or as furry they'll, they'll see something more fundamental that we humans might not actually be able to introspect on. Understand, like if I asked you what makes a cat versus a dog, you wouldn't probably not be able to answer that.

4 (14m 39s):
But if I showed you brought to you a cat and a dog, you'll be able to tell the difference, what are the ideas that your brain uses to make that difference? That's the whole dream was so supervised learning is it would be able to learn that on its own, that set of common sense knowledge, that's able to tell the difference. And then there's like a lot of incredible uses of self supervised learning, a very weirdly called self play mechanism. That's the mechanism behind the, the reinforcement learning successes of the systems that want to go at alpha zero that want a chess. Oh, I see that play games that play games.

4 (15m 22s):
Got it. So the idea of self play is probably applies to other domains than just games is a system that just plays against itself. And this is fascinating in all kinds of domains, but it knows nothing in the beginning. And the whole idea is it creates a bunch of mutations of itself and plays against those versions of itself. And the fascinating thing is when you play against systems that are a little bit better than you, you start to get better yourself like learning. That's how learning happens. That's true for martial arts. It's true in a lot of cases where you want to be interacting with, with systems that are just a little better than you.

4 (16m 4s):
And then through this process of interacting with systems, just a little better than you, you start following this process where everybody says getting better and better and better and better until you are several orders of magnitude, better than the world champion in chess, for example. And it's fascinating because it's like a runaway system, one of the most terrifying and exciting things that David silver, the creator of AlphaGo enough, zero. One of the leaders of the team said to me is a, they haven't found the ceiling for alpha zero, meaning it could just arbitrarily keep improving now in the realm of chess, that doesn't matter to us that it's like, it just ran away with the game of chess.

4 (16m 45s):
Like it's like just so much better than humans. But the question is what, if you can create that in the realm that does have a bigger, deeper effect on human beings and societies, that can be a terrifying process. To me, it's an exciting process. If you supervise it correctly, if you inject, if what's called a value alignment, you, you make sure that the goals that the AI is optimizing is aligned with human beings and human societies. There's a lot of fascinating things to talk about within the specifics of neural networks and all the problems that people are, are working on.

4 (17m 26s):
But I would say the really big, exciting one is self supervised learning where trying to get less and less human supervision, less and less, you must have supervision of neural networks. And also just to comment and I'll shut up now, please keep going. I'm I'm learning. I have questions, but I'm learning. So please keep going. So that, to me, what's exciting is not the theory. It's always the application. One of the most exciting applications of artificial intelligence, specifically neural networks and machine learning is Tesla autopilot. So these are systems that are working in the real world. This isn't an academic exercise. This is human lives at stake. This is safety critical.

4 (18m 7s):
These are automated vehicles, autonomous semi-autonomous. We want to be okay. W we've gone through wars on these topics semi autonomous for you send me a time. So even though it's called a FSD full self-driving, it is currently not fully autonomous, meaning human supervision is required. So human is tasked with overseeing the systems. In fact, liability-wise, the human is always responsible. This is a human factor psychology question, which is fascinating. I'm fascinated by the whole space, which is a whole nother space of human robot interaction. When AI systems and humans work together to accomplish tasks, that dance to me

3 (18m 52s):
Is, is

4 (18m 54s):
One of the smaller communities. But I think it will be one of the most important open problems. Once they're solved is how the humans and robots dance together to me, semi-autonomous driving is one of those spaces. So for, for Elon, for example, he doesn't see it that way. He sees semi-autonomous driving as a stepping stone towards fully autonomous driving

3 (19m 19s):
Like humans and robots, can't dance well together,

4 (19m 23s):
Humans and humans, dancing robots and robots dance. Like we need to, this is an engineering problem. We need to design a perfect robot that solves this problem to me forever. Maybe this is not the case with driving, but the world is going to be full of problems with always humans and robots have to interact because I think robots will always be flawed. Just like humans are going to be flawed,

3 (19m 46s):
Are flawed. And that's what makes life beautiful. That they're flawed.

4 (19m 51s):
That's where learning happens at the edge of your capabilities. So you always have to figure out how can flawed robots and flawed humans interact together, such that they like the, the sum was bigger than the hole, as opposed to focusing on just building the perfect robot.

3 (20m 13s):
So th so

4 (20m 13s):
That's one of the most exciting applications I would say of artificial intelligence to me is autonomous driving and semi-autonomous driving. And that's a really good example of machine learning because those systems are constantly learning. And there's a, there's a process there that maybe I can comment on the Andre chapati. Who's the head of autopilot calls it the data engine. And th this process applies for a lot of machine learning, which is you build a system that's pretty good at doing stuff. You send any, you send it out into the real world. It starts doing this stuff. And it runs into

3 (20m 48s):
What are called edge cases, like failure cases, where it screws up. You know, we do this as kids that, you know, you have this as adults. We do this as adults. Exactly. But we learned really quickly, but th the whole point, and this is the fascinating thing about driving is you realize there's millions of edge cases. There's just like weird situations that you did not expect. And so the data engine process is you collect those edge cases, and then you go back to the drawing board and learn from them. And so you have to create this data pipeline where all these cars, hundreds of thousands of cars are driving around and something weird happens. And so whenever this weird detector fires, that's another important concept.

3 (21m 33s):
That piece of data goes back to the mothership for the, for the training, for the retraining of the system. And through this data engine process, it keeps improving and getting better and better and better and better. So basically you send out a pretty clever AI systems out into the world and let it find the edge cases, let it screw up just enough to figure out where the edge cases are, and then go back and learn from them, and then send out that new version and keep updating that version is the updating done by humans. The annotation has done by humans. The, so you have to the weird examples, come back the edge cases, and you have to label what actually happened in there.

3 (22m 18s):
There's also some mechanisms for automatic automatically labeling, but mostly, I think you always have to rely on humans to improve, to understand what's happening in the weird we are cases. And then there's a lot of debate. And this, the other thing, what is artificial intelligence, which is a bunch of smart people, having very different opinions about what is intelligence. So AI is basically a community of people who don't agree. Anything

1 (22m 44s):
Seems to be the case. You know, first of all, this is a beautiful description of terms that I've heard many times among my colleagues at Stanford at meetings in the, in the outside world. And there's so many fascinating things. I have so many questions, but I do want to ask one question about the culture of AI, because it does seem to be a community where at least as an outsider, where it seems like there's very little consensus about what the terms and the operational definitions even mean. And there seems to be a lot of splitting happening now of not just supervised and unsupervised supervised learning, but the sort of intermediate conditions where machines are autonomous, but then go back for more instruction. Like kids go home from college during the summer and get a little, you know, moms still feeds them.

1 (23m 26s):
Then eventually they leave the nest kind of thing. Is there something in particular about engineers or about people in this realm of engineering that you think lends itself to disagreement?

3 (23m 39s):
Yeah, I think so. First of all, the more specific you get, the less disagreement there is. So there's a of disagreement about what is artificial intelligence, but there's less disagreement about what is machine learning and even less, when you talk about active learning or machine teaching or self supervised learning. And then when you get into like NLP language models or transformers, when you get into specific neural network architectures, there's less and less and less disagreement about those terms. So you might be hearing the disagreement from the high level terms. And that has to do with the fact that engineering, especially when you're talking about intelligence systems is, is a little bit of an art and a science.

3 (24m 21s):
So the art part is, is the thing that creates disagreements, because then you start having disagreements about how easy or difficult to particular problem is. For example, a lot of people disagree with Elan, how difficult the problem of autonomous driving is. And, and so, but nobody knows. So there's a lot of disagreement about what are the limits of these techniques. And through that, the terminology also contains within it, the, the disagreements, but overall, I think it's also a young science that also has to do with that. So like it's not just engineering, it's that artificial intelligence truly as a large-scale discipline, where it's thousands, tens of thousands, hundreds of thousands of people working on it, huge amounts of money being made as a very recent thing.

3 (25m 14s):
So we're trying to figure out those terms. And of course there's egos and personalities and a lot of fame to be made. You know, like the, the term deep learning, for example, neural networks have been around for many, many decades since the sixties, you can argue since the forties. So there was a rebranding of neural networks into the war, deep learning term, deep learning that was part of the re-invigoration of the field, but it's really the same exact thing I do.

1 (25m 43s):
No, that, I mean, I grew up in the age of neuroscience when neural networks were discussed computational neuroscience and theoretical neuroscience, they had their own journals. It wasn't actually taken terribly seriously by experimentalists until a few years ago, I would say about five to seven years ago. Excellent. Theoretical neuroscientists like Larry Abbott and other I've colleagues, certainly at Stanford as well that people started paying attention to computational methods. But these terms, neural networks, computational methods, I actually didn't know that neural network works in deep learning. We're those have now become kind of synonymous.

3 (26m 21s):
No, they're always the same thing. Interesting. It

1 (26m 23s):
Was. So I'm a neuroscientist and I didn't know that

3 (26m 26s):
So well, because, you know, no worries probably means something else in your science, not something else, but a little different flavor depending on the field. And that's fascinating too, because neuroscience and AI people have started working together in dancing a lot more in the recent, I would say probably decade oh, machines are going

2 (26m 44s):
Into the brain. I have a couple of questions, but one thing that I'm sort of fixated on that I find incredibly interesting is this example you gave of playing a game with a mutated version of yourself as a competitor. Yeah. I find that incredibly interesting as a kind of a parallel or a mirror for what happens when we try and learn as humans, which is we generate repetitions of whatever it is we're trying to learn. And we make errors. Occasionally we succeed in a simple example, for instance, of trying to throw bulls eyes on a dartboard. Yeah. I'm going to have errors, errors, errors. I'll probably miss the dartboard and maybe occasionally hit a bullseye. And I don't know exactly what I just did.

2 (27m 26s):
Right. But then let's say I was playing darts against a version of myself where my, I was wearing a visual prism, like my visual. I had a visual defect. You learn certain things in that mode as well. You're saying that a machine can sort of mutate itself. Does the mutation always cause a deficiency that it needs to overcome? Because mutations in biology sometimes give us super powers, right? Occasionally you'll get somebody who has better than 2020 vision. And they can see better than 99.9% of people out there. So when you talk about a machine playing a game against a mutated version of itself, is the mutation always what we call a negative mutation or a, or an adaptive or a maladaptive means,

4 (28m 8s):
No, you don't know until you get so you mutate first and then figure out and they compete against each other.

2 (28m 15s):
You're evolving. You're the machine gets to evolve itself in real time.

4 (28m 18s):
Yeah. And I think of it, which would be exciting if you could actually do with humans. It's not just, so usually you freeze a version of the system. So really you take on Andrew of yesterday and you make 10 clones of them, and then maybe you mutate, maybe not. And then you do a bunch of competitions of the Andrew of today. Like you fight to the death and who wins last. So I love that idea of like creating a bunch of clones of myself from like, from each of the day, for the past year. And just seeing who's going to be better at like podcasting or science or picking up chicks at a bar or a, I dunno, or competing in jujitsu.

4 (29m 2s):
That's the one way to do it. I mean, a lot of Lexus would have to die for that process, but that's essentially what happens is in reinforcement learning through the self play mechanisms, it's a graveyard of systems that didn't do that well. And the surviving, the good ones survive.

2 (29m 20s):
Do you think that, I mean, Darwin's theory of evolution might have worked in some sense in this way, but at the population level, I mean, you get a bunch of birds with different shaped beaks and some birds have the shape beak that allows them to get the seeds. I mean, it's a trivial trivially, simple example of Darwinian evolution, but I think it's correct. If not, even though it's not exhaustive, is that what you're referring

1 (29m 42s):
To you essentially that normally this is done between members of a different species, lots of different members of species have different traits and some get selected for, but you could actually create multiple versions of yourself with different traits.

4 (29m 54s):
So with, I should probably have said this, but perhaps it's implied with machine learning with reinforcement learning through these processes. One of the big requirements is to have an objective function, a loss function or utility function. Those are all different terms for the same thing is there's a like any equation that says what's good. And then you're trying to optimize that equation. So there's a clear goal for these systems, right?

1 (30m 21s):
Because it's a game like with chess, there's a, there's a goal,

4 (30m 24s):
But for anything, anything you want machine learning to solve, there needs to be an objective function. And machine learning is usually called loss function that you're optimizing the interesting thing about evolution complicated, of course, but the goal also seems to be evolving. Like it's a, I guess, adaptation to the environment is the goal, but it's unclear that you can convert that always it's a survival of the fittest. It's unclear what the fittest is in machine learning, the starting point. And this is like, what human ingenuity provides is that fitness function of what's good and what's bad, which it lets you know, which of the systems is going to win.

4 (31m 8s):
So you need to have a equation like that. One of the fascinating things about humans is we figure out objective functions for ourself. Like we're, it's the meaning of life? Like why the hell are we here? And a machine currently has to have a hard coded statement about why

1 (31m 29s):
It has to have a meaning of yeah. Artificial intelligence based life. Right? Yeah.

4 (31m 34s):
Can't so like, there's a lot of interesting explorations about that function. Being more about curiosity, about learning new things and all that kind of stuff, but it's still hard coded if you want a machine to be able to be good at stuff, it has to be given very clear statements of what good at stuff means. That's one of the challenges of artificial intelligence is you have to formalize the, in order to solve a problem, you have to formalize it and you have to provide both like the full sensory information. You have to be very clear about what is the data that's being collected. And you have to also be clear about the objective function.

4 (32m 16s):
What is the goal that you're trying to reach? And that's a very difficult thing for artificial intelligence.

1 (32m 22s):
I love that. You mentioned curiosity. I am sure this definition falls short in many ways, but I define curiosity as a strong interest in knowing something, but without an attachment to the outcome, you know, it's sort of a, it's not, it could be a random search, but

2 (32m 40s):
There's not really an emotional attachment. It's really just a desire to discover and unveil what's there without hoping it's a, you know, a gold coin under a rock. You're just looking under rocks. Is that more or less how the machine, you know, within machine learning, it sounds like there are elements of reward prediction and you know, rewards. The machine has to know when it's done the right thing. So it, can you make machines that are curious or are the sorts of machines that you are describing, curious by design?

3 (33m 10s):
Yeah. Curiosity is a kind of a symptom, not the goal. So what happens is one of the big trade-offs in reinforcement learning is this exploration versus exploitation. So when you know, very little, it pays off to explore a lot, even suboptimal, like even trajectories that seem like they're not going to lead anywhere. That's called exploration, the smarter and smarter and smarter. You get the, the more emphasis you put on exploitation, meaning you take the best solution. You take the best path. Now through that process, the exploration can look like curiosity by us humans, but it's really just trying to get out of the local optimal.

3 (33m 56s):
The thing is already discovered it's it's from an AI perspective, it's always looking to optimize the objective function, it derives, and we can talk about the slot more, but in terms of the tools of machine learning today, it derives no pleasure from just the curiosity of like, I don't know a discovery that

2 (34m 20s):
No dopamine rushing me. There's no reward system chemical, or I guess electronic reward system

3 (34m 27s):
That said, if you look at machine learning literature and reinforcement learning literature that we'll use like DeepMind, we use terms like dopamine. We're constantly trying to use the human brain to inspire totally new solutions to these problems. So they'll think like how does dopamine function in the human brain and how can it lead to more interesting ways to discover optimal solutions? But ultimately currently the, there has to be a formal objective function. Now you could argue, the humans also has a set of objective functions would try and optimize. We're just not able to introspect them. We don't yet.

2 (35m 5s):
We don't actually know what we're looking for in seeking and doing

3 (35m 9s):
Well. Like Lisa Feldman, Barrett, he's spoken with at least on Instagram. I hope you met her through you. Yeah. Yeah. I hope you actually have are on this podcast. That'd be terrific. So she has a very, it has to do with homeostasis like that. Basically there's a very dumb objective function that the brain is trying to optimize, like to keep like body temperature the same, like there's a very Dom kind of optimization function happening. And that what we humans do with our fancy consciousness and cognitive abilities is we tell stories to ourselves so we can have nice podcasts, but really it's the brain trying to maintain a, just like healthy state, I guess that's fascinating.

3 (35m 52s):
I also see the human brain and, and I hope artificial intelligence systems as not just systems that solve problems or optimize a goal, but are also storytellers. I think there's a power to telling stories. We tell stories to each other. That's what communication is like when you're alone, that's when you solve problems that that's when it makes sense to talk about solving problems. But when you're a community, the capability to communicate, tell stories, hold share ideas in such a way that those ideas are stable over a long period of time. That's like that's being a charismatic storyteller.

3 (36m 33s):
And I think both humans are very good at this. Arguably I would, I would argue. That's why we are who we are is we're great storytellers and AI, I hope will also become that. So it's not just about being able to solve problems with a clear objective function. It's afterwards be able to tell like a way better, like make up a way better story about why you did something or why

1 (36m 55s):
You failed. So you think that robots or, and, or machines of, of some sort are going to start telling human stories. Well,

3 (37m 3s):
Definitely. So the technical field for that is called explainable AI explainable artificial intelligence is trying to figure out how you get the AI system to explain to us humans, why the hell it failed, or why it succeeded, or there's a lot of different sort of versions of this, or to visualize how it understands the world. That's a really difficult problem, especially when you're on low works that are famously opaque, that they, we don't understand in many cases why a particular neural network does what it does so well. And to try to figure out where it's going to fail, that requires the AI to explain itself, there's a huge amount of money.

3 (37m 49s):
Like there's a huge amount of money in this, especially from government funding and so on. Because if you want to deploy AI systems in the real world, we humans at least want to ask it a question, like, why the hell did you do that? Like in a dark way, why did you just kill that person? Right? Like if a car ran over a person went to understand why that happened. And now again, we're sometimes very unfair to AI systems because we, humans can often not explain why very well, but that's the field of explainable AI. That's very, people are very interested in because the more and more we rely on AI systems, like the Twitter recommender system that AI algorithm that's, I would say impacting elections, perhaps starting wars, or at least military conflict.

3 (38m 42s):
That's that algorithm. We want to ask that algorithm, first of all, do you know what the hell you're doing? Do you know, do you understand the society level effects you're having it? Can you explain the possible other trajectories? Like we would have that kind of conversation with a human. We want to be able to do that with an AI and in my own personal level, I think it would be nice to talk to AI systems for stupid stuff like robots when they fail to why'd you fall down the stairs. Yeah. But I'm not an engineering question, but almost like a endearing question, like, like I'm looking for, if I fell and you and I were hanging out, I don't think you need an explanation.

3 (39m 28s):
Exactly. What were the dynamics like? What was the under actuated system problem here? Like what, what was the texture of the floor or so on? Or like, what was the, I want to know what you're thinking that, or you might joke about like, you're drunk again, go home or something like there could be humor in it that that's an opportunity. Like storytelling, isn't just explanation of what happened. It's something that makes people laugh. It makes people fall in love. It makes people dream and understand things in a way that poetry makes people understand things as opposed to a rigorous log of where every sensor was, where every actuator was.

1 (40m 9s):
I mean, I find this incredible because you know, one of the hallmarks of severe autism spectrum disorders is a report of experience from the autistic person that is very much a catalog of S of action steps. It's like, how do you feel today? And they'll say, well, I got up and I did this, and then I did this and I did this. And it's not at all the way that a, a person with who doesn't have autism spectrum disorder would, would respond. And the way you described these machines has so much, human has so much humanism or so much of, of a human and biological element. But I realized that we were talking about machines. I, I want to make sure that I understand if there's a distinction between a machine that learns a machine with artificial intelligence and a robot, like at what point does a machine become a robot.

1 (41m 4s):
So if I have a ballpoint pen, I'm assuming I wouldn't call that a robot, but if my ballpoint pen can come to me, when it's on, when I moved to the opposite side of the table, if it's moves by whatever mechanism at that point, does it become a robot?

3 (41m 21s):
Okay, there's a million ways to explore this question. It's a fascinating one. So first of all, there's a question of what is life like? How do you know something as a living form and not, and it's to the question of when does sort of a, maybe a cold computational system becomes a M w we're already loading these words with a lot of robot and machine, but so one, I think movement is important, but that's a kind of a boring idea that a robot is just a machine that's able to act in the world. So one artificial intelligence could be both just the thinking thing, which I think is what machine learning is.

3 (42m 4s):
And also the acting thing, which is what we usually think about robots. So robots are the things that have a perception system that's able to take in the world. However you define the world is able to think and learn and do whatever the hell it does inside and then act on the world. So that's the difference between maybe an AI system or machine and a robot is something that's able, a robot is something that's able to perceive the world and act in the way.

1 (42m 28s):
So it could be through language or sound, or it could be through movement or both.

3 (42m 33s):
Yeah. And I think it could also be in the digital space, as long as there's a aspect of entity that's inside the machine and a world that's outside the machine. And there's a sense in which the machine is sensing that world and acting in it.

1 (42m 50s):
For instance, there could be a version of a robot, according to the definition that I think you're providing where the robot I, where I go to sleep at night and this robot goes and forges for information that it thinks I want to see loaded onto my desktop in the morning, there was no movement of that machine. There was no language, but essentially has movement in, in cyberspace.

3 (43m 11s):
Yeah. There's a distinction that I think is important in that there's a, there's an element of it being an entity, whether it's in the digital or the physical space. So when you have something like Alexa in your home, most of the speech recognition, most of what Alexa is doing is constantly being sent back to the mothership. The one Alexa is there on its own. That's to me, a robot, when it's there interacting with the world, when it's simply a finger of the main mothership, that's not, then the Alexa is not a robot.

3 (43m 57s):
Then it's just an interaction device that then maybe the main Amazon Alexa AI, big big system is the robot. So the that's important because there's some element to us, humans, I think, where we want there to be an entity, whether in the digital or the physical space, that's where ideas of consciousness come in and all those kinds of things that we project our understanding of what it means to be a being. And so to take that further, when does a machine become a robot? I think

4 (44m 32s):
There's a, there's a special moment. There's a special moment in a person's life in a robot's life where it surprises you. I think surprise is a really powerful thing where, you know, how the thing works and yet it surprises you that that's a magical moment for us humans. So whether it's a chess playing program that does something that you haven't seen before, that makes people smile

1 (44m 59s):
Like, huh,

4 (45m 1s):
Those moments happen with alpha zero for the first time in chess playing or grand masters, we're really surprised by a move. They didn't understand the move and then they studied and studied and then they understood it. But that moment of surprise that's for grandmasters in chess, I find that moment of surprise, really powerful, really magical, and just everyday life, because it supersedes the human

1 (45m 25s):
Brain in that moment. Not

4 (45m 28s):
So not supersedes like outperforms, but surprises you in a positive sense. Like I didn't, I didn't think he could do that. I didn't think that you had that in you. And I think that moment is a big transition for a robot from a, from a moment of being a servant, that particular, that accomplishes a particular task with some level of accuracy, with some, a rate of, of failure to an entity, a being that's struggling just like you are in this world. And that, that's a really important moment that I think you're not gonna find many people in the ag community that talk like I just did.

4 (46m 11s):
I, I'm not speaking like some philosopher, some hippie I'm speaking from purely engineering perspective. I think it's really important for robots to become entities and explore that as a real engineering problem, as opposed to everybody treats robots in the robotics community, they don't even call them or he or she, they don't give them try to avoid giving them names. They really want to see it like a system, like a S a servant. They see it as a servant is trying to accomplish a task to me. And I don't think I'm just romanticizing the notion. I think it's a being, it's a currently perhaps a dumb being, but in the, in the long arc of a history, humans are pretty dumb beings too.

4 (46m 55s):
So

1 (46m 55s):
I would agree with that statement. So

4 (46m 58s):
I tend to really want to explore this treating robots really as, as entities. Yeah. And so like at the promoter physician, which is the sort of the act of looking at a inanimate object and projecting onto it, lifelike features, I think robotics generally sees that as a, as a negative. I see it as a superpower like that. We need to use that.

1 (47m 26s):
Well, well, I'm struck by how that really grabs onto the relationship between human and machine or human and robot. So this, the simple question is, and I think you've already told us the answer, but does interacting with a robot change, you does it, in other words, do, do we develop relationships to robots?

3 (47m 48s):
Yeah, I think I definitely, I definitely think so. I think, I think the moment you see a robot or AI systems, as more than just servants, but entities, they begin to change and just like good friends do just like relationships just to other humans. I think of, for that, you have to have certain aspects of that interaction. Like the robot's ability to say no, to, to have its own sense of identity, to have its own set of goals. That's not constantly serving you, but instead, trying to understand the world and do that dance of understanding through communication with you.

3 (48m 29s):
So I definitely think there's a, I mean, I have a lot of thoughts about this, as you may know, and that's at the core of my lifelong dream actually of what I want to do, which is I believe that most people have a notion of loneliness in them that we haven't discovered that, that we haven't explored, I should say. And I see AI systems as helping us explore that so that we can become better humans, better people towards each other. So I think that connection between human and AI, human and robot is, is not only possible, but will help us understand ourselves in ways that are like several orders of magnitude deeper than we ever could have imagined.

3 (49m 21s):
I tend to believe that, well, I have very wild levels of belief in terms of how impactful that will be. Right?

1 (49m 36s):
So when I think about human relationships, I don't always break them down into variables, but we could explore a flute, a few of those variables and see how they map to human robot relationships. One is just time, right? If you spend zero time with another person at all in, in cyberspace or on the phone or in person, you essentially have no relationship to them. If you spend a lot of time, you have a relationship. This is obvious, but I guess one variable would be time. How much time you spend with the other entity, robot or human, the other would be wins and successes. You know, you enjoy successes together. I'll give a absolutely trivial example this in, in a moment, but the other would be failures.

1 (50m 19s):
When you struggle with somebody, whether or not you struggle between one another, you disagree. Like I was really struck by the fact that you said that robot saying, no, I've never thought about a robot saying no to me, but there it is.

3 (50m 31s):
I look, I look forward to you being one of the first send this robots.

1 (50m 36s):
So there's, there's struggle. You grow, you know, when you struggle with somebody, you grow closer. Sometimes the struggles are imposed between those two people. So-called trauma bonding. They call it in the whole a psychology literature and pop psychology literature. But in any case, I can imagine. So time successes together struggle together, and then just peaceful time hanging out at home, watching, watching movies, waking up near one another here, we're breaking down the elements of relationships of any kind. So do you think that these elements apply to robot human relationships?

1 (51m 16s):
And if so, then I could see how, if the, if the robot is its own entity and has some autonomy in terms of how it reacts you, it's not just there just to serve you. It's not just a servant. It actually has opinions. And can tell you when maybe your thinking is flawed or your actions are flawed and

3 (51m 36s):
Can also leave. It can,

1 (51m 38s):
Could also leave. So I've never conceptualized robot human interactions this way. So tell me more about how this might look, are we thinking about a human appearing robot? I know you and I have both had intense relationships to our, we have separate dogs obviously, but to, to animals, it sounds a lot like human animal interaction. So what is the ideal human robot relationship?

3 (52m 4s):
So there's a lot to be said here, but you actually pinpointed one of the big, big, first steps, which is say, do you have time? And it's a huge limitation in machine learning community currently, as now, we're back to like the actual details. Lifelong learning is as a, as a problem space that focuses on how AI systems can learn over a long period of time. What's currently most machine learning systems are not able to do is to all of the things you've listed under time. The successes, the failures are just chilling together. Watching movies, AI systems are not able to do that, which is all the beautiful, magical moments that I believe are the days filled with.

3 (52m 54s):
They're not able to keep track of those together with you

1 (52m 57s):
There because they can't move with you and be with you.

3 (52m 59s):
No, no, no. Like literally we don't have the techniques to a suit to do the learning. The actual learning of containing those moments. Current machine learning systems are really focused on understanding the world in the following way. It's more like the perception system, like looking around, understand like what's in the scene, that there's a bunch of people sitting down that there is cameras and microphones that a table

4 (53m 26s):
Understand that. But the fact that we shared this moment of talking today and still remember that for next time, for like next time you're doing something, remember that this moment happened. We don't know how to do that technique wise, this is what I'm, this is what I'm hoping to innovate on. As they think it's a very, very important component of what it means to create a deeper relationship, that sharing of moments together,

1 (53m 52s):
You post a photo of you in the robot, like cell selfie with robot, and the robot sees that image and recognizes. That was time spent. There was, there were smiles or there were tears and create some sort of metric of, of emotional depth in the relationship and update its behavior. So could it, it texts you in the middle of the night and say, why haven't you texted me?

4 (54m 17s):
Well, yes, all of those things, but we can, we can dig into that. But I think that time element, forget everything else, just sharing moments together that changes everything. I believe that changes everything. Now there's specific things that are more in terms of systems that can explain you. It's, it's more technical and probably a little bit offline. Cause I have kind of wild ideas how that can revolutionize social networks and, and operating systems. But the point is that element alone, forget all the other things we're talking about. Like emotions saying no all that time, just remembering sharing moments together would change everything.

4 (55m 0s):
We don't currently have systems that share, share moments together. Like even just you and your fridge, just all those times, you went late at night and ate things. You shouldn't have eaten. That was a secret moment you had with your refrigerator. You shared that moment, that darkness or that beautiful moment where you just, you know, like heartbroken, for some reason, you're eating that ice cream or whatever. That's a special moment. And that refrigerator was there for you. And the fact that it missed the opportunity to remember that is, is, is tragic. And once it does remember that, I think you're going to be very attached to their refrigerator.

4 (55m 42s):
You're going to go through some, through some hell with their refrigerator. Most of us have, like in, in, in the developed world, have weird relationships with food, right? So you can go through some, some deep moments of trauma and triumph with food. And at the core of that is the refrigerator. So a smart refrigerator, I believe would change society, not just the refrigerator, but the, these ideas in the systems all around us. So that, I just want to comment on how powerful that idea of time is. And then there's a bunch of elements of actual interaction of allowing you as a human to feel

3 (56m 24s):
Like you're being heard, truly heard, truly understood that we human, like deep friendship is like that. I think, but we're still, there's still an element of selfishness. There's still an element of not really being able to understand another human and a lot of the times when you're going through trauma together through difficult times and through successes, you're actually starting to get that inkling of understanding of each other. But I think that could be done more aggressively, more efficiently. Like if you think of a great therapist, I think I've never actually been to a therapist, but I'm a believer I used to want to be a psychiatrist

1 (57m 5s):
To Russians, go to therapists. Now they

3 (57m 7s):
Don't, they don't. And if they do the therapists, don't live to tell the story. No, I, I, I do believe in talk there, which w friendship is to me is, is talk therapy. Or like it's, it's, you don't even necessarily need to talk. It's like just connecting through in the space of ideas and the space of experiences. And I think there's a lot of ideas of how to make AI systems, to be able to ask the right questions and truly hear another human. This is what we try to do with podcasting. Right? I think there's ways to do that with AI, but above all else, just remembering the collection of moments that make up the day, the week, the months.

3 (57m 52s):
I think you maybe have some of this as well. Some of my closest friends still are the friends from high school. That's time. We've been through a bunch of shit together. And that like, we've, we're very different people, but just the fact that we've been through that, and we remember those moments and those moments somehow create a depth of connection. Like nothing else like you and your refrigerator.

1 (58m 17s):
I love that because the Ida, my graduate advisor, unfortunately, she passed away. But when she passed away, somebody said at her at our Memorial, you know, all these amazing things she had done, et cetera. And then her kids got up there and she had young children then that I knew as they were, when she was pregnant with them. And so it was really, you know, even now I can feel like your heart gets heavy, thinking about this are going to grow up without their mother. And it was really amazing, very, very strong young girls. And now the young women and what they said was incredible. They said what they really appreciated most about their mother, who was an amazing person, is all the unstructured time they spent together.

1 (58m 59s):
So it wasn't the trips to the zoo. It wasn't, you know, oh, you know, she woke up at five in the morning and drove us to school. She did all those things too. She had two hour commute, each direction who was incredible, ran a lab, et cetera, but it was the unstructured time. So on the passing of their mother, that that's what they remembered was that the biggest give and what bonded them to her was all the time where they just kind of hung out. And the way you, the relationship

2 (59m 22s):
To a refrigerator is, so I want to say humanlike, but I'm almost reluctant to say that because what I'm realizing as we're talking is that what we think of as humanlike might actually be the lower form of relationship. There may be relationships that are far better than the sorts of relationships that we can conceive in our minds right now, based on what these machine relationship interactions could teach us. Do I have that right?

3 (59m 53s):
Yeah, I think so. I think there's no reason to see machines as somehow incapable of teaching us something that's deeply human. I, I don't think humans have a monopoly on that. I think we understand ourselves very poorly and we need the, to have the kind of prompting from, from a machine. And definitely part of that is just remembering the moments, remembering the moments, you know, I think the unstructured time together, I wonder if it's quite so unstructured. That's like calling this podcast on structured time.

2 (1h 0m 30s):
Maybe what they meant was it wasn't a big outing. It wasn't as there was no specific goal, but a goal was created through the lack of a goal. Like we, you just hang out and then you start playing, you know, thumb warn, you end up playing thumb war for an hour there. So it's, it's the structure emerges from lack of structure.

3 (1h 0m 49s):
No, but the thing is the moments. There's something about those times that creates special moments. And I think th those could be optimized for, I think we think of like a big outing as, I don't know, going to six flags or something, or some big the grand canyon or go into some, I dunno, that I think we would need to, we don't quite yet understand as humans, what creates magical moments. I think this possible to optimize a lot of those things and perhaps like podcasting is helping people discover that, like maybe the thing we want to optimize for isn't necessarily like some sexy, like quick clips, maybe what we want as long form authenticity, depth, depth.

3 (1h 1m 36s):
So we were trying to figure that out, certainly from a deep connection between between humans and humans and NAS systems, I think long conversations or long periods of communication over a series of moments like my new, perhaps seemingly insignificant to the big ones, the big successes, the big failures, those are all just stitching those together and talking throughout. I think that's a, the formula for a really, really deep connection that from like a very specific engineering perspective is I think a fascinating open problem that has been really worked on very much.

3 (1h 2m 18s):
And for me a, if I have the guts and I mean, there's a lot of things to say, but one of it is guts is I'll build a startup around it.

1 (1h 2m 29s):
Yeah. So let's talk about this startup and let's talk about the dream. You mentioned this dream before in our previous conversations, always as little hints dropped here and there just for anyone listening, there's never been an offline conversation about this dream. I'm not privy to anything except what Lex says now. And I realized that there's no way to capture the full essence of a dream in any kind of verbal statement in a way that captures all of it. But what is the, what is this dream that you've referred to now several times when we've sat down together and talked on the phone, maybe it's this company, maybe it's something distinct.

1 (1h 3m 9s):
If you feel comfortable, it'd be great. If you could share a little bit about what that is.

3 (1h 3m 13s):
Sure. So th the way people express long-term vision I've noticed is quite different. Like Elon is an example of somebody who can very crisply say exactly what the goal is also has to do with the fact that problems he's solving have nothing to do with humans. So my long-term vision is a little bit more difficult to express. In words, I've noticed, as I've tried, it could be my brain's failure, but there's a way to sneak up to it. So let me just say a few things early on in life. In, in also in the recent years, I've interacted with a few robots where I understood there's magic there, and that magic could be shared by millions.

3 (1h 4m 2s):
If it's brought to light. When I first met spot from Boston dynamics, I realized this magic there that nobody else is seeing is the dog is the dog, sorry. The spot is the four legged robot from Boston dynamics. Some people might have seen it as this yellow dog. And, you know, sometimes in life, you just notice something that just grabs you. And I believe that this is something that this magic is something that could be every single device in the world. The way that I think maybe Steve jobs thought about the personal computer was didn't think about it, the personal computer this way.

3 (1h 4m 47s):
But Steve did, which is like, he thought that the personal computer should be as thin as a sheet of paper. And everybody should have one. I mean, this idea, I think it is heartbreaking that we're getting the world is being filled up with machines that are soulless. And I think every one of them can have that same magic. One of the things that also inspired me in terms of a startup is that magic can engineered much easier than I thought. That's my intuition with everything I've ever built and worked on. So the dream is to add a bit of that magic in every single computing system in the world.

3 (1h 5m 32s):
So the way that windows operating system for a long time was a, the primary operating system, everybody interacted with, they built apps on top of it. I think this is something that should be as a layer as almost as an operating system in every device that humans interacted with in the world. Now, what that actually looks like the actual dream. When I was officially a kid, it didn't have this concrete form of a business. It had more of a, a dream of exploring your own loneliness by interacting with machines, robots, this deep connection between humans and robots was always a dream.

3 (1h 6m 17s):
And so for me, I'd love to see a world where there's every home has a robot and not a robot that washes the dishes or a, or a sex robot, or I don't know, I think of any kind of activity that a robot can do, but more like a companion, the way Emily member, a family member, the way a dog is, but a dog that's able to speak your language too. So not just connect the way a dog does by looking at you and looking away and almost like smiling with its soul in that kind of way, but also to actually understand what the hell like, why are you so excited about the successes? Like understand the details, understand the traumas.

3 (1h 6m 60s):
And that, I just think that has always filled me with excitement that I could with artificial intelligence, bring joy to a lot of people more recently, I've been more and more heartbroken to see the kind of division derision even hate that's boiling up on, on the internet, through social networks. And I thought this kind of mechanism is exactly applicable in the context of social networks as well. So it's an operating system that serves as your guide to in the, on the internet.

3 (1h 7m 43s):
One of the biggest problems with YouTube and social networks currently is they're optimizing for engagement. I think if you create AI systems that know each individual person you're able to optimize for long-term growth for a longterm happiness of the individual or the individual of the individual. And there's a lot of other things to say, which is the in order for AI systems to, to learn everything about you, they need to collect, they need to just like you and I, when we talk offline or collecting data about each other secrets about each other, the same way AI has to do that.

3 (1h 8m 27s):
And that allows you to, and that requires you to rethink ideas of ownership of data. I think each individual should own all of their data and very easily be able to leave just like AI systems can leave. Humans can disappear and delete all of their data in a moment's notice, which is actually better than we humans can do this. Once we load the data into each other, it's there. I think it's very important to be both, give people complete control over their data in order to establish trust that they can trust you. And the second part of trust is transparency.

3 (1h 9m 9s):
Whenever the data is used to make it very clear what is being used for, and not clear in a lawyerly legal sense, but clear in a way that people really understand what it's used for. I believe when people have the ability to delete all their data and walk away and know how the data is being used, I think they'll stay

2 (1h 9m 30s):
The, the possibility of a clean breakup is actually what will keep people doing.

3 (1h 9m 33s):
Yeah, I think so. I think exactly. I think a happy marriage requires the ability to divorce easily without the, the divorce industrial complex or whatever these currently going on. And then there's so much money to be made from lawyers and divorced. But yeah, the ability to leave is what enables love. I think

2 (1h 9m 55s):
It's interesting. I've heard the phrase from a semi cynical friend that marriage is the leading cause of divorce, but now we've heard that divorce or the possibility of divorce could be the leading cause of marriage of a happy marriage, good point,

3 (1h 10m 9s):
Have a happy marriage. So, yeah, so, but there's, there's a lot of details there, but the big dream is that connection between AI system and a human. And I haven't, you know, there's so much fear about artificial intelligence systems and about robots that I haven't quite found the right words to express that vision because the vision I have is one is not like some naive delusional vision of like technology is going to save everybody it's I really do just have a positive view of ways AI systems can help humans explore themselves.

2 (1h 10m 45s):
I love that positivity and I, I agree that the, the stance, everything is doomed is equally bad to say that everything's gonna turn out. All right. There has to be a dedicated effort. And clearly you're thinking about what that dedicated effort would look like. You mentioned two, two aspects to this dream, and I want to make sure that I understand where they connect if they do, or if they are independent streams. One was this

1 (1h 11m 14s):
Hypothetical robot, family member, or some other form of robot that would allow people to experience the kind of delight that you experienced many times and that you would like the world to, to be able to have. And it's, it's such a beautiful idea of, of this give and the other is social media or social network platforms that really serve individuals and, and their best selves in their happiness and their growth. Is there crossover between those are these two parallel dreams.

3 (1h 11m 47s):
It's a hundred percent the same thing. It's, it's difficult to kind of explain without going through details, but maybe one easy way to explain the way I think about social networks is to create an AI system that's yours, that's yours. It's not like Amazon Alexa, that's centralized. You own the data. It's, it's low. It's like your little friend that becomes your representative on Twitter. That, that helps you find things that will make you feel good. That will also challenge your thinking to make you grow, but not get to that. Not let you get lost in the negative spiral of dopamine, that, that, that gets you to be angry or most just get you to be not open to learning.

3 (1h 12m 34s):
And so that little representative is optimizing your longterm health. And it's, I believe that that is not only good for human beings. It's also good for business. I think longterm, you can make a lot of money by challenging this idea that the only way to make money as a maximizing engagement. And one of the things that people disagree with me on is they think Twitter's always going to win. Like maximizing engagement is always going to win. I don't think so. I think people have woken up now to understanding that, like, they don't always feel good. The ones who are on Twitter a lot, that they don't always feel good at the end of the week.

1 (1h 13m 19s):
I would love feedback from whatever this creature, whatever I can't, I don't know what to call it as to, you know, maybe at the end of the week, it would automatically unfollow some of the people that I follow, because it realized through some real, really smart data about how I was feeling inside or how I was sleeping or something that, you know, that just wasn't good for me, but it might also put things and people in front of me that I ought to see is that kind of a sliver of what this, what this looks

3 (1h 13m 49s):
The whole point because of the interaction, because of sharing the moments and learning a lot about you, you're now able to understand what interactions led you to become a better version of yourself. Like the person you yourself are happy with. I mean, this isn't, you know, if you're into flat earth and you feel very good about it, that you believe that earth is flat, like the idea that you should sensor, that is ridiculous. If it makes you feel good and you becoming the best version of yourself, I think you should be getting as much flat earth as possible. Now it's also good to challenge your ideas, but not because the centralized committee decided, but because you tell to the system that you like challenging your ideas, I think all of us do.

3 (1h 14m 41s):
And then which actually YouTube doesn't do that. Well, once you go down the flat earth rabbit hole, that's all you're going to see. It's nice to get some really powerful communicators to argue against flat earth. And it's nice to see that for you and potentially at least long-term to expand your horizons. Maybe the earth is not flat, but if you continue to live your whole life, thinking the earth is flat, I think, and you're being a good father or son or daughter. And like you're being the best version of yourself. And you're happy with yourself. I think there is flat. So like, I, I think this kind of idea, and I'm just using that kind of silly, ridiculous example because I don't like the idea of centralized forces controlling what you can and can't see.

3 (1h 15m 33s):
But I also don't like this idea of like, not censoring anything, because that's always the biggest problem with that is this is there's a central decider. I think you yourself can decide what you want to see and not, and it's good to have a companion that reminds you, that you felt shitty last time you did this, or you felt good last time you did this,

2 (1h 15m 59s):
Man. I feel like in every good story, there's a, there's a guide or a companion that flies out or forges a little bit further, a little bit differently and brings back information that helps us, or at least tries to steer us in the right direction.

3 (1h 16m 11s):
So, yeah, that's exactly, that's exactly the, what I'm thinking and what I've been working on. I should mention as a bunch of difficulties here, you you've seen me up and down a little bit recently, so there's technically a lot of challenges here. This like with a lot of technologies and the reason I'm talking about it on a podcast comfortably, as opposed to working in secret is it's really hard and maybe it's time has not come. And that's something you have to constantly struggle with in terms of like entrepreneurially as, as a startup, like I've also mentioned to you maybe offline, I really don't care about money.

3 (1h 16m 53s):
I don't care about business success, all those kinds of things. So it's a difficult decision to make. How much of your time do you want to go all in here and give everything to this? It's a big roll the dice, because I've also realized that's working on some of these problems, both with the robotics and the technical side on the, in terms of the, the machine learning system that I'm describing. It's lonely is really lonely because both on a personal level and a technical level. So on the technical level, I'm surrounded by people that kinda doubt me.

3 (1h 17m 38s):
I think all entrepreneurs go through and they doubt you in the following sense. They, they, they know how difficult it is. Like the people that the colleagues of mine, they know how difficult lifelong learning is. They also know how difficult it is to build a system like this does to build a competitive social network. And in general, there's a kind of a loneliness to just working on something on your own for long periods of time. And you start to doubt whether given the, you don't have a track record of success. Like that's a big one when you look in the mirror, especially when you're young, but I still have that.

3 (1h 18m 21s):
I'm most things you look in the mirror is like, and you have these big dreams. How do you know your, how do you know you're actually as smart as you think you are? Like, how do you know you're going to be able to accomplish this dream? You have this ambition

2 (1h 18m 37s):
Don't, but you're, you're kind of pulling on a, on a string, hoping that there's a bigger ball of yarn. Yeah.

3 (1h 18m 44s):
Well, you have this kind of intuition. I I've, I think I pride myself in knowing what I'm good at, because the reason I have that intuition is because I think I'm very good at knowing all the things I suck at, which is basically everything. So like whenever I notice, like, wait a minute, I I'm of good at this, which is very rare for me. I think like that, that might be a ball yarn worth pulling it. And the thing with in terms of engineering systems that are able to interact with humans, I think I'm very good at that. And because we talk about podcasting and so on, I don't know if I'm very good at pocket. You're very good at podcasting, but I certainly don't.

3 (1h 19m 27s):
I think maybe it is compelling to, to, for people to watch a kindhearted idiot struggle with this, with this form. Maybe that's what what's compelling, but in terms of like actual being a good engineer of human robot interaction systems, I think I'm good, but it's hard to know until you do it. And then the world keeps telling you you're not, and it's just, it's full of doll. It's really hard. And I've been struggling with that recently. It's kind of a fascinating struggle, but then that's where the Goggins thing comes in is like, aside from the state hard motherfucker is the, like, whenever you're struggling, that's a good sign that if you keep going that you're going to be alone in the success, right?

3 (1h 20m 15s):
Like,

2 (1h 20m 16s):
Well, in your case, however, I agree. And actually David had a post recently that I thought was among his many brilliant posts was one of the more brilliant about how, you know, he talked about this myth of the light at the end of the tunnel. And instead what he replaced that myth with was a concept that eventually your eyes adapt to the dark, that the tunnel, it's not about a light at the end, that it's really about adapting to the target of the tunnel is very Goggins loves him so much that yeah, you guys share a lot, a lot in common knowing you both a bit, you know, share a lot in common, but in this loneliness and the, and the pursuit of this dream, it seems to me, it has a certain component to it that is extremely valuable, which is that the loneliness itself could serve as a driver to build the companion for the jury.

3 (1h 21m 10s):
Well, I'm very deeply aware of that. So like some people can make, cause I talk about love a lot. I really love everything in this world. And, but I also love humans, friendship and romantic, you know, like even the cheesy stuff,

2 (1h 21m 30s):
Just you like romantic movies. Yeah.

3 (1h 21m 35s):
Well, I got so much shit from Rogan about like, was it the tango scene from a scent of a woman, but yeah, I find like there's nothing better than a woman in a red dress. Like a, you know, just like classy,

2 (1h 21m 49s):
You should move to Argentina. My, you know, my father's Argentine and you know what he said, when I, when I went on your podcast for the first time he said he dresses. Well, because in Argentina, the men go to a wedding or a party or something, you know, in the U S they buy halfway through the night, 10 minutes in the night, all the jackets are off. Yeah. It looks like everyone's undressing for the party. They just got dressed up for it. And he said, and he said, you know, I liked the way he dresses. And then when I started, he was talking about you. And then when he, when I started my podcast, he said, why don't you wear a, a real suit? Like your friend?

3 (1h 22m 20s):
I don't know that,

2 (1h 22m 23s):
But let's talk about this, this pursuit just a bit more, because I think what you're talking about is, is building a, not just a solution for loneliness, but you've alluded to the loneliness as itself, an important thing. And I think you're right. I think within people, there is caverns of faults and shame, but also just the desire to be, to have resonance, to, to be seen and heard. And I don't even know that it's seen and heard through language, but these reservoirs are of loneliness. I think they're well, they're interesting. Maybe you could comment a little bit about it because just as often as you talk about love, I'm in quantified

1 (1h 23m 6s):
It, but it seems that you talk about this loneliness. Maybe you just w if you're willing, you could share a little bit more about that. And, and what, what that feels like now in the pursuit of building this robot human relationship. And you've been, let, let me be direct, even spending a lot of time on building a robot human relationship. Where's that at?

3 (1h 23m 28s):
Oh, in terms of business and in terms of systems, no, I'm talking about a specific robot. Oh, rope. So, okay. I should, I should mention a few things. So one is, there's a startup where an idea where I hope millions of people can use, and then there's my own personal, like, almost like Frankenstein explorations with the particular robots. So I'm very fascinated with the legged robots in my own private sounds like dark, but like it one N of one experiments to see if I can recreate the magic.

3 (1h 24m 10s):
And that's been, I have a lot of really good already the perception systems and control systems that are able to communicate affection in a dog-like fashion. So I'm in a really good place there. The stumbling blocks, which also been part of my sadness recently is that I also have to work with robotics companies that, you know, I gave so much of my heart, soul and love and appreciation towards Boston dynamics. But Boston dynamics is also, you know, as a company that has to make a lot of money and they have marketing teams. And they're like looking at this silly Washington kid in a suit and tie, it's like, what's he trying to do with all this love and robot interaction and dancing and so on.

3 (1h 24m 53s):
So there was a, I think let's say for now, it's like, when you break up with a girlfriend or something, right now, we decided to part ways on this particular thing, they're huge supporters of mine. They're huge fans, but on this particular thing, Boston dynamics is not focusing on or interested in human robot interaction. In fact, their whole business currently is keep the robot as far away from humans as possible because it's, it's an in the industrial setting where it's doing monitoring in dangerous environments. It's almost like a remote security camera essentially is its application to me. I thought, ah, it's still, even in those applications, exceptionally useful for the robot to be able to perceive humans like see humans and to be able to, in a big map, localize with those humans are and have human intention.

3 (1h 25m 48s):
For example, like this, I did this a lot of work with pedestrians, for a robot to be able to anticipate what the, how the human is doing, like where it's walking. If you're humans are not ballistics object, they're not just because you're walking this way. One moment doesn't mean you'll keep walking that

4 (1h 26m 4s):
You have to infer a lot of signals, especially the head movement and the eye movement. And so I thought that's super interesting to explore, but they didn't feel that. So I'll be working with a few other robotics companies that are, are much more open to that kind of stuff. And they're super excited and fans of mine and hopefully Boston dynamics. My first love that getting back with an ex-girlfriend will come around, but so the algorithmically it's basically a done there. The, the rest is actually getting some of these companies to work with, and then there's for people who'd worked with robots. Know that one thing is to write software that works.

4 (1h 26m 45s):
And the other is to have a real machine that actually works. And it breaks down all kinds of different ways that are fascinating. And so there's a big challenge, but that's almost, yeah, it may sound a little bit confusing in the context of our previous discussion because the previous discussion was more about the big dream, how I hoped to have millions of people enjoy this moment of magic. The, this current discussion about a robot is something I personally really enjoy just brings me happiness. I really try to do now everything that just brings me joy. I maximize that because it's because robots are awesome. But to give them my like little bit growing platform, I want to use the opportunity to educate people.

4 (1h 27m 32s):
It's just, it's like robots are cool. And if I think they're cool, I'll be able to, I hope be able to communicate why they're cool to others. So the, this little robot experiment is a little bit of research project too. There's a couple of publications with MIT folks are on that, but the, the other is just the, make some cool videos and explain to people how they actually work. And as opposed to people being scared of robots, they can be, they can still be scared, but also excited, like see the, the dark side, the beautiful side, the magic of what it means to bring, you know, for a machine to become a robot. I want to inspire people with that, but that's less.

4 (1h 28m 14s):
It's interesting because I think the big impact in terms of the dream does not have to do with embodied AI. So it does not need to have a body. I think the refrigerators enough that for an AI system, just to have a voice and to hear you that's enough for loneliness, the embodiment is just by embodiment. You meet the physical structure, physical instantiation of intelligence. So it's a Lego robot, or even just a thing. I have a few other, a humanoid robot, a little human or robot. Maybe I'll keep them on the table. This is like walks around or even just like a mobile platform.

4 (1h 28m 55s):
They can just like turn around and look at you. It's like we mentioned with the pen, something that moves and can look at you,

3 (1h 29m 2s):
It's like that bought a robot. But w that asks what is my purpose? That, that is really, it's almost like art. There's something about a physical entity that moves around. That's able to look at you and interact with you. That makes you wonder what it means to be human, like challenges you to think if I, if that thing looks like he has consciousness, what the hell am I? And I like that feeling. I think that's really useful for us. It's humbling for us humans, but that's less about research, certainly less about business and more about exploring our own, our own cells and challenging others to think, like, to, to, to think about what makes them human.

2 (1h 29m 53s):
I love this desire to share the delight of an interaction with a robot. And as you describe it, I actually, I find myself starting to crave that because we all have those elements from childhood where, or from adulthood, where we experienced something, we want other people to feel that. And I think that you're right. I think a lot of people are scared of AI. I think a lot of people are scared of robots. My only experience of a robotic like thing is my Roomba vacuum, where it goes about actually was pretty good at picking up Castillos hair when he was shed. And then, and I was grateful for it, but then when it would, when I was on a call or something, and it would get caught on a, on a wire or something, I would find myself getting upset with the Roomba in that moment.

2 (1h 30m 36s):
I'm like, what are you doing? You know, and I, and obviously it's just doing what it does, but, but that's a kind of mostly positive, but slightly negative interaction. But what you're describing, it has so much more richness and layers of detail that I can only imagine what those relationships are like.

3 (1h 30m 54s):
Well, there's a few, just a quick comment. So I've had, they're currently in Boston and I have a bunch of Roombas from I robot, and I did this experiment. Wait, how many Roombas sounds like a fleet of probably seven or eight?

2 (1h 31m 9s):
Well, a lot of room was, so this place is very clean.

3 (1h 31m 13s):
Well, so this I'm kind of waiting this, this is the, the place we're currently in, in Austin is way larger than I need, but it's, I basically got it. So to, to make sure I have room for robots.

2 (1h 31m 27s):
So you're going to, so you have these seven or so Roombas, you deploy all seven at once.

3 (1h 31m 32s):
Oh no. I do different experience with them at different experiments with them. So one of the things I want to mention is this is a, I think there was a YouTube video that inspired me to try this is I got them to a disc scream in pain and moan in pain whenever they were kicked or contacted. And I did that experiment to see how I would feel. I meant to do like a YouTube video on it, but then just seemed

2 (1h 31m 60s):
Very cruel. Did any Roomba rights activists?

4 (1h 32m 2s):
Yeah. That's like, I think if I released that video, I think is going to make me look insane, which I know people know I'm already insane. Now you have to release the video. Sure. I think maybe if I contextualize it by showing other robots like to show why this is fascinating, because ultimately I felt like there were human almost immediately. And that display of paying was what did that, giving them a voice, giving them a voice, especially a voice of a dislike of, of pain.

2 (1h 32m 37s):
I have to connect you to my friend, Eddie Chang. He studied speech and language. He's a neurosurgeon and we're lifelong friends. He studied speech and language, but he describes some of these more primitive, visceral vocalizations, cries, groans, moans of delight. Other sounds as well, use your imagination as such powerful rudders for the other, for the emotions of other people. So I find it fascinating. I can't wait to see this video. Is that, so is the video available?

4 (1h 33m 9s):
No, I haven't. I haven't recorded it. I just hit a bunch of Roombas that are able to scream in pain in my Boston, in my Boston place. So I like people already as

2 (1h 33m 22s):
Next podcast episode with Lex, maybe we'll have that one

4 (1h 33m 26s):
Who knows. Also the thing is like people I I've noticed because I talk so much about love and it's really who I am. I think they want to, to a lot of people, it seems like there's, there's there gotta be a dark person in there somewhere. And I thought if I release videos and room for screaming and they're like, yep, yup. That guy's definitely insane.

2 (1h 33m 45s):
She like shouts of glee and delight. You could do that too. Right?

4 (1h 33m 49s):
Well, I don't know how to, I don't how to, to me delight is quiet. Right? Like you're rushing. I don't know.

2 (1h 33m 58s):
Americans are much louder than Russians.

4 (1h 33m 60s):
Yeah, yeah. Yeah. But like, I don't, I mean, unless you're talking about like, I dunno how you would have sexual relations with a room, but

2 (1h 34m 7s):
Necessarily saying a sexual delight, but

4 (1h 34m 11s):
I tried, that's a joke internet. Okay. But I was fascinating. The psychology of how little it took. Cause you mentioned you had a negative relationship with the Roomba a little bit.

2 (1h 34m 22s):
Well, I'd find that mostly. I took it for granted. Yeah. It just served me. It collected Costello's hair. And then when it would do something, I didn't like I would get upset with it. So that's not a good relationship. It was taken for granted and I would get upset and then I'd park it again. And I'd just like, you're, you're in the, in the corner. Yeah.

4 (1h 34m 40s):
But there's a way to frame, it's it being quite dumb as almost cute, you know, you're almost connecting with it for its dumbness. And I think that's a artificial intelligence problem. Thanks, Justin. I think flaws are, should be feature, not a bug.

2 (1h 34m 59s):
So along the lines of this, the different sorts of relationships that one could have with robots and the fear, but also that some of the positive relationships that one could have, there's so much, dimentionality, there's so much to explore, but power dynamics in relationships are very interesting because the obvious ones that the unsophisticated view of this is, you know, one there's a master and a servant, right? But there's also manipulation. There's benevolent manipulation. You know, children do this with parents puppies, do this puppies, turn their head and look cute and maybe give out a little, little noise kids coup and parents always think that they're, you know, they're doing this because you know, they, they love the parent.

2 (1h 35m 45s):
But in many ways, studies show that those coups are ways to extract the sorts of behaviors and expressions from the parent that they want. The child doesn't know it's doing this. It's completely subconscious, but it's benevolent manipulation. So there's one version of fear of robots. And I hear a lot about that. I think most people can relate to where the robots take over and they become the masters and we become the servants. But there could be another version that, you know, in certain communities that I'm certainly not a part of, but they call topping from the bottom where the robot is actually manipulating you into doing things. But it, you are under the belief that you are in charge, but actually they're in charge.

2 (1h 36m 29s):
And so I think that's one that if we could explore that for a second, you could imagine it wouldn't necessarily be bad, although it, it could lead to bad things. The reason I want to explore this as I think people always default to the, the extreme, like the robots take over and we're in little jail cells and they're out having fun and ruling the universe. What, what, what sorts of manipulation can a robot potentially carry out good or bad?

4 (1h 36m 56s):
Yeah. Just so there's a lot of good and bad manipulation between humans, right? Just like you said to me, especially like you said, a topping from the bottom, is that the term? So

2 (1h 37m 12s):
I think someone from MIT told me that term wasn't blacks.

4 (1h 37m 18s):
I think so first of all, there's power dynamics in bed and power dynamics in relationships and power dynamics on the street and in the work environment, those are all very different. I think, I think power dynamics can make human relationships, especially romantic, romantic relationships, fascinating and rich and fulfilling and exciting and all those kinds of things. So I don't, I don't think in themselves they're bad and the same goes with robots. I really love the idea that a robot will be at top or a bottom in terms of like power dynamics. And I think everybody should be aware of that.

4 (1h 37m 60s):
And then manipulation is not so much manipulation, but a dance of like pulling away, push and pull and all those kinds of things in terms of control. I think we're very, very, very far away from AI systems that are able to lock us up. They to lock us up in a, in a, you know, like to have so much control that we basically cannot live our lives in the way that we want. I think there's a, in terms of dangers of AI systems, there's much more dangerous that have to do with autonomous weapon systems and all those kinds of things. So the power dynamics as exercised in a struggle between nations and war and all those kinds of things, but in terms of personal relationships, I think power dynamics are a beautiful thing.

4 (1h 38m 46s):
Now there's of course going to be all those kinds of discussions about consent and rights and all those kinds.

1 (1h 38m 53s):
Well, you're, we're talking about, I always say, you know, in any discussion around this, if, if we need to define really the context, it's always, it always should be consensual age, appropriate context, appropriate species appropriate. Yeah. But now we're talking about human robot interactions. And so I guess that

4 (1h 39m 11s):
I, I w actually was trying to make a different point, which is, I do believe that robots will have rights down the line. And I think in order for, in order for us to have deep, meaningful relationships with robots, we would have to consider them as entities in themselves that deserve respect. And that's a really interesting concept that I think people are starting to talk about a little bit more, but it's very difficult for us to understand how entities that are other than human. I mean, the same as with dogs and other animals can have rights on a level as humans.

1 (1h 39m 45s):
Well, the, yeah, I mean, th we, we can't, and nor should we do whatever we want with animals, we have a USDA with the, we have departments of, of agriculture that deal with, you know, animal care and use committees for research, for Agra, you know, for farming and ranching and all that. So I, I, while it, when you first said it, I thought, wait, why would have, there'll be a bill of robotic rights, but it absolutely makes sense in the context of everything we've been talking about up until now. It let's, if you're willing, I'd love to talk about dogs because you've mentioned dogs a couple of times, a robot dog, you had a biological dog.

1 (1h 40m 26s):
Yeah.

4 (1h 40m 27s):
A Newfoundland named Homer for many years, growing up in Russia or in the U S in the United States. And he was about, is over 200 pounds. It's a big dog. That's a big dog. If people know, people know Newfoundland, so he's this black dog, that's really a long hair and just a kind soul. I think perhaps that's true for a lot of large dogs, but he thought

3 (1h 40m 54s):
He was a small dog. So he moved like that. And was he your dog? Yeah. Yeah. So you had him since he was fairly young office since, yeah. Since the very, very beginning till the very, very end. And one of the things, I mean, he had this kind of a, we mentioned like the Roombas, he had kind hearted dumbness about him. That was just overwhelming. Part of the reason I named him Homer, because it's after Homer Simpson case, people are wondering which Homer I'm referring to. I'm not, you know, so that there's exactly does a CLA clumsiness.

3 (1h 41m 34s):
That was just something that immediately led to a deep love for each other. And one of the, I mean, he was always, it's the shared moments. He was always there for so many, a nights together. That's a, that's a powerful thing about a dog there. He was there through all the loneliness, through all the tough times through the successes and all those kinds of things. And I remember, I mean, that was a really moving moment for me. I still miss him to this day. How long ago did he die? Maybe 15 years ago. So it's, it's been awhile, but it was the first time I've really experienced, like the feeling of death is, so what happened is he, he got cancer.

3 (1h 42m 23s):
And so he was dying slowly. And then the certain point he couldn't get up anymore. There's a lot of things that could say here, you know, that I struggle with. I may be, maybe he suffered much longer than he needed to. That's something I really think about a lot, but I remember I had to take him to the hospital and the nurses couldn't carry him. Right. So you talked about a 200 pound dog, and I was really into power lifting at the time. And I remember like, they, they, they tried to figure out all these kinds of ways to, so in order to put them to sleep it, to take them into a room.

3 (1h 43m 7s):
And so I had to carry them everywhere. And here's this dying friend of mine that I just had to, first of all, it's really difficult to carry somebody that heavy when they're not helping you out. And yeah, so I remember it was the first time seeing a friend laying there and seeing wife drained from his body.

5 (1h 43m 34s):
And that

3 (1h 43m 35s):
Realization that we're here for a short time was made so real that he has a friend that was there for me the week before, the day before and now he's gone. And that was, I don't know that that spoke to the fact that he be deeply connected with the dog. Also spoke to the fact that the, the shared moments together that led to that deep friendship is, are what make life so amazing. But it also spoke to the fact that death is a motherfucker. So I know you've lost Castella recently.

3 (1h 44m 16s):
Yeah. And you've been going,

1 (1h 44m 17s):
And as you're saying this, I'm definitely fighting back the tears. I thank you for sharing that, that, I guess we're about to both cry over our dead dogs, that it was that it was bound to happen just given when this is when this is happening. Yeah. It's

3 (1h 44m 35s):
How long, how long did you know that Castello was not doing well?

1 (1h 44m 40s):
We'll, let's see a year ago during the start of about six months into the pandemic, he started getting abscesses and he was not his behavior change and something really changed. And then I put him on testosterone because which helped a lot of things that certainly didn't cure everything, but it helped a lot of things. He was dealing with joint pain, sleep issues. And then it just became a very slow decline to the point where, you know, two, three weeks ago he had, you know, a closet full of medication. I mean, this dog was, you know, it was like a pharmacy. It's amazing to me. When I looked at it the other day, I still haven't cleaned up and removed all those things.

1 (1h 45m 22s):
Cause I can't quite bring myself to do it. But did you think he was suffering well? So what happened was about a week ago, it was really just about a week ago. It's amazing. He was going up the stairs and I saw him slip and he was a big dog. He wasn't 200 pounds, but he was about 90 pounds, but he's a bulldog that's pretty big. And he was fit. And then I noticed that he wasn't carrying the afoot in the back. Like it was injured. It had no feeling at all. He never liked me to touch his hind paws and I could do that thing. It was just flopping there. And then the vet found some spinal degeneration and I was told that the next one would go, did he suffer? Sure. Hope not. But something changed in his eyes.

1 (1h 46m 1s):
Yeah. The eyes again, I know you and I spend long hours on the phone and tell you about like the eyes and how, what they convey and what they mean about internal states and for sake of robots and biology of other kinds, but

3 (1h 46m 14s):
Something about him was gone in his eyes.

1 (1h 46m 18s):
I, I think he was real here. I am anthropomorphizing. I think he was realizing that one of his great joys in life, which was to walk and sniff and pee on things. This dog, the fundamental pee on things is amazing. I've wondered where he put it. He was like a reservoir of urine. It was incredible. I think, oh, that's Eddie. He's just, he'd put like one drop on the 50000000th plant. And then we get to the 50,000,001 plant more. He just

2 (1h 46m 50s):
Have, you know, leave a puddle. And here I am talking about Costello peeing. He was losing that ability to stand up and do that. He was falling down while he was doing that. And I do think he started to realize, and the passage was easy and peaceful, but you know, I'll say this, I'm not ashamed to say it. I mean, I wake up every morning since then. Just, I don't even make the conscious decision to allow my self to cry, wake up crying. And I'm unfortunately able to make it through the day. Thanks to the great support of my friends and, and you and my family. But I miss him, man. I miss him. Yeah. I miss him. And I feel like he, you know, Homer Costello, you know, the relationship to one's dog is so specific, but so that, that party is gone.

4 (1h 47m 38s):
That's the hard thing,

2 (1h 47m 39s):
You know, what's what, what I think is different is that I made the mistake. I think one more, I hope it was a good decision, but sometimes I think I made the mistake of, I brought Costello a little bit to the world, through the podcast or posting about them. I gave, I answered for Morris. Vised about him in public. Let's be honest. I have no idea what his mental life was or his relationship to me. And I'm just exploring all this for the first time, because he was my first dog, but I raised him since he was seven weeks. Yeah. You got it.

4 (1h 48m 10s):
Hold it together. I noticed the episode a year, at least on Monday, you mentioned Costello. Like you, you brought them back to life for me for that brief moment. Yeah. But he's, he's, he's gone. W that's the he's going to be gone for a lot of people too.

2 (1h 48m 28s):
Well, this is what I'm struggling with. I think that maybe you're pretty good at this law. Like, wait, have you done this before? This is the challenge. Is I actually a part of me? I know how to take care of myself pretty well. Yeah. Not perfectly, but pretty well. And I have good support. I do worry a little bit about how it's going to land and how people will feel I'm concerned about their internalization. So that's something I'm still, I'm still iterating on. Yeah.

4 (1h 48m 57s):
They have to watch you struggle with just fascinating. Right.

2 (1h 48m 60s):
And I've mostly been shielding them from this, but what would make me happiest if, is if people would internalize some of Costello's best traits and his best traits were that he was incredibly tough. I mean, he was a, you know, 22 inch neck bulldog the whole thing. He was just born that way. But was what was so beautiful is that his toughness is never what he rolled forward. It was just out sweet and kind he was. And so if people can take that, then, then there's a win in there someplace.

4 (1h 49m 32s):
So I think there's some ways in which she should probably live on in your podcast too. You should. I mean, it's such a, one of the things I loved about his role in your podcast is that brought so much joy to you. I mentioned the robots, right? I think that's such a powerful thing to bring that joy into like allowing yourself to experience that joy, to bring that joy to others, to share it with others. That's really powerful. And I mean, not to, this is, this is like the Russian thing is it's it touched me when Lucy Kay had that moment that I keep thinking about in this, his show, Louie, where like an old man was criticizing Louis for whining about breaking up with his girlfriend.

4 (1h 50m 23s):
And he was saying like the most, the most beautiful thing about love, they made a song that's catchy. Now that's not making me feel horrible saying it, but like, is the loss, the loss really also is making you realize how much that person, that dog meant to you and like allowing yourself to feel that loss and not run away from that loss is really powerful. And in some ways that's also sweet, just like the love was the loss is also sweet because you know, that you felt a lot for that for your friend.

4 (1h 51m 4s):
So I, you know, and like continue to bring that joy. I think it would be amazing to the podcast. I hope to do the same with, with robots or whatever else is the source of joy. Right. And maybe, do you think about one day getting a, another dog?

1 (1h 51m 22s):
Yeah. In time you're hitting on all the key buttons here. I want that to, we're thinking about, you know, ways to kind of immortalize Costello in a way that's real, not just, you know, creating some little logo or something silly, you know, Costello much like David Goggins is a, a person, but Goggins also has grown into kind of a verb. You're going to Goggins this or you're, and there's an adjective. Like that's extreme. Like it, I think that for me, Costello was all those things. He was a, he was a being, he was his own being, he was a noun, a verb and an adjective. So, and he had this amazing superpower that I wish I could get, which is this ability to get everyone else to do things for you without doing a damn thing.

1 (1h 52m 9s):
The Costello effect is I call it that as an idea he lives on. Yes. Thank you for that. This actually has been very therapeutic for me, which, which actually brings me to a question we're friends. We're not just a co scientists, colleagues working on a project together and, and in the world, that's somewhat similar to two dogs, just two dogs basically. But let's talk about friendship because I think, I certainly know as a scientist, that there are elements that are very lonely of the scientific pursuit.

1 (1h 52m 53s):
There are elements of many pursuits that are lonely music. Math always seemed to me like they're like the loneliest people who knows if that's true or not. Also people work in teams. And sometimes people are surrounded by people interacting with people and they feel very lonely. But for me, and I, and I think as well for you, friendship is an incredibly strong force in making one feel like certain things are possible or worth reaching for maybe even making us compulsively reach for them. So when you were growing up, you grew up in Russia until what age?

1 (1h 53m 33s):
13. Okay. And then, and then you moved directly to Philadelphia. Is that where you

4 (1h 53m 39s):
Chicago, Chicago, and then Philadelphia, you know, and San Francisco and Boston and so on, but really to Chicago, that's where I went to high school. Do you have siblings, brother, older brother. But most people don't know that. Yeah, he is a very different, very different person, but somebody I definitely look up to. So he's a wild man. He's extrovert he's he was into, I mean, so he's, he's also scientists, a bio engineer, but he's when we were growing up and he was the person who, you know, did drank and did every drug. And, but also as the life of the party.

4 (1h 54m 19s):
And, and I just thought he was the COO, you know, when your older brother, five years older, he was the coolest person that, you know, I, I was wanting to be him so that he was, he definitely had a big influence, but I think for me on, in terms of friendship, growing up, I had a, I had one really close friend. And then when I came here at another close friend, but I'm very, I believe, I don't know if I believe, but I draw a lot of strength from deep connections with, with other people and just a small number of people, just a really small number of people.

4 (1h 54m 59s):
That's when I moved to this country, I was really surprised how, like, there were these be these large groups of friends, quote unquote. But they, the, the, the, the depth of connection was not there at all. From my sort of perspective. Now I moved to the suburb of Chicago was Naperville. It's more like a middle class, maybe upper middle class. So it's like people that cared more about material possessions than deep human connection. So that added to the thing. But I, I drew more meaningful than almost anything else was from friendship early on. I had a best friend. His name was he, his name is UDA.

4 (1h 55m 42s):
I don't know how to say it in English. How do

3 (1h 55m 44s):
You say in Russian UDA,

2 (1h 55m 45s):
What's his last name? Do you remember if it was a

3 (1h 55m 49s):
Miracle of a UDA musical? So we just spent all our time together. There's, there's a, there's also a group of friends. Like, I dunno, it's like eight guys in Russia growing up. It's like, parents didn't care if you're coming back at certain hour. So it was spent all day, all night, this playing soccer, usually called football and just talking about life and all those kinds of things. Even at that young age, I think people in Russia and Soviet union grew up much quicker. I think the education system at the university level is world-class in the United States in terms of like really creating really big, powerful minds.

3 (1h 56m 39s):
At least they used to be. But I think that they aspire to that. But the education system for like, for younger kids in the Soviet union was incredible. Like they did not treat us as kids. We, the, the level of literature, Tulsa does the esky

2 (1h 56m 55s):
When you were a small child. Yeah.

3 (1h 56m 57s):
Amazing. And like the level of mathematics, and you're made to feel like shit, if you're not good at mathematics, like we, I think in this country, there's more like, especially young kids. Cause they're so cute. Like they're being baby read. We only start to really push adults later in life. Like, so if you want to be the best in the world at this, then you get to be pushed. But we were pushed at a young age. Everybody was pushed and that brought out the best in people. I think it really forced people to discover, like discover themselves in the Goggin style, but also discover what they're actually passionate about what, they're not

2 (1h 57m 36s):
This true for boys and girls. Were they pushed equally there?

3 (1h 57m 39s):
Yeah. They were pushed. Yeah. They were pushed equally. I would say there was a, obviously there was more, not obviously, but there at least from my memories more of what's the right way to put it, but there was like gender roles, but not in a negative connotation. It was, it was the red dress versus the suit and tie kind of connotation, which is like, there's, you know, like guys like lifting heavy things and girls like creating beautiful art and you know, like

2 (1h 58m 14s):
There's a more traditional view of gender Maura, 1950 sixties.

3 (1h 58m 18s):
We didn't think in terms of, at least at that age, in terms of like roles and then like a homemaker or something like that or not, it was more about what people care about. Like girls cared about this set of things. And guys cared about the set of things. I think mathematics and engineering was something that guys cared about and sort of at least my perception of that time. And then creative

4 (1h 58m 42s):
Girls cared about beauty. So like guys want to create machines. Girls want to create beautiful stuff. And now of course that I don't take that forward in some kind of philosophy of life, but it's just the way I grew up in the way I remember it. But all everyone worked hard. The, the value of hard work was instilled in everybody. And through that, I think it's a little bit of hardship. Of course also economically everybody was poor, especially with the collapse of the Soviet union. There's poverty everywhere. You didn't notice it as much, but there was a, because there's not much material possessions.

4 (1h 59m 24s):
There was a huge value placed on human connection. Just a meeting with neighbors. Everybody knew each other. We lived in an apartment building very different than you have in the United States. These days, everybody knew each other, you know, you would get together, drink vodka, smoked cigarettes and play guitar and sing sad songs about, about life.

2 (1h 59m 47s):
What's it, what's with the, the sad songs and the Russian thing. I mean, I I've Russians at, do I express joy from time to time? Certainly you do, but w what do you think that's about? Is it cause it's cold there, but it's cold other places too,

4 (2h 0m 3s):
Right? I think let's just the first of all the Soviet union the echoes of world war II and the millions and millions and millions of people that civilians that were slaughtered and also starvation is there. Right? So like the echoes of that, of the ideas, the literature, the art is there. Like that's a grandparents, that's parents, that's all there. So that contributes to it. That life can be absurdly unexplainably, cruel at any moment, everything can change. So that's in there then I think there's an empowering aspect to finding beauty in suffering that then everything else is beautiful too.

4 (2h 0m 45s):
Like if you just linger or it's like, why you meditate on death is like, if you just think about the worst possible case and find beauty in that, then everything else is beautiful too. And so you write songs about the dark stuff and that somehow helps you deal with whatever comes there. There's a hopelessness to the, the Soviet union that like, you know, inflation, all those kinds of things where people or a soul dreams and never delivered. And so like, there's a D there's a, if you don't sing songs about sad things, you're going to become cynical about this world. Hmm. Interesting. So they don't want to give in to cynicism.

4 (2h 1m 27s):
Now, a lot of people did, you know, one of the, but that is the battle against cynicism. One of the things that may be common in Russia is a kind of cynicism about,

3 (2h 1m 41s):
Like, if I told you the thing I said earlier about dreaming about robots, it's very common for people to dismiss that dream of saying, no, that's not, that's too wild. Like, who else do you know that did that? Or you want to start a podcast? Like who else? Like nobody's making money on podcasts? Like, why do you want to start a pocket? That kind of mindset I think is quite common, which is why I would say entrepreneurship in Russia is still not very good, which to be a business, to be an entrepreneur, you have to dream big and you have to have others around you like friends and support group that makes you make you dream big. But if you don't give in to cynicism and appreciate the beauty in the, the unfairness of life, the absurd unfairness of life, then I think it just makes, makes you appreciative of everything.

3 (2h 2m 34s):
It's like a, it's a prerequisite for gratitude. And so, yeah, the, I think that instilled in me ability to appreciate everything, just like everything everything's amazing. And then also there is a culture of romance of like romanticizing everything. Like it's almost like, like romantic relationships were, were very like soap, opera, like is very like over the top dramatic. And I think, I think that it was instilled in me to not only do I appreciate everything about life, but I get like emotional about it in a sense, like, I get like a visceral feeling of joy for everything and the same with, you know, friends or people of the opposite sex.

3 (2h 3m 26s):
Like there's a deep, like emotional connection there that like, that's like way too dramatic to like, I guess, relative to what the actual moment is. But I derive so much deep, like dramatic joy from so many things in life. And I think I would attributed that to the upbringing in Russia. But the thing that sticks most of all is the friendship and have now since then had one other friend like that in, in the United States, he lives in Chicago. His name is Matt and slowly here.

3 (2h 4m 7s):
And they're accumulating really fascinating people, but I'm very selective with that. Funny enough, the few times, you know, it's not few it's a lot of times now interacting with Joe Rogan, it's sounds surreal to say, but there was a kindred spirit there, like F connected with him. And there's been people like that also in the grappling sports that are really connected with I've actually struggled, which is why I'm. So, I'm so glad to be your friend is have struggled

4 (2h 4m 38s):
To connect with scientists.

2 (2h 4m 40s):
Like they can be a little bit wooden sometimes. Yeah. Even the biologists. I mean, one thing that I, well, I'm so struck by the fact that you, you know, you work with robots, you're an engineer, AI, you know, science technology. And that all sounds like hardware. Right. But what you're describing, and I know is true about you is this deep emotional life and this resonance and it's, it's really wonderful. I actually think it's one of the reasons why so many people, scientists and otherwise have gravitated towards you and your podcast is because you hold both elements, you know, in the Herman Hesse's book. I don't know if you were at narcissists and Goldman, right? It's about these elements of the logical rational mind and the, the, the emotional mind and how those are woven together.

2 (2h 5m 23s):
And if people haven't read it, they should, and you embody the full picture. And I think that's so much of what draws people to you. I read every Herman has said book, as usual as usual, I've done about 9% of wetlands, is it? No, it's true. You will. You mentioned Joe, who is a phenomenal human being, not just for his amazing accomplishments, but for how he shows up to the world. One-on-one I think I heard him say the other day on an interview, he said, there is no public or private version of him. He was like, this is, this is me. He said that it was beautiful. He said, I'm like the fish that got through the net, you know, there is no onstage offstage version.

2 (2h 6m 4s):
And you're absolutely right. And I, I so, well, you guys, I have a question actually about

4 (2h 6m 10s):
A really good point about public and private life. He was a huge, if I could just comment real quick like that, he was, I've been a fan of Joe for a long time, but he's been an inspiration to, to not have any difference between public and private life. I actually had a conversation with Neval about this. And he said that you can't have a rich life like exciting life if you're the same person publicly and privately. And I think I understand that idea, but I don't agree with it. I, I think that's really fulfilling and exciting to be the same person privately and publicly with very few exceptions.

4 (2h 6m 52s):
Now that said, I don't have any really strange sex kinks. So like, I feel like it can be open with basically everything. I don't have anything I'm ashamed of. You know, there's some things that could be perceived poorly, like the screaming Roombas, but I'm not ashamed of them. I just have to present them in the right context. But there is a there's freedom to being the same person in private as in public. And that Joe made me realize that you can, you can be that. And also to be kind to others, it sounds, it sounds kind of absurd, but I really, I really always enjoyed like being good to

3 (2h 7m 36s):
Others. Like just being kind towards others. But I always felt like the world didn't want me to be like, there's so much negativity when I was growing up. Like just the wrong people. If you actually just notice how people talk, they from like complaining about the weather, this could be just like the big cities that I've visited, but there's a general negativity and positivity is kind of a suppressed. You're not one you're not seen as, as very intelligent and two, there's a kind of, you're seen as like a little bit of a weirdo. And so I always felt like I had to hide that.

3 (2h 8m 16s):
And what Joe made me realize one, I have, I could be fully just the same person private in public and two, I can embrace being kind. And just in the way that I like in the way I know how to do and sort of for me on like on Twitter or like publicly, whenever I say stuff, that means saying stuff simply almost to the point of cliche. And like, I have the strength now to say it, even if I'm being mocked, you know what I mean? Like just it's okay if everything's going to be okay. Okay. Some people will think you're dumb. They're probably right. The point is like, just enjoy being yourself and that Joe, Joe, more than almost anybody else, because he's so successful at, it inspired me to do that.

3 (2h 9m 3s):
Be kind and be the same person, private and public.

2 (2h 9m 6s):
I love it. And I love the idea that authenticity doesn't have to be oversharing, right. That it doesn't mean you reveal every detail of your life. What, you know, it, it's a way of being true to an essence of oneself, right? Yeah.

3 (2h 9m 20s):
Not, there's never a feeling when you deeply think and introspect that you're hiding something from the world, are you being dishonest in some fundamental way? So yeah, that, that that's truly liberating. And it allows you to think it allows you to like think freely to speak freely to just to be freely that said, it's not like, you know, it's not like there's not still a responsibility to be the best version of yourself. So, you know, I'm very careful with the way I say something. So the whole point it's, it's not so simple to express the spirit that's inside you with words.

3 (2h 10m 5s):
It depends. I mean, some people are much better than, than others. I struggle. Like oftentimes when I say something and I hear myself say it, it sounds really dumb and not at all what I meant. So that's the responsibility you have. It's not just like being the same person publicly and privately means. You can just say whatever the hell it means. There's still responsibility to try to be, to express who you truly are. And that that's, that's, that's hard. It is. And I think

2 (2h 10m 34s):
That, you know, we have this pressure, all people, when I say we, I mean, all, all humans, maybe robots to feel this pressure, to be able to express ourselves in that one moment in that one form. And it is beautiful, beautiful. When somebody, for instance, can capture some essence of love or sadness or anger or something in a song or in a poem or in a short quote. But perhaps it's also possible to do it in aggregate, you know, all, all the things, you know, how you show up you're for instance, one of the things that initially drew me to want to get to know you as a human being and a scientist, and eventually we became friends was the level of respect that you brought to your podcast listeners by wearing a suit.

2 (2h 11m 19s):
Yeah. I'm being serious here. You know, I was raised thinking that if you overdress a little bit overdressed by American, certainly by American standards, you're overdressed for a podcast, but this is, but it's genuine. You're not doing it for any reason, except I have to assume. And I assumed at the time that it was because you have a respect for your audience, you respect them enough to show up a certain way for them. It's for you also, but it's for them. Yeah. And I think between that and your commitment to your friendship, the way that you talk about friendships and love and the way you hold up these higher ideals, I think at least as a consumer of your content and as your friend, I, what I find is that in aggregate, you're communicating who you are.

2 (2h 12m 3s):
It doesn't have to be one quote or something. And I think that, you know, we, we're sort of obsessed by like the one Einstein quote or the one line of, of poetry or something. But it's the, I think you, so embody the way that and Joe as well, it's about how you live your life and how you show up as a collection of things and said, and done.

4 (2h 12m 25s):
Yeah. That that's fast. And so the aggregate is, is the goal. The, the tricky thing, and Jordan Peterson talks about this because he's under attack way more than you and I will ever be. But that for now, right? This is very true for now that the people who attack on the internet, this is one of the problems with Twitter is they don't consider the aggregate. They, they take a single statements. And so one of the defense mechanisms like again, why Joe has been an inspiration is that when you in aggregate are a good person, a lot of people will know that.

4 (2h 13m 7s):
And so that makes you much more immune to the attacks of people that bring out an individual statement that might be a misstatement of some kind or doesn't express who you are. And so that I, I like that idea. That is the aggregate and the, the, the power of the podcast is you have hundreds of hours out there and being yourself and people get to know who you are. And once they, and you, and you post pictures of screaming room buzz, as you kick them, they will understand that you don't mean well, by the way, as a side comment, I don't know if I want to release this because it's not just the Roombas.

2 (2h 13m 47s):
You have a whole dungeon of robots.

4 (2h 13m 48s):
Okay? So this is, this is a problem. The Boston dynamics came up against this problem. But let me just let me work this out, like workshop this out with you. And maybe because we'll post this people let me know. So there's legged robots. You know, they look like a dog. They have a very, I'm trying to create a very real human robot connection. But like, they're also incredible because you can throw them like off of a building and they'll land fine. And this beautiful,

2 (2h 14m 22s):
That's amazing. I've seen the Instagram videos of like cats getting jumping off of like fifth story buildings and then walking away, no one should throw their cat.

4 (2h 14m 31s):
Well, this is the problem I'm experiencing all, certainly kicking the robots. It's really fascinating how they recover from those kicks, but like just seeing myself do it and also seeing others do it. It just does not look good. And I don't know what to do with that because it's such a fluent. I see. But you don't, I, you, cause you,

2 (2h 14m 53s):
No, I'm kidding. I'm now, now I'm, you know, what's interesting. Yeah. Before today's conversation, I probably could do it. And now I think I'm thinking about robots, bills of rights and things. I'm actually, and not, not for any, not to satisfy you or to satisfy anything, except that if I, if they have some sentience aspect to their being, then I would it to kick.

4 (2h 15m 16s):
I don't think we'd be able to kick it. You might be able to kick the first time, but not the second. This, this is the problem of experience. One of the cool things is one of the robots I'm working with. You can pick it up by one leg and is dangling. You can throw it in any kind of way and it'll land correctly. So it's really at a cat like that, man, we look forward to the letters,

2 (2h 15m 41s):
Oh no, I'm not suggesting anyone did that. But his, he had this cat and the cat, he would just, you know, throw it onto the bed from across the room. And then it would run back for more somehow they had, that was the nature of the relationship. I think most, no one should do that to an animal, but a pet cat seemed to, you know, return for it.

4 (2h 15m 58s):
There's the robots, the robot. It's fascinating to me how hard it is for me to do that. So it's unfortunate, but I don't think I can do that to a robot. Like I, I struggle with that. So for me to be able to do that with a robot, I have to almost get like into the state that I imagine like doctors get into, when they're doing surgery, like I have to start, I have to do what robotics colleagues of mine do, which is like, start seeing it as an object associated like dissociate. So it was just fascinating that I have to do that in to do that with a robot. I just wanted to take that little bit of a tangent.

2 (2h 16m 34s):
No, I think it's an important thing. I mean, I am not, I'm not shy about the fact that for many years, I've worked on experimental animals and that's been a very challenging aspect to being a biologist, mostly mice, but in the past no longer, thank goodness because I just don't like doing it larger animals as well. And now I work on humans, which I can give consent verbal consent. So I think that it's extremely important to have an understanding of what the guidelines are and where one's own boundaries are around this. It's it's, it's not just an important question. It might be the most important question before any work can progress.

4 (2h 17m 13s):
So you, you asked me about friendship. I know you have a lot of thoughts about friendship. What do you think is the value of friendship in life?

2 (2h 17m 22s):
Well, for me personally, just because of my light, my life trajectory and arc of friendship, and I should say, I do have some female friends that are just friends. They're completely platonic relationships, but it's been mostly male friendship to me has been,

4 (2h 17m 40s):
Has been all male friendships may, actually, huh?

2 (2h 17m 42s):
Interesting. Yeah. It's been a, an absolute lifeline. They are my family. I have a biological family and I have great respect and love for them and appreciation for them, but it, it provide it's provided me the, I won't even say confidence because there's always an anxiety and taking any good risk or any risk worth taking. It's given me the sense that I should go for certain things and try certain things to take risks, to, to weather that anxiety. And I, I don't consider myself a particularly competitive person, but I would sooner die than disappoint or let down one of my friends, but I can think of nothing worse actually than disappointing.

2 (2h 18m 28s):
One of my, my friends, everything else is secondary to me.

4 (2h 18m 31s):
Well, disappointment

2 (2h 18m 33s):
Disappointing, meaning not, I mean, certainly I strive always to show up as best I can for the friendship. And that can be in small ways. That can mean, you know, making sure the phone is away. Sometimes it's about, I'm terrible with punctuality because I'm an academic. And so I just get lost in time and I don't mean anything by, but it's striving to, to listen, to, to enjoy good times and to make time, you know, it kind of goes back to this first variable. We talked about to make sure that I spend time and to get time in person and check in. And it it's, I think there's so many ways in which friendship is vital to me. It's it's actually, to me, what makes life worth living?

4 (2h 19m 15s):
Yeah. Well, there's a, I am surprised like with the high school friends, how we don't actually talk that often these days in terms of time, but every time we see each other, it's immediately right back to where we started. So I, you know, I struggled that how much time you really allocate for the, for the friendship to be deeply meaningful, because they're just, they're always there with me, even if we don't talk often. So there's a kind of loyalty. I think maybe it's a different style, but I think I'm much more to me. Friendship is being there in the hard times. I think like, I I'm much more reliable when you're going through shit.

4 (2h 19m 56s):
Then, then like you're pretty reliable anyway. No, but if like you're, if you're like, like a wedding or something like that, or like, I don't know, like you want an award of some kind like, yeah, the I'll congratulate the, the shit out of you, but like that's not, and I'll be there, but that's not as important to me as being there when like nobody else is like just being there when shit gets a shit, hits the fan, or something's tough where the world turns their back on you, all those kinds of things. That, to me, that's what friendship is.

2 (2h 20m 29s):
Well, I know that to be true about you and that's a felt thing and a real thing with you. Let me ask you one more thing about that actually, because I'm not a practitioner jujitsu, I know you are, Joe is, but years ago I read a book that I really enjoyed, which is Sam Sheridan's book a fighter's heart. He talks about all these different forms of martial arts and, and maybe it was in the book, maybe it was in an interview, but he said that, you know, fighting or being in physical battle with somebody jiu-jitsu boxing or some other form of physical direct physical contact between two individuals creates this bond, unlike any other, because he said, it's, it's like a one night stand. You're sharing bodily fluids with somebody that you barely know.

2 (2h 21m 10s):
Yeah. And I, you know, and I chuckled about it cause it's, it's kind of funny and it kind of tongue in cheek, but at the same time, I think this is a fundamental way in which members of a species bond is through physical contact. And certainly there are other forms there's cuddling and there's hand holding and there's and there's sexual intercourse. And there's also,

4 (2h 21m 30s):
What's cuddling. I haven't heard of it. I heard it

2 (2h 21m 33s):
Recently. I didn't know this term, but there's a term they've turned the noun cupcake into a verb. Cupcaking it turns out, I just learned about this cupcaking is when you spend time, just cuddling. I didn't know about this. You heard it here first. Although I heard it first, just the other day cupcaking is that

4 (2h 21m 50s):
Cuddling is everything. It's not just like, is it in bed or is it on the college? Like what's cuddling. I had to look up what cooling.

2 (2h 21m 56s):
We need to look at this up and we need to define the variables. I think it definitely has to do with physical contact. I been told, but, but in terms of battle competition, you know, and the Sheraton quote, I'm just curious. So do you get close or feel a bond with people that, for instance, you roll jujitsu with, or even though you don't know anything else about them, is he, was he right about, yeah.

4 (2h 22m 26s):
I mean, on many levels, he also has the book. What a fighter's mind. Yeah.

2 (2h 22m 31s):
He's, he's actually an excellent writer. What's interesting about him just briefly about Sheridan. I don't know him, but I did a little bit of research. He, he went to Harvard, he was an art major at Harvard. He claims all he did was smoke cigarettes and do art. I don't know if his art was any good and, and I think his father was in the seal teams. And then when he got out of Harvard graduated, he took off around the world, learning all the forms of martial arts and was early to the kind of ultimate fighting kind of mixed martial arts and things. Great. Great.

4 (2h 22m 60s):
Yeah. It's, it's amazing. I don't actually remember it, but I read it and I remember thinking there was an amazing encapsulation of what makes fighting the, like the art, like what makes it compelling. I would say that there's so many ways that jujitsu grappling wrestling, combat sports in general is like one of the most intimate things you can do. I don't know if I would describe it in terms of bodily liquids and all those kinds of things. I think he was more or less joking Japan. I think there's a few ways that it does that. So one, because you're so vulnerable so that the honesty of stepping on the mat and often all of us have ego thinking we're better than we are at this particular art.

4 (2h 23m 52s):
And then the honesty of being submitted or being worse than you thought you are. And just sitting with that knowledge, that kind of honesty. We don't get to experience it in most of daily life. We can continue living somewhat of an illusion of our conceptions of ourselves because people are not going to hit us with the reality. The mat speaks only the truth that the reality just hits you. And that vulnerability is the same as like the loss of a loved one though. It's the loss of a reality that you knew before you now have to deal with this new reality. And when you're sitting there in that vulnerability and there's these other people that are also sitting in that vulnerability, you get to really connect like, fuck, like I'm not as special as I thought I was.

4 (2h 24m 40s):
And life is like not, you know, life is harsher than I thought I was. And we're just sitting there with that reality. Some of us can put words to them. Some we can't. So I think that definitely is a thing that leads to intimacy. The, the other thing is, is the human contact. There's something about, I mean like a big hug, like during COVID very few people hugged me and I hugged them and I always felt good when they did like, were all tested and especially now we're vaccinated, but there's still people. This is true of San Francisco. Is this true in Boston? They want to keep not only six feet away, but stay at home and never touch you.

4 (2h 25m 22s):
That, that was that loss of basic humanity is the opposite of what I feel in jujitsu, where it was like that, that contact where you're like, I don't give a shit about whatever rules we're supposed to have in society where you're not, you have to keep a distance and all that kind of stuff. Just the hug like that, the intimacy of a hug, that's like a good bear hug. And you're like just controlling another person. And also there is some kind of love communicating through just trying to break each other's arms. I don't, I don't exactly understand why violence is the such a close neighbor to love, but it is like,

2 (2h 26m 3s):
Well in, you know, in the hypothalamus, the neurons that control sexual behavior, but also non-sexual contact are not just nearby the neurons that control aggression and fighting. They are salt and pepper with those neurons. It's a very interesting, and you know, it almost sounds kind of risque and controversial and stuff. There's, I'm not anthropomorphizing about what this means, but in the brain, those structures are interdigitated. They, they, you can't, you can't separate them except at a very fine level. And here your, the way you describe it as the same as a real thing,

4 (2h 26m 40s):
I, I do want to make an interesting comment. Again, these are the things that could be taken out of context, but, you know, I, one of the amazing things about jujitsu is both guys and girls train it. And I was surprised. So like, I'm a big fan of yoga pants, you know, at the gym kind of thing. It reveals the beauty of, of the female form. But the thing is like girls are, you know, dressed in skintight clothes and you just often, and I found myself, like not at all thinking like that at all, with when training with girls,

2 (2h 27m 15s):
Well, the context is very non-sexual, but

4 (2h 27m 17s):
Th that I was surprised to learn that like I, when I first started to jets, so I thought, wouldn't it be kind of weird to train with the opposites that like, in something so intimate

2 (2h 27m 26s):
Boys and girls, men and women, they, they rolled jujitsu

4 (2h 27m 30s):
Together completely interesting. And the only times girls kinda try to stay away from guys. I mean, there's two contexts, of course, there's always going to be creeps in this world. So everyone knows who, you know, who kind of to stay away from. And the other is like, there's a size disparity. So girls will often try to roll with people a little bit closer weight-wise but no, there that's one of the things that are empowering to women. That's what they fall in love with when they start doing jujitsu is I can, first of all, they gain an awareness and a pride over their body, which is great. And then second, they get, especially later on start submitting big dudes like these like bros that come in, who are all shredded and like muscular.

4 (2h 28m 11s):
And they get to technique to exercise dominance over them. And that's a powerful feeling.

2 (2h 28m 18s):
You've seen women force a larger guy to, or even choking. Well, I was,

4 (2h 28m 24s):
I was dead lifting F four, oh boy, I think it's 4 95. So I was really into power lifting when I started ju jujitsu. And I remember being submitted, but, you know, I thought I walked in feeling like I'm going to be, if not the greatest fighter ever at least top three, as so as a white belt, you roll in like all happy. And then you realize that as long as you're not applying too much force that you're having, I remember being submitted many times by like 130, 120 pound girls at dusk balanced studios in Philadelphia, that a lot of incredible female jujitsu players. And that's really humbling to that technique can overpower in, in combat pure strength.

4 (2h 29m 12s):
And that I that's the other thing, there is something about combat that's primal. Like there, it just feels, it feels like we were born to do this like that.

2 (2h 29m 27s):
We have circuits in our brain that are dedicated to this kind of interaction. There's no, there's no question.

4 (2h 29m 34s):
That's what it felt like. It w it wasn't that I'm learning a new skill. It was like, somehow I am remembering echoes of something I've learned in the past. Right.

2 (2h 29m 44s):
It's like hitting puberty, a child before puberty has no concept of boys and girls having this attraction, regardless of whether or not they're attracted to boys or girl doesn't matter at some point, most people, not all, but certainly, but most people, when they hit puberty, suddenly people appear differently and certain people take on a romantic or sexual interest for the very first time. Yeah. And so it's like, it's revealing a circuitry in the brain. It's not like they learn that it's innate. And I think it, when I hear the way you describe jujitsu and enrolling jujitsu, it reminds me a little bit, Joe was telling me recently about the first time he went hunting and he felt like it revealed a circuit.

2 (2h 30m 27s):
That was, that was in him all along. But he hadn't experienced before.

4 (2h 30m 31s):
Yeah. That's, that's definitely there. And, and of course there's the physical activity. One of the interesting things about jujitsu is a, it's one of the really strenuous exercises that you can do late into your adult life, like into your 50, 60 seventies, eighties. I've when I, when I came up, there's a few people in their eighties that were training. And as long as you're smart, as long as you practice techniques and pick your partners correctly, you can do that kind of art. That's late into life. And you, so you're getting exercise. There's not many activities I find that are amenable to that. So, because it's such a thinking game that the jujitsu in particular is an art or technique pays off a lot.

4 (2h 31m 13s):
So you can still maintain, first of all, remain injury free. If you use good technique and also through good technique, be able to go, you know, be active with people that are much, much younger. And so it, that was to me, that, and running are the two activities you can kind of do late in life because to me, a healthy life is, has exercises as a piece of the puzzle.

1 (2h 31m 40s):
Absolutely. And I'm glad that we're on the physical component because I know that there's for you, you've talked before about the crossover between the physical and the intellectual and the mental. And are you still running at ridiculous hours of the night for ridiculously long?

4 (2h 31m 60s):
Yeah, so definitely I've been running late at night here in Austin. People tell me the area we're in. Now. People say it's a dangerous area, which I find laughable coming from the bigger cities. No, I run late at night. There's something,

1 (2h 32m 15s):
If you see a guy running through Austin at 2:00 AM in a suit and tie, it's

4 (2h 32m 19s):
Probably, well, yeah. I mean, I do think about that because I get recognized more and more in Austin. I, I worry that not really that I get recognized late at night, you know, but there is something about the night that brings out those deep philosophical thoughts and self-reflection that I really enjoy. But recently I started getting back to the grind. So I'm going to be competing or hoping to be compete in September and October. So yeah. To get back to competition. And so that requires getting back into a great cardio shape. I said, I've been getting a running as part of my daily routine.

1 (2h 33m 2s):
Got it. Yeah. Well, I always know I can reach you regardless of time zone in the middle of the night, wherever that happens to me,

4 (2h 33m 9s):
Part of that has to be just being single and being a programmer. So those two things just don't work well in terms of a steady sleep schedule.

1 (2h 33m 18s):
It's not bankers hours kind of work nine to five. I want to, you mentioned single. I want to ask you a little bit about the other form of relationship, which is a romantic love. So your parents are still married, still married, still happily married. That's impressive. Yeah. A rare thing nowadays. Yeah. So you grew up with that example.

4 (2h 33m 39s):
Yeah. I guess that's a powerful thing, right? If there's an example that I think can work,

1 (2h 33m 43s):
It, it, yeah. I didn't have that in my own family, but when you, when I see it, it's, it's inspiring and it's, and it's beautiful. The fact that they have that, and that was the norm for you, I think is really,

4 (2h 33m 57s):
Oh, it was in the case of my parents. It was interesting to watch because there's obviously tension. Like, there'll be times when they fought and all those kinds of things. They, they obviously get frustrated with each other and they like, but they find mechanisms how to communicate that to each other, like to make fun of each other a little bit like the teas to get some of that frustration out and then ultimately to reunite in a, in to find their joyful moments and be that the energy, I think it's clear. Cause I got together in there. I think early twenties, like very, very young. I think you grow together as people.

2 (2h 34m 32s):
Yeah. You're still in the critical period of brain plasticity.

4 (2h 34m 37s):
And also, I mean, it's just like divorce was so frowned upon that you stick it out. And I think a lot of couples, especially from that time and the Soviet union, that's probably applies to a lot of cultures. You stick it out and you put in the work, you learn how to put in the work. And once you do, you start to get to some of those rewarding aspects of being like through time has sharing so many moments together. You know, that that's definitely something that, that was an inspiration to me, but maybe that's where I have. So I have a similar kind of longing to have a lifelong partner like that have that kind of view where same with friendship, lifelong friendship is the most meaningful kind that there is something with that time of sharing all that time together, like till death do us part as a powerful thing.

4 (2h 35m 27s):
Not by force, not because of the religion said it or the government said it or your culture said it, but because you want to, do you want children? Definitely. Yeah. Definitely want children. It's it's comedy room was, do you have, oh, I thought you should know that

2 (2h 35m 43s):
Human children now human to human child. Cause I already have the children. Exactly what I was saying. You probably need at least as many, a human children as you do remote big family, small family.

4 (2h 35m 53s):
So

2 (2h 35m 53s):
In your mind's eyes, they're a big bunch of bunch of Friedman's running around.

4 (2h 35m 59s):
So I'll tell you like realistically I can explain exactly my thinking is, and this is similar to the robotics work is if I'm like purely logical right now, my answer would be, I don't want kids because I just don't have enough time. I have so much going on. But when I'm using the same kind of vision I use for the robots is I know my life will be transformed with the first, like I know I would love being a father. And so the question of how many I, that that's on the other side of that hill, it could be some ridiculous number. So I just know that I have

2 (2h 36m 37s):
Feeling and I could be, I don't have a crystal ball, but I dunno. I S I see an upwards of, of certainly three or more come comes to mind.

4 (2h 36m 48s):
So much of that has to do with the partner you're with too. So like that, that's such an open question, especially in this society of what the right partnership is. Cause I'm, I'm, I'm, I'm, I'm deeply empathetic. I want to see, like, to me, what I look for in your relationship is for me to be really excited about the passions of another person, like whatever they're into, it doesn't have to be a career success and you kind of success just to be excited for them and for them to be excited for me. And they share in that excitement and build and build and build, but there was also practical aspects of like, what kind of shit do you enjoy doing together?

4 (2h 37m 29s):
And I think family is a real serious undertaking.

1 (2h 37m 33s):
It certainly is. I mean, I think that I have a friend who said it, I think best, which is that you first have gays in a very successful relationship and, and has a family. And he said, you first have to define the role. And then you have to cast the right person for the role.

4 (2h 37m 51s):
Well, yeah, there, there, there's some deep aspects of that, but there's also an aspect to which you're not smart enough from this side of it to define the right, to define the role. I think there's part of it that has to be a leap that you have to take. And I see, I S I see having kids that way, you just, you just have to go with it and figure it out. Also, as long as there's love there, like what the hell is life for even? So I've, I've, there's so many incredibly successful people that I know that I've gotten to know that all have kids and the presence of kids for the most part has only been something that energize them, something they gave a meaning, something that made them the best version of themselves, like made them more productive, not less, which is fascinating to me.

1 (2h 38m 44s):
It is fascinating. I mean, you can imagine if the way that you felt about Homer, the way that I feel and felt about Costello is, is at all a, a glimpse of what that must be like then exactly.

4 (2h 38m 55s):
You know, the, the downside, the thing I worry more about is the, the, the partner side of that I've seen, the kids are almost universally a source of increased productivity and joy and happiness. Like, yeah, they're a pain in the ass. He has complicated. Yeah. So, so on and so forth, people like to complain about kids, but then when you actually look past that little shallow layer of complaint, kids are great. The source of pain for a lot of people is the, if, when the relationship doesn't work. And so I'm very kinda concerned about, I, you know, dating is very difficult and I'm a complicated person.

4 (2h 39m 37s):
And so it's very, it's been very difficult to find, to find the right kind of person, but that statement doesn't even make sense because I'm not on dating apps. I don't see people. You're like the first person I saw in awhile. It's like you and Michael malice and like Joe. So like, I don't think I've seen like females, what is it an element of the female species in quite an quite a while. So I think you have to put yourself out there. What is it? Daniel Johnston says, true love will find you, but only if you're looking. So there's some element of really the leap and putting yourself out there and kind of different situations.

4 (2h 40m 17s):
And I don't know how to do that when you're behind a computer all the time.

2 (2h 40m 20s):
Well, you're a, you're a builder and you're a problem solver. And you're you find solutions and I'm confident this solution is the solution is out there. And

4 (2h 40m 34s):
I think you're implying that I'm going to build the girlfriend, which

2 (2h 40m 38s):
I think, well, and maybe we shouldn't separate this friendship, the notion of friendship and community, and the act. If we go back to this concept of the aggregate, you know, maybe you'll meet this woman through, through a friend or maybe you'll or something to that.

4 (2h 40m 54s):
So one of the things, I don't know, if you, if you feel the same way, I definitely one of those people that just falls in love and that's

2 (2h 41m 2s):
Yeah. I can't say I'm like that with Costello. It was instantaneous. Yeah. It really was. I mean, I know it's not, it's not romantic love, but it was instantaneous. No, I, I, but that's me, you know, and I think that if, you know, you know, because that, that's a, that's a good thing that you have that, well,

4 (2h 41m 19s):
It's a, I'm very careful with that because you don't want to fall in love with the wrong person. So I try to be very kind of careful with, I I've noticed this because I fall in love with every, like this mug, everything I fall in love with things in this world. So like, you have to be really careful because a girl comes up to you and says she loves dusty Husky. That doesn't necessarily mean

2 (2h 41m 45s):
Marry her tonight. Yes. And I liked the way you said that out loud so that you heard it, it doesn't mean you need to marry here tonight.

4 (2h 41m 52s):
Right, exactly. Right. But I mean, but people, people are amazing and people are beautiful and that's, so I I'm fully embraced that, but I also, you have to be careful with relationships. And at the same time, like I mentioned to you offline, I, I don't, there's something about me that appreciates swinging for the fences and not dating, like doing serial dating or dating.

2 (2h 42m 16s):
You're a one guy, one girl kind of guy. Yeah. He said that

4 (2h 42m 19s):
It's, it's tricky because you want to, you want to be careful with that kind of stuff. Especially now there's a growing platform that have a ridiculous amount of female interests of, of a certain kind. But I'm looking for deep connection and, and I'm looking by sending home alone and every once in a while talking to Stanford professors.

2 (2h 42m 41s):
Perfect. Perfect solution workout. It's well, incorporated, it's part of that constitutes machine learning of sorts. Yeah. I'm sorry. I do. You mentioned what has now become a quite extensive and expansive public platform, which is incredible. I mean, the number of people, when I, first time I saw your podcast, I noticed the suit. I was like, he respects his audience, which was great, but I also thought this is amazing. You know, people are showing up for science and engineering and information

1 (2h 43m 12s):
And those discussions and other sorts of discussions. Now I do want to talk for a moment about the podcast. So my two questions about the podcast are when you started it, did you have a plan? And regardless of what that answer is, do you, do you know where you're taking it or would you like to leave us? I do believe in an element of surprise is always fun. But what about the podcast? Do you enjoy the podcast? I mean, your audience certainly includes me, really enjoys the podcast. It's incredible.

4 (2h 43m 43s):
So I love talking to people and there's something about microphones that really bring out the best in people like you do. You don't get a chance to talk like this, if you and I were just hanging out, we would have a very different conversation in the amount of focus we allocate to each other. It would be having fun, talking about other stuff and doing other things. There'll be a lot of distraction. There would be some phone use and all that kind of stuff. But here we're 100% focused on each other and focus on the idea and like sometimes playing with ideas that we both don't know, like the answer to like a question we don't know the answer to.

4 (2h 44m 23s):
We're both like fumbling with it, trying to figure out, trying to get some insights. That's something we haven't really figured out before and together arriving at that. I think that's magical. I don't know why we need microphones for that, but we somehow do. It feels like doing science. It feels like doing science for me. Definitely. That's exactly it then. And I'm really glad you said that because I don't actually often say this, but that's exactly what I felt like. I wanted to talk to friends and colleagues at MIT to do real science together. That's how I felt about it. Like to really talk through problems that are actually interesting, as opposed to like incremental work that we're currently working for for a particular conference.

4 (2h 45m 11s):
So it really asking questions, like, what are we doing? Like, where's this headed to like, what are the big, is this really going to help us solve in the case of AI solve intelligence? Like, is this even working on intelligence? There's a, there's a certain sense, which is why I initially called it. Artificial intelligence is like, most of us are not working on artificial intelligence. You're, you're working on some very specific problem and a set of techniques at the time it's machine learning to solve this particular problem. This is not going to take us to a system that is anywhere close to the generalizability of the human mind.

4 (2h 45m 51s):
Like the kind of stuff the human mind can do in terms of memory, in terms of cognition, in terms of reasoning, common sense reasoning. This is doesn't seem to take us there. See the initial impulse was, can I talk to these folks, do science together through conversation. And I also thought that there was not enough Not, I didn't think there was enough good conversations would world-class minds that I got to meet and not the ones with the book or the, this was the thing. Oftentimes you go on this tour when you have a book, but there's a lot of minds that don't write books and the books constrain the conversation too, because then you're talking about this thing. It's the same book, but there's, I've, I've noticed that with people who haven't written a book who are brilliant, we get to talk about ideas in a new way.

4 (2h 46m 40s):
We both haven't actually, when we raise a question, we don't know the answer to it. One, the question is raised and we try to arrive there. Like, I dunno, I remember asking questions of world-class researchers in deep learning of why do neural networks work as well as they do that question is often loosely asked, but like when you have microphones and you have to think through it and you have 30 minutes to an hour to think through it together, I think that's that science. I think that's really powerful. So that was, that was the one goal. The other one is

3 (2h 47m 23s):
I,

4 (2h 47m 24s):
And again, don't usually talk about this, but there's some sense in which I want it to have dangerous conversations.

3 (2h 47m 32s):
Part

4 (2h 47m 33s):
Of the reasons I wanted to wear a suit is like, I want it to be fearless. And that the reason I don't usually talk about it is because I feel like I'm not good at conversation. So it looks like it doesn't match the current skill level, but I wanted to have

3 (2h 47m 49s):
Really

4 (2h 47m 52s):
Dangerous conversations that I uniquely would be able to do. Not completely uniquely, but like, I'm a huge fan of Joe Rogan. And I had to ask myself, what conversations can I do that Joe Rogan can't for me, I know I bring this up. But for me, that person I thought about at the time was Putin. Like that's, that's why I bring him up. He's he's just like with Costello, he's not just a person. He's also an idea to me, for what I strive for just to have those dangerous conversations and the reason I'm uniquely qualified as both the Russian, but also there's the judo and the martial arts.

4 (2h 48m 34s):
There's a lot of elements that make me have a conversation he hasn't had before. And, and there's a few other people that I kept in mind, like Don Knuth is a computer scientist from Stanford that I thought is one of the most beautiful minds

3 (2h 48m 53s):
Ever. And nobody really talked to him,

4 (2h 48m 58s):
Really talk to him. He did a few lectures, which people love, but really just have a conversation with him. There's a few people like that. One of them passed away, John Conway and it never got, we agreed to talk, but he died before we didn't. There's a few people like that, that I thought like it's such a crime to not hear those folks. And I have the unique ability to know how to purchase an microphone on Amazon and plug it into a device that records audio and then publish it, which seems relatively unique. Like it's, that's not easy in the scientific community. People knowing how to plug in a microphone.

2 (2h 49m 37s):
No, they can build Faraday cages and two photon microscopes, and a bioengineer, all sorts of things. But the idea that you could take ideas and export them into a structure or a pseudo structure that people would benefit from seems like a cosmic achievement to them. I don't know

4 (2h 49m 54s):
If it's a fear or just a basically they haven't tried it so they haven't learned the skill level, but

2 (2h 50m 1s):
I think they're not trained. I mean, we could riff on this for awhile, but I think that, but it's important. And maybe we should, which is that it's, they're not trained to do it. They're trained to think in specific games and specific hypotheses and, and many of them don't care to, right. They, they don't, they, they became scientists because that's where they felt safe. And so why would they leave that Haven of safety?

4 (2h 50m 26s):
Well, they also don't necessarily always see the value in it. It's, it's, we're all together learning you and I are learning the value of this. I think you're probably having an exceptionally successful and amazing podcast that you started just

2 (2h 50m 40s):
Recently dynasty your encouragement.

4 (2h 50m 42s):
Well, but there's, there's a raw skill there that that's a, you're definitely an inspiration to me in how you did the podcast in the, in the level of excellence you reach. But I think you've discovered that that's also an impactful way to do science that podcast. And I think a lot of scientists have not yet discovered that, that this is a, if they apply same kind of rigor as they do to academic publication or to even conference presentations and they do that rigor and effort to, to podcast, whatever that is, that could be a five minute podcast, a two hour podcasts, it could be conversational, or it can be more like lecture.

4 (2h 51m 22s):
Like if they apply that effort, you have the potential to reach over time, tens of thousands, hundreds of thousands, millions of people. And that's, that's really, really powerful. But yeah, for, for me giving a platform to a few of those folks, especially for me personally, so maybe you can speak to what fields you're drawn to, but I thought computer scientists were especially bad at this. So there's brilliant computer scientists that I thought it would be amazing to explore their mind, explore their thinking.

4 (2h 52m 2s):
And so that I took that almost as an on, as an effort and at the same time had other guests in mind or people that connect to my own interests. So the, the wrestling, wrestling, music, football, both American football and soccer, I have a few particular people that I'm really interested in <inaudible> brothers even could be for wrestling just to talk to them. Cause, cause you can, you guys can communicate in Russian and in wrestling, right. As wrestlers and as Russians. And so that, that little, it's like an opportunity to explore a mind that that I'm able to bring to the world.

4 (2h 52m 48s):
And, and also it, I feel like it makes me a better person, just that being that vulnerable and exploring ideas together, I don't know like good conversation. I don't know how often you have really good conversation with friends, but like podcasts are like that. And it's, it's deeply moving.

2 (2h 53m 7s):
That's the best, you know? And, and what, what you've brought through. I mean, when I saw you sit down with Penn rose, you know, Nobel prize, winning physicist and these other folks, it's not just because he has a Nobel, it's what comes out of his mouth is incredible. And what you were able to, to hold in that conversation was so much better light years beyond what he had. Any other interviewer, I don't want to even call you an interviewer because it's really about conversation light years beyond what anyone else had been able to engage with him was, was such a beacon of what's possible. And I know that I think that's what people are drawn to. And there's a certain intimacy that certainly two people are friends as we are, and they know each other that there's more of that, but there's an intimacy in those kinds of private conversations that are made public and

4 (2h 53m 56s):
Well, th that's the, with you, you're probably starting to realize, and Costello is like part of it because you're authentic and you're putting yourself out there completely. People are almost not just consuming the words you're saying. They also enjoy watching you, Andrew struggle with these ideas or try to communicate these ideas. They like the flaws. They like, they like a human being. Okay.

2 (2h 54m 25s):
Well, that's good because I got plenty of

4 (2h 54m 26s):
Those, the like the self-critical aspects, like where you're very careful where you're very, self-critical about your flaws. I mean that, in that same way, it's interesting. I think for people to watch me talk to Penrose, not just because Penrose is communicating ideas, but here's this like silly kid trying to explore ideas. Like they know this kid that there's a human connection that has really powerful. Same, I think with Putin, right? Like it's not just as a good interview with Putin. It's also, here's this kid struggling to, to talk with one of the most, somewhat argue, dangerous people in the world that they love that the, the, the authenticity that led up to that, like, and in return, I get to connect everybody I run to in the street and all those kinds of things.

4 (2h 55m 19s):
Th there's a depth of connection there almost within like a minute or two that's unlike any other.

1 (2h 55m 24s):
Yeah. There's an intimacy that you've formed with them.

4 (2h 55m 27s):
I've been on this like journey together. And yeah, I have the same thing with Joe Rogan before I ever met him. Right. Like I was because I was a fan of Joe for so many years, you have the there's something there's, there's a kind of friendship as absurd as it might be to say in podcasting and listening to podcasts.

1 (2h 55m 46s):
Yeah. Maybe it, maybe it fills in a little bit of that or solves a little bit of that loneliness that been talking

4 (2h 55m 51s):
Until the robots are here.

1 (2h 55m 54s):
I have just a couple more questions, but one of them is on behalf of your audience, which is, I'm not going to ask you the meaning of the hedgehog, but I just want to know, does it have a name and you don't have to tell us the name, but just, does it have a name? Yes or no?

4 (2h 56m 12s):
Well, there's a, there's a name he likes to be referred to as, and then there's a private name in the privacy of my own company that we call each other. No, I'm not that insane. Now his name is Hedgie, he's a hedgehog. I don't like stuffed animals, but his story is one of minimalism. So I gave away everything. I own now three times in my life by everything. I mean, almost everything kept jeans and shirt and a laptop. And recently it's also been guitar, things like that, but he survived because he was always in, at least in the first two times was in the laptop bag and he just got lucky.

4 (2h 57m 0s):
And so I just liked the perseverance of that. And I first saw him in the, the reason I got a stuffed animal and that don't have other stuffed animals is it was in a thrift store in this like giant pile of stuffed animals. And he jumped out at me because unlike all the rest of them, he has this intense mean look about him. That he's just, he's upset at life at the cruelty of life. And it's just, especially in the contrast of the other stuffed animals, they have this dumb smile on their face. If you look at most stuffed animals, have this dumb look on their face and they're just happy. Yeah.

1 (2h 57m 40s):
Pleasantville. It's what we say in neuroscience. They have a smooth cortex, not, yeah, not many folks.

4 (2h 57m 44s):
Exactly. And this like Hedgie like saw through all of it. He was like dusty eskies man from underground. I mean, there's a sense that he saw the darkness of the world and persevered. So like, and there's also a famous Russian cartoon hedgehog in the fog that I, I, I grew up with, I connected with people who no of that cartoon. You could see it. I need to, it's like a jog in the fog. Yeah. He is just, as you would expect, especially from like early Soviet cartoons, it's a hedgehog like sad walking through the fog, exploring like loneliness and sadness.

4 (2h 58m 25s):
It's like, but it's beautiful. It's like a piece of art people should just, even if you don't speak Russian, you'll see it you'll understand.

2 (2h 58m 32s):
Oh, it's an, the moment you said that I was going to ask, so it's in Russian, but

4 (2h 58m 35s):
It's in Russian, but it's more, it's very little speaking in it. It's almost, there's an interesting exploration of, of how you make sense of the world when you see it only vaguely through the fog. So he's trying to understand the world

2 (2h 58m 55s):
Here. We have Mickey mouse. We have bugs bunny. Yeah. We have all these, you know, crazy animals and you have the hedgehog in the fall.

4 (2h 59m 3s):
So there's a, there's a certain period. And this is again, I dunno what it's attributed to, but it was really powerful, which does a period in Soviet history. I think probably seventies and eighties where like, especially kids were treated very seriously. Like they were treated like they're able to deal with the, with the weightiness of life. And that, that was reflected in the cartoons. And there was, it was allowed to have like, like really artistic content, not like dumb cartoons that are trying to get you to like smile and run around, but like create art, like stuff that, you know, how like shortcut tunes of short films can win Oscars.

4 (2h 59m 47s):
Like that's what they're swinging for.

2 (2h 59m 49s):
So what strikes me about this is a little bit how we were talking about the suit earlier. It's almost like they treat kids with respect. Yeah. Like that they have, they have an intelligence and they honor that intelligence.

4 (2h 59m 60s):
Yeah. They're really just adult in a small body. Like you want to protect them from the true cruelty of the world. Sure. But in terms of their intellectual capacity or like philosophical capacity, they're right there with you. And so the, the cartoons reflected that the art that they consumed, the education reflected that. So he represents that. I mean, there's a, there's a sense of, because it's survived so long and because I don't like stuffed animals that it's like, we've been through all of this together and it's, it's the same sharing the moments together as the friendship. And there's a sense in which, you know, if all the world turns on you and goes to hell, at least we got each other that, and he doesn't die because he's an inanimate object.

4 (3h 0m 44s):
So until you animate him at the T to animate him, and then I probably wouldn't want to know what he was thinking about this whole time. He's probably really into Taylor swift or something like that. It just sucks that I wouldn't even want to know anyway. Well, well,

2 (3h 0m 60s):
I now feel a connection to Hedgie the headshot that I certainly didn't have before. And I think that encapsulates the kind of possibility of connection that is possible between human and, and other object in through the, through robotics. Certainly there's a saying that I heard when I was a graduate student that I that's just been ringing in my mind throughout this conversation in such a, I think appropriate way, which is that Lex, you are in a minority of one, you are truly extraordinary in your ability to encapsulate so many aspects of science, engineering, public communication, about so many topics, martial arts and the emotional depth that you bring to it.

2 (3h 1m 47s):
And just the purposefulness. And I think if it's not clear to people, it absolutely should be stated, but I'm think it's abundantly clear that just the amount of time and thinking that you put into things, is it, it is the ultimate mark of respect. So I'm just extraordinarily grateful for your friendship. And for this conversation,

6 (3h 2m 9s):
I'm a proud to be a friend. And I just wished you showed me the same kind of respect by wearing a suit and make your father proud maybe next time.

2 (3h 2m 17s):
Next time. Indeed. Thanks so much my friend. Thank you. Thank you, Andrew. Thank you for joining me for my discussion with Dr. Lex Friedman. If you're enjoying this podcast and learning from it, please consider subscribing on YouTube as well. You can subscribe to us on Spotify or apple. Please leave any questions and comments and suggestions that you have for future podcast, episodes and guests in the comment section on YouTube at apple, you can also leave us up to a five-star review. If you'd like to support this podcast, we have a Patrion that's patrion.com/andrew Huberman. And there you can support us at any level that you like. Also, please check out.

2 (3h 2m 57s):
Our sponsors mentioned at the beginning of the podcast episode, that's the best way to support this podcast. Links to our sponsors can be found in the show notes. And finally, thank you for your interest in science.