0 (0s):
Welcome to the Huberman Lab Podcast, where we discuss science and science based tools for everyday life. I'm Andrew Huberman and I'm a professor of neurobiology and ophthalmology at Stanford School of Medicine Today. my guest is Marc Andreessen. Marc Andreessen is a software engineer and an investor in technology companies. He co-founded and developed Mosaic, which was one of the first widely used web browsers. He also co-founded and developed Netscape, which was one of the earliest widespread used web browsers. and he co-founded and is a general partner at Andreessen Horowitz, one of the most successful Silicon Valley venture capital firms.

0 (41s):
All of that is to say that Mark Andreessen is one of the most successful Innovators and investors ever. I was extremely excited to record this episode with Mark for several reasons. First of all, he himself is an incredible innovator. Second of all, he is an uncanny ability to spot the Innovators of the future. And third Mark has shown over and over again the ability to understand how technologies not yet even developed are going to impact the way that humans interact at large. Our conversation starts off by discussing what makes for an exceptional innovator, as well as what sorts of environmental conditions make for exceptional innovation and creativity more generally. In that context, we talk about Risk Taking, not just in terms of Risk Taking in one's profession, but about how some people, not all, but how some people who are risk takers and Innovators in the context of their work also seem to take a lot of Risks in their personal life and some of the consequences that can bring.

0 (1m 36s):
Then we discuss some of the most transformative technologies that are now emerging, such as novel approaches to developing clean energy as well as AI or Artificial Intelligence. With respect to ai, mark shares his views as to why AI is likely to greatly improve human experience, and we discussed the multiple roles that AI is very likely to have in all of our lives in the near future. Mark explains how not too long from now, all of us are very likely to have AI assistance, for instance, assistance that give us highly informed health advice, highly informed psychological advice. Indeed, it is very likely that all of us will soon have AI assistance that govern most if not all of our daily decisions.

0 (2m 17s):
And Mark explains how, if done correctly, this can be a tremendously positive addition to our life. In doing so, mark provides a stark counterargument for those that argue that AI is going to diminish human experience. So if you're hearing about and or concerned about the ways that AI is likely to destroy us today, you are going to hear about the many different ways that AI technologies now in development are likely to enhance our human experience at every level. What you'll soon find is that while today's discussion does center around technology and technology development, it is really a discussion about human beings and human psychology. So whether you have an interest in technology development and or ai, I'm certain that you'll find today's discussion to be an important and highly lucid view into what will soon be the future that we all live in.

0 (3m 5s):
Before we begin, I'd like to emphasize that this podcast is separate from my teaching and research roles at Stanford. It is, however, part of my desire and effort to bring zero cost to consumer information about science and science related tools to the general public. In keeping with that theme, I'd like to thank the Sponsors of today's podcast.

LMNT (3m 21s):
Our first Sponsor is element. Element is an electrolyte drink that has everything you need and nothing you don't. That means plenty of the electrolytes, sodium, magnesium, and potassium in the correct ratios, but no sugar. The electrolytes and hydration are absolutely key for mental health, physical health and performance. Even a slight degree of dehydration can impair our ability to think our energy levels and our physical performance element makes it very easy to achieve proper hydration, and it does so by including the three electrolytes in the exact ratios they need to be present. I drink element first thing in the morning. When I wake up, I usually mix it with about 16 to 32 ounces of water. If I'm exercising, I'll drink one while I'm exercising, and I tend to drink one after exercising as well. Now, many people are scared off by the idea of ingesting sodium because obviously we don't want to consume sodium in excess. However, for people that have normal blood pressure and especially for people that are consuming very clean diets, that is consuming not so many processed foods or highly processed foods, oftentimes we are not getting enough sodium, magnesium, and potassium, and we can suffer as a consequence. And with element simply by mixing and water, it tastes delicious. It's very easy to get that proper hydration. If you'd like to try Element, you can go to Drink Element, that's LMNT dot com slash Huberman to claim a free element sample pack with your purchase. Again, that's drink element LMNT dot com slash Huberman.

Eight Sleep (4m 43s):
Today's episode is also brought to us by Eight Sleep. Eight. Sleep makes smart mattress covers with cooling, heating and sleep tracking capacity. I've spoken many times before in this podcast about the fact that sleep that is getting a great night's sleep is the foundation of all mental health, physical health and performance. When we're sleeping well, everything goes far better. When we are not sleeping well or enough, everything gets far worse at the level of mental health, physical health and performance. Now, one of the key things to getting a great night's sleep and waking up feeling refreshed is that you have to control the temperature of your sleeping environment. And that's because in order to fall and stay deeply asleep, you need your core body temperature to drop by about one to three degrees. And in order to wake up feeling refreshed and energized, you want your core body temperature to increase by about one to three degrees. With Eight Sleep, it's very easy to induce that drop in core body temperature by cooling your mattress early and throughout the night and warming your mattress toward morning. I started sleeping on an Eight Sleep mattress cover a few years ago. And, it has completely transformed the quality of the sleep that I get. So much so that I actually loath traveling because I don't have my Eight Sleep mattress cover when I travel. If you'd like to try Eight Sleep, you can go to Eight Sleep dot com slash Huberman and you'll save up to $150 off their pod three cover Eight Sleep currently ships in the U Ss a Canada UK select countries in the EU and Australia. Again, that's Eight Sleep dot com slash Huberman.

0 (6m 2s):
And now for my discussion with Marc Andreessen, mark, welcome. Hey,

2 (6m 7s):
Thank you.

0 (6m 8s):
Delighted to have you here and have so many questions for you about innovation, ai, your view of the landscape of tech and humanity in general. Yep. I wanna start off by talking about innovation from three different perspectives. There's the inner game, so to speak, or the psychology of the innovator or Innovators things like their propensity for engaging in conflict or not their propensity for having a dream or a vision, and in particular their innovation as it relates to some psychological trait or expression. So we'll get to that in a moment. The second component that I'm curious about is the outer landscape around Innovators, who they place themselves with, the sorts of choices that they make, and also the sorts of Personal Relationships that they might have or not have.

0 (6m 60s):
And then the last component is this notion of the larger landscape that they happen to find themselves in. What time in history, what's the geography, bay Area, New York, Dubai, et cetera. Sure. So to start off, is there a common trait of Innovators that you think is absolutely essential as a seed to creating things that are really impactful?

2 (7m 26s):
Yeah, so I'm not a psychologist, but I've picked up some of the concepts, some of the, some of the terms. And so I've, I've, it, it was a great moment of delight in my life when I learned about the big five personality traits. 'cause I was like, aha, there's a way to actually describe right, the answer to this question in at least reasonably scientific terms. And so I, I think what you're looking for when you're, when you're talking about real Innovators like people who actually do really creative breakthrough work, I think you're talking about a couple things. So one is very high in what's called trait openness. Right? Which is one of the big, one of the big five, which is basically just like flat out open to new ideas. And of course the, the nature of trait openness is trait openness means you're not just open to new ideas in one category. You're open to many different kinds of new ideas. And so we, we might talk about the fact that a lot of Innovators also are very creative people in other aspects of their lives, right?

2 (8m 10s):
Even outside of their, their, their specific creative domain. So that's important. But of course, just being open is not sufficient. 'cause if you're just open, you could just be curious and explore, right? And spend your entire life reading and, and doing, you know, talking to people and never actually create something. So you also need a couple of other things. You need a high level of conscientiousness, which is another one of the big five. You need somebody who's really willing to apply themselves and in our world, typically over a period of many years, right, to be able to, to, to accomplish something. Right. You know, they, they, they typically work very hard. That often gets obscured. 'cause the stories that end up getting told about these people are, you know, it's just like there's this kid and he just had this idea And, it was like a stroke of genius And. it was like a moment in time and you know, it was just like, oh, he was so lucky. And it's like, no, like for most of these people it's years and years and years of, of, of applied effort.

2 (8m 51s):
And so you need, you need somebody with like an extreme, you know, basically willingness to defer gratification and really apply themselves to a, a specific thing for a long time. And of course, this is why there aren't very many of these people is there aren't many people who are high in openness and high in conscientiousness. 'cause to a certain extent they're, they're opposed, right? Traits. And so you need somebody who has both of those. Third is you need somebody high in Disagreeableness, which is the third of the big five. So you need somebody who's just like, basically ornery, right? Because if they're not ornery, then they'll be talked outta their ideas by people who will be like, oh, well that, you know, 'cause the reaction most people have new ideas is, oh, that's dumb. And so somebody who's too agreeable will, will be easily dissuaded to, to not Pursue, you know, not pulling the thread anymore.

2 (9m 32s):
So you need somebody highly disagreeable. Again, the nature of Disagreeableness is they tend to be disagreeable, disagreeable about everything, right? So they tend to be these very sort of iconoclastic, you know, kind of renegade characters. And then there's just a table stakes component, which is they just also need to be high iq, right? They just, they just need to be really smart because it's just, it's hard to innovate in any category if you can't synthesize large amounts of information quickly. And so those are four, like basically like high spikes, you know, very rare traits that basically have to have to come together. You could probably also say they, they probably at some point need to be relatively low in neuroticism, which is another the big five. 'cause if they're too neurotic, they probably can't handle the stress, right? So, so it's kind of this, this, this dial in there.

2 (10m 12s):
And then of course, if you're, if you're into, if you're into like this, the, the sort of science of the big five, basically, you know, these are all people who are on like the far outlying kind of point on the, on the normal distribution across all these traits. And, and, and, and then that just gets you to, I think the, the sort of hardest topic of all around this, this, this whole concept, which is just, there are very few of these people.

0 (10m 29s):
Do you think they're born with these traits? Yeah,

2 (10m 32s):
Well, so the, the, they're, they're born with the traits and then, and then of course the traits are not, you know, genetics are not destiny. And so the, the, the, the traits are not deterministic in the sense of that, you know, just 'cause they have those personality traits doesn't mean they're, they're, they're gonna, you know, deliver great creativity, but like, they need to have those properties because otherwise they're just not either gonna be able to do the work or they're not gonna enjoy it. Right? Or, I mean, look, a lot of these people are highly capable, competent people. It, you know, it, it is very easy for them to get like high paying jobs in traditional Institutions and, you know, get lots of, you know, traditional awards and, you know, end up with big paychecks. And you know, there, there's a lot of people at, you know, big Institutions that we, we know, you and I know well, and I, I deal with many of these where people get paid a lot of money and they get a lot of respect and they go for 20 years and it's great. And they never create anything, anything new, right?

2 (11m 13s):
And so there's, there's lot of administrators.

0 (11m 15s):
Yeah.

2 (11m 15s):
Well, and a lot of them, yeah, a lot of, a lot of 'em end up in, in administrative jobs. And that's fine, that's good. The world needs, you know, the world needs that also. Right? The, the, the Innovators can't run everything, 'cause everything, you know, the, the, the rate of change would be too high. Society I think probably wouldn't be able to handle it. So you need some people who are on the other side who are gonna kind of keep the lights on and keep things running. But, but there is this decision that people have to make, which is okay, if I have the sort of latent capability to do this, is this actually what I want to spend my life doing? And do I want to go through the stress and the pain and the trauma, right? And the anxiety, right? And the risk of failure, right? And so do, do I really want to, once in a while you run into somebody who's just like, can't do it any other way. Like they, they just have to, to,

0 (11m 53s):
Who's an example of that? I

2 (11m 54s):
Mean, e e Elon, Elon's, the, you know, the paramount example of our time. and he and I, I bring him up in part 'cause he's such an obvious example, but in part, 'cause he's talked about this in, in, in in interviews where he, he basically says like, he's like, I can't turn it off. Like the ideas come, I have to Pursue them, right? It's why he's like running five companies at the same time and like working on a sixth, right? It is just like, he, he can't, he can't turn it off. You know? Look, there's a lot of other people who are pro probably had the capability to do it, who ended up talking themselves into, or you know, whatever events conspired to put them in a position where they did something else. You know, obviously there are people who try to be creative who just don't have the, the, the capability. And so there, there's some Venn diagram there of determinism through traits, but also choices in life.

2 (12m 36s):
And then also of course, the situation in which they're born, the context within which they grow up culture, right? And, What, their parents expect of them and, and, and so forth. And so you have to, you know, you, you kind of get all the way through this. You have to thread all these, all these needles kind of at the same time.

0 (12m 50s):
Do you think there are folks out there that meet these criteria who are disagreeable, but that can feign agreeableness, you know, that can, for those just listening, mark just raised his right hand. Oh, Andrew, in other words that that can sort of phrase that comes to mind maybe 'cause I can relate to a little bit. They sneak up through the system, meaning they behave ethically as it relates to the requirements of the system. They're not breaking laws or breaking rules. In fact, quite the opposite. They're paying attention to the rules and following the rules until they get to a place where being disagreeable feels less threatening to their overall sense of security.

2 (13m 29s):
Yeah. I mean, look, the really highly competent people don't have to break laws, right? Like it's, it's the, there was this, there was this, there was this myth, you know, that started, happened around the movie The Godfather. And then there was this character, Meyer Lansky, you know, who's like ran basically the mafiaa, you know, 50, 60, 70 years ago. And there was this, there was this great line of like, well, Olansky had only like applied himself to running General Motors. He would've been the best c e o of all time. And it's like, no, not really, right? Like the, the, the, the people who are like great at running the big companies, they don't have to run, they don't have to be mob bosses. They don't have to like break laws. They can, you know, they can, they can do, they can work in, they're, they're smart and sophisticated enough to be able to work inside the system. You know, they don't need to take the easy out. So I, I don't think there's any implication that they have to, you know, that they have to, they have to break laws that said they have to break norms, right? And, and, and specifically the, the, the thing, this is probably the thing that gets missed the most.

2 (14m 11s):
'cause the process of the process of innovating, the process of creating something new, like once it works, like the stories get rec con as they say in comic books. So the, the stories get adapted to where it's like, it was inevitable all along. You know, everybody always knew that this was a good idea. You know, the, the person has won all these awards, society embraced them. And it's if, if in invariably if you, if you were with them when that was, when they were actually doing the work, or if you actually get a couple drinks into them and talking about it, it'd be like, no, that's not how it happened at all. They faced a wall of skepticism, just like a wall of basically social, you know, essentially denial. No, this is not gonna work. No, I'm not gonna join your lab. No, I'm not gonna come work for your company. No, I'm not gonna buy your product. Right? No, I'm not gonna meet with you. And so they, they, they, they, they get just like tremendous Social Resistance.

2 (14m 54s):
Like they're, they're, they're not getting positive feedback from their, from, from their social network the way that more agreeable people need to have. Right? And this is why, this is why agreeableness is a problem for innovation. If, if you're agreeable, you're gonna listen to the people around you, they're gonna tell you that new ideas are stupid, right? End of story. You're, you're not gonna proceed. And so I, I would put it more on like, they need to be able to deal with, they need to be able to deal with social discomfort to the level of ostracism or at some point they're gonna get shaken out and they're just gonna quit.

0 (15m 21s):
Do you think that people that meet these criteria do best by banding with others that meet these criteria early? Or is it important that they form this deep sense of self, like the ability to cry oneself to sleep at night or, you know, line the fetal position worrying that things aren't going to work out and then still get up the next morning and get right back out there? Right.

2 (15m 42s):
So Sean Parker has the best line by the way, on, on, on, on this. He says, you know, being a, being an entrepreneur, being a creator is like, you know, getting punched in the face like over and over again. He said, eventually you start to like the taste of your own blood. And I love that line 'cause it makes everybody like massively uncomfortable, right? But it gives you a sense of like how basically painful the process is. If you, if you talk to any entrepreneur, you know, who's been through it about that, they're like, oh yeah, that's exactly, that's exactly what it's like. So, so, so there is this, there is a big individual component to it, but look, but look, it can be very lonely, right? And it be especially, you know, very hard I think to do this. If, if nobody around you is trying to do anything even remotely similar, right? And if you're getting just universally negative responses, like I, you know, very few people I think have very few people have ego strength to be able to survive that for years.

2 (16m 23s):
So I do think there's a huge advantage. And, and this is why you, you do see clusters, there's a huge advantage to clustering, right? And so you have, and and you know, throughout history you've had this clustering effect, right? You had, you know, clustering of the great artists and sculptors and renaissance Florence, you know, you have the clustering of the philosophers, Greece, you have the clustering of, of tech people in Silicon Valley, you have the clustering of creative, you know, arts movie TV people in Los Angeles, right? And so forth and so on. You know, for, you know, it, there's always a scene, right? There's, there's al there's always like a nexus and a place where, where people come together, you know, for, for these kinds of things. So generally speaking, like if somebody wants to work in tech, innovate in tech, they're gonna be much better off being around a lot of people who are trying to do that kind of thing than they are in a place where nobody else is doing it. Having said that, the clustering has, it can have downsides, it can have side effects, and, and you put any group of people together and you do start to get Group Think even among people who are individually very disagreeable.

2 (17m 13s):
And so these same clusters where you get these very idiosyncratic people, they do have fads and trends just like every place else, right? And so they get, they get, they get wrapped up in their own social dynamics. And the good news is the social dynamic in those places is usually very forward looking, right? And so it's usually, it's usually like, you know, it's, I dunno, it's like a herd of iconoclast looking for the next big thing, right? So iconoclast looking for the next big thing that's good, the herd part, right? That's what you gotta be careful of. So even when you're in one of these environments, you have to be careful that you're not getting sucked into the Group Think too much.

0 (17m 43s):
When you say Group Think, do you mean excessive friction due to pressure testing each other's ideas to the point where things just don't move forward, forward? Or are you talking about Group Think where people start to form a consensus or the self-belief that, gosh, we are so strong because we are so different. What, what do you, can we better define Group Think?

2 (18m 1s):
It's actually less either one of those things both happen. Those are good. So those are good. The part of Group Think I'm talking about is just like we all, we all basically zero in, we, we just end up zeroing in on the same ideas, right? In Hollywood there's this classic thing, it's like, and you know, there, there, there, there are years where there all of a sudden there's like a lot of volcano movies. It's like, why are there all these volcano movies? And it's just like, I don't, there was just something in that just stalled, right? There was just something in the air, you know, there look, tech Silicon Valley has this, you know, there, there are moments in time where you'll, you'll have these, well, it's like, it's the old thing. Like what's the difference between a fad and a trend, right? You know, the fad fad is the trend that doesn't last. Right? And so, you know, Silicon Valley is subject to fads and both fads and trends just like any place else, right? In other words, you take smart disagreeable people, you cluster them together, they will act like a herd, right?

2 (18m 44s):
They, they will end up thinking the same things unless they try very hard not to.

0 (18m 48s):
You've talked about these personality traits of great Innovators before, and we're talking about them now. You invest in Innovators, you try and identify them and you are one. So you can recognize these traits Here I'm making the presumption that you have these traits. Indeed you do. We'll just get that out of the way. Have you observed people trying to feign these traits? And are there any specific questions or behaviors that are a giveaway that they're pretending to be the young Steve Jobs, or that they're pretending to be the, the young Henry Ford? Pick your list of other, other names that qualify as authentic legitimate Innovators.

0 (19m 32s):
We won't wanna name names of people who've tried to disguise themselves as true Innovators, but what are some of the, the, the litmus tests? And I realize here that we don't want you to give these away to the point where they're lose their potency. But if you could share a few of those, that'd

2 (19m 47s):
Be helpful. Yeah, no, that's, that's good. We're we're actually a pretty open book on this. So, so yeah. So, so first of all, yes, so there are people who definitely try to like, come in and basically present as being something that they're not. And they've, you know, like they've read all the books, they will have listened to this interview, right? They, they will, they, you know, they study everything and they, they construct a facade and they come in and present at something. They're not, I would say the amount of that varies exactly correlated to the Nasdaq, right? And so when stock prices are super low, like you actually get the opposite. When stock prices are super low, people get two demoralized and people who should be doing it basically give up. 'cause they just think that whatever, whatever the industry's over the trend is, over whatever, it's all hopeless. And so you get this flushing thing, so nobody ever shows up at a stock market low, right? And says like, I'm the new, I'm the new, I'm the new next big thing.

2 (20m 28s):
And, and, and, and does and, and doesn't really want to do it because, because there are higher status. The kinds of people who do the thing that you're talking about, they're, they're fundamentally oriented for social status. They're, they're trying, they're, they're, they're trying to get the social status without, without actually, without actually the substance. And there are always other places to go get social status. So, so after 2000, the joke was, so, you know, when I got to Silicon Valley in 93, 94, the valley was dead. We, we can talk about that. By 98 it was roaring and you had a lot of these people showing up who were, you know, basically, you basically had a lot of, a lot of people showing up with, with side kind of stories. 2000 market crash. By 2001, the joke was that there were these terms B two C and B two B. And in 1998, they meant B two C meant business to consumer.

2 (21m 10s):
And B two B meant business to business, which is two different kinds of business models for internet companies. By 2001, B two B, B two B meant back to banking. And B two C meant back to consulting, right? Which is the high status people who, the people oriented to status who showed up to be in tech we're like, yeah, screw it. Like this is over, stick a fork in it. I'm gonna go back to, you know, Goldman Sachs or go back to McKinsey, you know, where I can, where I can, where I can be high status. And so you, you you, you get this flushing kind of effect that happens in a, in a downturn that said on a, on a on in a big upswing. Yeah. You, you get, you get a lot of, you get a lot of people showing up with, with a lot of, you know, with a lot of, you know, kind of let's say public persona without the substance to back it up. So the, the way we stress that, I can actually say exactly how we test for this, which, because it, it's, it, the the test exactly addresses the issue in a way that is impossible to fake.

2 (21m 56s):
And, and it's actually this, it's actually the same way homicide detectives trying to find out if you've, if you, if you're, if you've actually like, you're innocent or whether you've killed somebody, it's the same, it's the same tactic, which is you, you ask increasingly detailed questions, right? And so, you know, the way a homicide cop does this is, you know, what were you doing last night? You know, oh, I was at a movie. Well, which movie? You know, da da da, oh, which theater? You know, okay, which seat did you sit in? You know, okay, what was the end of the movie? Right? Like, right. And you, you ask increasingly detailed questions and people have trouble, ma at, at some point people have trouble making up And. it things just fuzz into just kind of obvious bullshit. And basically, fake founders basically have the same problem. They have a cons, they, they're able to relay a conceptual theory of what they're doing that they've kind of engineered.

2 (22m 37s):
But as they get into the details, it just, it just fusses out. Whereas the, the, the true people that you wanna back that can do it, basically what you find is they've spent five or 10 or 20 years obsessing on the details of whatever it's they're about to do. And they're so deep in the details that they know so much more about it than you ever will. And in fact, the best possible reaction is when they get mad. Right? Which is also what the homicide cops say, right? What you actually want is you actually want the, you absolutely want the emotional response of like, I can't believe that you're asking me questions this detailed and specific and picky. And they kind of figure out what you're doing. And then they get upset. Like, that's good, that's perfect, right? But, but, but they, they have, you know, but then they have to prove themselves and in the sense of like, they have to be able to answer the questions in, in, in great detail.

0 (23m 18s):
Do you think that people that are able to answer those questions in great detail have actually taken the time to systematically think through the, if ands of all the possible implications of what they're going to do and they have a specific vision in mind of how things need to turn out or will turn out? Or do you think that they have a, a vision and it's a, no matter what, it will work out because the world will sort of bend around it? I mean, in other words, do you think that they place their vision in context or they simply have a vision and they have that tunnel vision of that thing and that's gonna be it. Let's use you for an example with Netscape. I mean, that's how I first came to know your name when you were conceiving Netscape.

0 (24m 1s):
Do you think, okay, there's this search engine and this browser and, and it's going to be this thing that looks this way and works this way and feels this way. Did you think that, and also think about, you know, that there was gonna be a gallery of other search engines And, it would fit into that landscape of other search engines. Or were you just projecting your vision of this thing as this unique and special brainchild?

2 (24m 27s):
Well, I'm gonna, lemme give the general answer and then we can talk about the specific example. So the general answer is what, what ent, entrepreneurship, creativity, innovation is what economists call decision making under uncertainty, right? And so been both parts those important decision making, like you're gonna make a ton of decisions 'cause you have to decide what to do, And, What not to do. And then uncertainty, which is like the world's a complicated place, right? And, and in mathematical terms, the world is a complex adaptive system with feedback loops. And like, it's really, I mean, it's, it's extreme. You know, Isaac Asimov wrote the, you know, the, in his novels, he, he wrote about this field called psychos history, right? Which is the idea that there's like a supercomputer that can predict the future of like human affairs, right? And it's like, we don't have that. Not yet. Not, not yet, not yet, not yet. Well,

0 (25m 5s):
We're working on, we'll get into that later, right?

2 (25m 7s):
We, we certainly don't have that yet. And so you're just dealing, you know, military commanders call this the fog of war, right? You're, you're, you're just dealing with the situation where the number of variables are just off the charts. It's all these other people, right? Who are inherently unpredictable making all these decisions in different directions. And, and then the whole system is combinatorial, which is these people are colliding with each other influencing the decisions. And so, I mean, look, the, the most straightforward kind of way to think about this is, it's just, it's amazing. Like anybody who believes in economic central planning, it always blows my mind. 'cause it's like, it's just like, try opening a restaurant. Like try just opening a restaurant on the corner down here and like 50 50 odds the restaurant's gonna work. And like, all you have to do to run a restaurant is like, have a thing and serve food. And like, and it's like, most restaurants fail, right?

2 (25m 49s):
And so, and restaurant people run restaurants are like pretty smart. Like they're, you know, they're, they're, they usually think about these things very hard. They all wanna succeed, And it, it's hard to do that. And so to start a tech company or to start an artistic movement or to, or to fight a war, like you're just going into this like basically about conceptual battleground or you know, military terms real battleground where there's just like incredible levels of complexity, branching future paths. And so there, there's nothing, it's, it's, you know, there's nothing predictable. And so what we look for is basically the, the the sort of dr the, the really good Innovators they've gotta drive to basically be able to cope with that and deal with that. And they, they basically do that in two steps. So one is they try to pre-plan as much as they possibly can. And, and, and we call that the process of navigating the, we call the idea maze, right?

2 (26m 29s):
And so the idea maze basically is, I've got this general idea And, it might be the internet's gonna work or search or whatever. And then it's like, okay, in their head they have thought through of like, okay, if I do it this way, that way, this third way, here's what'll happen, then I have to do that. Then I have to do this, then I have to bring in somebody to do that. That here's the technical challenge I'm gonna hit. And they've got in their, again, in their heads as best anybody could, they've, they've got as complete as sort of a map of possible futures as they could possibly have. And and this is where I say when you ask them increasing detailed questions, that's what you're trying to kind of get them to kind of chart out is, okay, how far ahead have you thought and how much are you anticipating all of the different twists and turns that this is gonna take? Okay? So then they start on day one. And then of course what happens is, you know, now they're in, now they're in it, they're in, now they're in the fog of war, right?

2 (27m 9s):
They're in future uncertainty. And now that idea ma is maybe not helpful practically, but now they're gonna be basically constructing on, on the fly day by day as they learn and discover new things. And as the world changes around them, and of course it's a feedback loop. 'cause they're gonna change, you know, if their thing starts to work, it's gonna change the world. And then the fact the world is changing is gonna cause you know, their plan to, to, you know, to change as well. And so yeah, the great, the great ones, basically, they, they course correct, you know, they cor the great ones course correct every single day. You know, they take stock of what they've learned, you know, they, they modify the plan. The great ones tend to think in terms of hypotheses, right? It, it's a little bit like a scientific sort of mentality, which is they tend to think, okay, I'm gonna try this. Like, sorry, I'm gonna go into the world, I'm gonna announce that I'm doing this for sure, right? Like, I'm gonna say like, this is my plan and I'm gonna tell all my employees that.

2 (27m 52s):
I'm gonna tell all my investors that. And I'm gonna put a stake in there, And. it is my plan. I then I'm gonna try it, right? And even though I sound like I have complete certainty, I know that I need to test to find out whether it's gonna work. And if it's not, then I have to go back to all those same people and I have to say, well actually we're not going left, we're going right. And they have to run that loop thousands of times, right? And they had, you know, to get through the other side And it, this, this led to the creation of this great term Pivot, which has been very helpful in our industry. 'cause the, the, the word when I was, when I was young, the word we used was fuck up. And Pivot like, sounds like so much better. It sounds like so much more professional, but yeah, you like make mistakes. You, you, you, it's just, it's just too complicated to understand you course correct. You adjust, you evolve. Often these things, at least in business, the businesses that end up working really well tend to be different than the original plan.

2 (28m 35s):
But that's, that's part of the process of a really smart founder basically working their way through reality, right? As as, as they're executing their plan.

0 (28m 43s):
The way you're describing this has parallels to a lot of models in biology and the practice of science, you know, random walks, but that aren't truly random pseudorandom walks in biology, et cetera. But one thing that is becoming clear from the way you're describing this is that I could imagine a great risk to early success. So for instance, somebody develops a product, people are excited by it, they start to implement that product, but then the landscape changes and they don't learn how to Pivot to use the less profane version of it, right? They don't learn how to do that. In other words, the, and I think of everything these days or most everything in terms of reward schedules and dopamine reward schedules.

0 (29m 23s):
'cause that is the universal currency of reward. And so when you talk about the Sean Parker quote of learning to enjoy the taste of one's own blood, right? That is very different than learning to enjoy the taste of success. That's right. Right? It's about internalizing success as a process of being self-determined and less agreeable, et cetera. In other words, building up of those five traits right? Becomes the source of dopamine perhaps in a way that's highly adaptive. So on the outside, we just see the product, the end product, right? The iPhone, the MacBook, the Netscape, et cetera. But I have to presume, and I'm not a psychologist, but I have done neurophysiology and I've studied the dopamine system enough to know that what's being rewarded in the context of what you're describing sounds to be a reinforcement of those five traits rather than, oh, it's going to be this particular product or the company's gonna look this way, or the logo's gonna be this or that, that all seems like the peripheral to what's really going on.

0 (30m 22s):
That great Innovators are really in the process of, of establishing Neural Circuitry that is all about reinforcing the me Yeah. And the process of being me. Yeah.

2 (30m 33s):
Yeah. So this, this goes to, yeah, so this is like Extrinsic versus intrinsic Motivation. So the, the Steve Jobs kind of zen version of this, right? Or the sort of hippie version of this was the journey is the reward, right? and he always, he always, he always told, he always told his employees that it's like, look, like, you know, everybody thinks in terms of these big public markers, like the stock price or the I p o or the product launch or whatever. He is like, no, it's, it's actually the, the process itself is the point, right? And, and, and, and if you, to your point, if you have that mentality, then that's, that's an intrinsic Motivation not an ex Extrinsic Motivation. And so that's the kind of intrinsic Motivation that that can keep you going for a long time. Another way to think about it is competing against yourself, right? It's like, can I get better at doing this? Right? And can I prove to myself that I can get better? There's also a big social component to this, and this is one of the reasons why Silicon Valley punches so, so far above its weight as a place, there's a psychological component which is also goes to the Comparison set.

2 (31m 21s):
So a phenomenon that we've observed over time is the leading tech company in any city will aspire to be as large as the previous leading tech company in that city, but often not larger, right? Because they, they sort of have, they have a model of success and as long as they beat that level of success, they've kind of, you know, checked the box like they've made it, you know, and, and, and then they, but then in contrast, you're in Silicon Valley and you look around and it's just like Facebook and Scicomm and Oracle and you know, Hewlett Packard and Gladiators.

0 (31m 48s):
Yeah.

2 (31m 48s):
And you're just like looking at these, you know, giants and, and, and, you know, many of them are still, you know, mark Zuckerberg's still, you know, gonna work every day and like trying to, trying to do, you know, like, and, and so like, these people are like, you know, the, the role models are like alive, right? Right. And they're like right there, right? And it's so clear, like how much better they are and how much bigger their accomplishments are. And so what what we find is young founders in that environment have much greater aspirations, right? Because they just, again, maybe it's like, maybe at that point, maybe it's the social status, maybe there's, there's an Extrinsic component to that. But, or, or maybe it helps calibrate that internal system to basically say, actually, you know, no, the opportunity here is not to build a local, you know, which you may call local maximum form of success, but let's build to a global maximum form of success, which is, which is something as big as we possibly can.

2 (32m 31s):
Ultimately the great ones are probably driven more internally than externally when, when it comes down to it. And that is where you get this phenomenon where you get people who are, you know, extremely successful and extremely wealthy, wealthy who very easily could punch out and move to Fiji and just call it, and they're still working 16 hour days, right? And so obviously something explains that, that has nothing to do with external rewards. And, and I think it's, it's, it's an internal thing.

AG1 (32m 52s):
As many of you know, I've been taking a G one daily since 2012, so I'm delighted that they're sponsoring the podcast. AG one is a vitamin mineral probiotic drink that's designed to meet all of your foundational nutrition needs. Now of course, I try to get enough servings of vitamins and minerals through whole food sources that include vegetables and fruits every day. But oftentimes I simply can't get enough servings. But with AG one, I'm sure to get enough vitamins and minerals and the probiotics that I need, And, it also contains adaptogens to help buffer stress. Simply put, I always feel better when I take AG one. I have more focus and energy and I sleep better. And. it also happens to taste great for all these reasons whenever I'm asked, if you could take just one supplement, what would it be? I answer AG one. If you'd like to try AG one, go to drink AG one.com/ Huberman to claim a special offer. They'll give you five free travel packs plus a year supply of vitamin D three K two. Again, that's drink AG one.com/ Huberman.

2 (33m 50s):
I've heard you talk a lot about the inner landscape, the inner psychology of these folks, and I appreciate that we're going even deeper into that today. And we will talk about the, the landscape around whether or not Silicon, Valley or New York, whether or not there are specific cities that are ideal for certain types of pursuits. I think there was an article written by Paul Graham some years ago about the conversations that you overhear in a city tell you everything you need to know about whether or not you belong there in terms of your professional pursuits.

0 (34m 18s):
Some of that's changed over time. And Now, we should probably add Austin to the mix as, 'cause it was written some time ago. In any event, I wanna return to that, but I want to focus on an aspect of this intrinsic versus Extrinsic motivators in terms of something that's a bit more cryptic, which is one's Personal Relationships. You know, if I think about the catalog of Innovators in Silicon Valley, some of them, like Steve Jobs had complicated personal lives, romantic personal lives early on, And, it sounds like he worked it out. I don't know, I didn't, I wasn't their couple's therapist, but you know, he, when he died, he was in marriage that for all the world, seemed like a happy marriage.

0 (34m 59s):
You also have examples of Innovators who have had many partners, many children with other partners. Elon comes to mind, you know, I don't think I'm disclosing anything that isn't already obvious. Those could have been happy relationships and just had many of them. But the reason I'm asking this is you can imagine that for the innovator, the person with these traits who's trying to build up this, this thing, whatever it is, that having someone or several people in some cases who just truly believe in you when the rest of the world may not believe in you yet or at all right? Could be immensely powerful. And we have examples from cults that embody this.

0 (35m 41s):
We have examples from politics, we have examples from tech innovation and science. And I've always been fascinated by this because I feel like it's the more cryptic and yet very potent form of allowing someone to build themselves up. It's a combination of inner psychology and Extrinsic Motivation because obviously if that person were to die or leave them or cheat on them, or, you know, pair up with some other innovator, which we've seen several times recently and in the past, it can be devastating to that person. But what are your thoughts on the role of personal and in particular romantic relationship as it relates to people having an idea and their feeling that they can really bring that idea to fruition in the world?

2 (36m 23s):
So I it's a real mixed bag. You have lots of examples in all directions. I, and I think it's something like, it's something like something like following. So first is we talked about the personality traits of these people. They tend to be highly disagreeable.

0 (36m 34s):
Doesn't foster a good romantic relationship. A highly disagreeable people

2 (36m 37s):
Can be difficult to be in

0 (36m 38s):
A relationship. I may have heard of that once or twice before a friend may have given me that example.

2 (36m 41s):
Yeah. Right. And you know, maybe you just need to find the right person who like compliments that and is willing to, you know, there's, there's a lot of relationships where, like you, it's always this question about relationships, right? Which is, do you want to have the same personality, you know, growth profile, the same behavioral traits, basically as your partner? Or do you actually want to have, you know, is it is, is it an opposite opposites thing? And you know, look, I'm sure you've seen this. There are relationships where you'll have somebody who's highly disagreeable, who's paired with somebody who's highly agreeable, And. it actually works out great. 'cause one person just gets to be on their soapbox all the time, and the other person's just like, okay. Right. It's, it's fine. Right? It's fine, it's good. You know, you put two disagreeable people together, you know, maybe sparks fly and they have great conversations all the time and maybe they come to hate each other, right? And so, so anyway, so these people, if you're gonna be with one of these people, you're fishing outta the disagreeable end of the pond.

2 (37m 22s):
And, and again, when I say disagreeable, I don't mean, you know, these are, these are normal distributions. I don't mean like 60% disagreeable or 80% disagreeable. The people we're talking about are 99.99% disagreeable, right? So these are ornery, ornery people. So, so, so part of it's that, and then of course they have the other personality traits, right? They're, they're, you know, super conscientious, they're super driven. As a consequence, they tend to work really hard. They tend to not have a lot of time for, you know, family vacations or other things. You know, they're not, they and they don't enjoy them if they're forced to go on them. And so, again, that kind of thing can fray at a, at a relationship. So, so there, there's a, so there's a fair amount in there that's loaded. Like somebody's gonna partner with one of these people needs to be signed up for the ride. And that's, that's a hard thing, you know, that's a, a hard thing to do. Or, or you need a true partnership with two of these, which is also a hard to do.

2 (38m 4s):
So I think that's part of it. And then, look, I think a big part of it is, you know, people achieve a certain level of success and you know, either in their own minds or, you know, publicly, and then they start to be able to get away with things, right? And they start to be able to, it's like, well, okay, you know, now we're rich and successful and famous, and now I deserve, you know, and, and this is where you get into, I, I view this now in the realm of personal choice, right? You get in, in this thing where people start to think that they deserve things. And so they start to behave in, you know, very bad ways and, and then they blow up their personal worlds as a consequence. And maybe they regret it later and maybe they don't, right? It's always a, always a question. So yeah. So I think there's that, and then, I don't know, like, yeah, some people just need, maybe the other part of it is some people just need more emotional support than others.

2 (38m 47s):
And I don't know that that's a big, I don't know that that tilts either way. Like I know, I know some of these people who have like great loving relationships and seem to draw very much on having this kind of firm foundation to rely upon. And then I know other people who are just like, their personal lives are just a continuous train wreck. And, it doesn't seem, doesn't seem to matter. Like professionally, they just keep doing what they're doing. And, and, and maybe there's a, maybe we could talk here about like, you know, whatever is the personality you trait for Risk Taking, right? Like, some people are so incredibly risk prone that they need to take risk in all aspects of their lives at all times. And, and if, if, if part of their life gets stable, they find a way to blow it up. And that's some of, some of these people you could describe in those terms also.

0 (39m 24s):
Yeah. Let's talk about that. Yeah. Because I think Risk, Taking and sensation seeking is something that fascinates me for my own reasons and in my observations of others, does it dovetail with these five traits in a way that can really serve innovation in ways that can benefit everybody? The reason I say to benefit everybody is because there is a view of how we're painting this picture of the innovator as this like really cruel person, but oftentimes what we're talking about are innovations that make the world far better Correct. For billions of people. Yeah, that's

2 (39m 58s):
Right. Yeah. Yeah. And by the way, we're, everything we're talking about also is not just in tech or science or, or in in business, it's also everything we're also talking about is true for the arts, right? So, and you, the history of like artistic expression is, you know, you have people with all these same kinds of

0 (40m 10s):
Traits, right? Well, I was thinking about Picasso and his and his regular turnover of lovers and partners. Yeah. and he was very open about the fact that it was one of the sources of his productivity slash creativity. He wasn't shy about that. I, I suppose if he were alive today, it might be a little bit different. He might be judged a little differently, right? Or

2 (40m 27s):
That was his story for, you know, behaving in a pattern that, you know, was very awful for the people around him and he didn't care. Right?

0 (40m 32s):
Right. Maybe they left him.

2 (40m 34s):
Yeah. Like, you know, who knows, right? So, so, so, you know, puts and takes to all this. But, but no. Okay, so I have a theory. So here, here's a theory. This, this is one of these. I keep a list of things that will get me kicked out of a dinner party, And it topics in any given point in time. Do you

0 (40m 46s):
Read it before you go in? Yeah,

2 (40m 47s):
I just, I, yeah. So I, I I have it on, oh my goodness, on auto recall so that I can, I can get out of these things. But so here, here's the thing that get me kicked out dinner party, especially these days. So think of the kind of person where it's like very clear that they're like super high to your point, somebody is super high output in whatever domain they're in. They've done things that have like, fundamentally like changed the world. They've brought new, whether it's businesses or technologies or art, you know, works of art, you know, entire schools of, of creative expression in some cases to the world. And then at a certain point they blow themselves to smithing, right? And they do that either through like a massive, like financial scandal. They do that through a massive personal, you know, breakdown. They do that through some sort of public expression that causes them a huge amount of problems.

2 (41m 30s):
You know, they say, they say the wrong thing, maybe not once, but several hundred times and blow themselves to Smither Marines. And, and, and there's this, you know, there's this kind of arc, there's this Moral arc that people kind of wanna apply, which it's like the Icarus, you know, the, the, you know, flying too close to the sun and, you know, he had it coming and he needed to keep his ego under control. And you, you get kind of this, you know, kind of this, this, this, this judgment that applies. So I have a different theory on this. So the term I use to describe these people, and a lot of o and by the way, a lot of other people who don't actually blow themselves up, but get close to it, which is a whole nother set of people. I call them Martyrs to Civilizational Progress, right? So, so, and we're backwards Civilizational progress. So, look, the only way civilization gets moved forward is when people like this do something new, right?

2 (42m 13s):
Because civilization as a whole does not do new things, right? Groups of people do not do new things, right? These, these things don't happen automatically. Like by, by by default, nothing changes. The, the only way Civilizational change on any of these axes ever happens is because one of these people stands up and says, you know, no, I'm gonna do something different than what everybody else has ever done before. So the, this is the, this is progress. Like this is actually how it happens. Sometimes they get line as you're awarded. Sometimes they get crucified. Sometimes the crucification is literal. Sometimes it's just, you know, symbolic. But like, you know, the, they, they, they, they, they are those kinds of people. And then, and then, and then murders. Like when, when they go down in flames, like they have, and, and again, this is where it really screws the people's Moral judgments.

2 (42m 54s):
'cause everybody wants to have this sort of super clear story of like, okay, he did a bad thing and he was punished. And I'm, I'm like, no, no, no, no, no, no. He was the kind of person who was going to do great things and also was going to take on a level of risk and take on a level of sort of extreme behavior such that he was going to expose himself to flying too close to the sun wings melt, crashed to ground. But, but, but it's a package deal, right? The, the, the reason you have the Picassos and the Beethoven's and all these people is because they're willing to take these extreme level of Risks. They, they, they are that creative and original, not just in their art or their business, but in everything else that they do, that they will set themselves up to be able to fail psychologic. You know, a psychologist would probably, or psychiatrist would probably say, you know, maybe, you know, to what extent do they actually like have a death wish?

2 (43m 35s):
Do they, do they actually, you know, at some point, do they wanna punish themselves? Do they wanna fail? That, I don't know. But you see this, they, they, they, they deliberately move themselves too close to the sun. And, and, and you can, you can see it when it's happening. 'cause like if they get too far from the sun, they deliberately move back towards it, right? They, you know, they, they come right back and they, they, they want the risk. And, and so anyway, like I I, I, yeah, so Martyrs to Civilizational, Progress, like, this is how progress happens when these people crash and burn the national, you know, inclination is to judge them morally. I I tend to think we should basically say look like, and I, and I don't even know if this means like giving them a Moral pass or whatever, but it's like, look, like this is how Civiliz civilization progresses, and we need to at least understand that there's a self-sacrificial aspect to this.

2 (44m 15s):
That, that, that may be tragic and, and often is tragic. But it it is, it is quite literally self-sacrificial.

0 (44m 20s):
Are there any examples of great Innovators who were able to compartmentalize their Risk Taking to such a degree that they had what seemed to be a morally impeccable life? Yeah. In every domain except in their business pursuits.

2 (44m 37s):
Yeah, that's right. So some people are very, some people are very highly controlled like that. Some people are able to like, very narrowly. And I, I don't, I don't really wanna cite myself an example in a lot of this, but I, I will tell you, like, as an example, like, I, I never, I, I will never use debt in business, number one. You know, number two, like I have the most placid personal life, you can imagine. Number three, I'm not, I'm, I'm the last person in the world who's ever gonna do an extreme sport. I mean, I'm not even gonna go on the saw on the ice bath. Like, I'm not, I'm not doing any of this. Like, I don't doing anything. I'm not tele skiing, no

0 (45m 3s):
Obligation,

2 (45m 4s):
The ice, I'm not, I'm not on the titan. I'm not, you know, I'm not going down to see the Titanic.

0 (45m 8s):
Goodness, you weren't there.

2 (45m 8s):
I'm not doing any of this. I'm not doing any of this stuff. I have no interest. I don't play golf. I don't ski. I, I have no interest in any of this stuff, right? And so, like, there are ex, and I know people like this, right, who are very high achievers. It's just like, yeah. They, they're, they're, they're completely segmented. They're extreme risk takers in business. They're completely buttoned down on the personal side. They're completely buttoned down, you know, financially, they're, you know, they, they're scrupulous with following every rule in law you can possibly imagine. But, but they're still fantastic Innovators. And then I know many others who are just like, they've, their life is on fire all the time in every possible way. And whenever it looks like the fire is turning into embers, they figure out a way to like relight the fire, right? And they just really want to live on the edge. And so I, I think that's maybe, I think that's an independent variable.

2 (45m 49s):
And again, I would apply the same thing. I think the same thing applies to the arts, you know, classical music as an example. Like I think Bach was, you know, as an example, one of the, you know, kind of best, you know, musicians of all time had just a completely sedate personal life, you know, never had any, ever behavior at all. And his personal life, you know, fam, family man, tons of kids apparently, you know, pillar of the community, right? And so, like, if Bach could be Bach and yet not like burn his way through, you know, 300 mistresses or whatever, you know, maybe you can too.

0 (46m 16s):
So in thinking about these two different categories of Innovators, those that take on tremendous risk in all domains of their life, and those that take on tremendous risk in a very compartmentalized way. I don't know what the percentages are, but I have to wonder if in this modern age of the public being far less forgivable, what I'm referring to is Cancel Culture. Do you think that we are limiting the number of innovations in total, like by just simply frightening or eliminating an enormous category of Innovators because they don't have the confidence or the means or the strategies in place to regulate. So they're just either bowing out or they're getting crossed off.

0 (46m 57s):
They're getting canceled one by one.

2 (46m 58s):
So do you think the public is less tolerant than they used to be? Or more tolerant?

0 (47m 4s):
Well, the systems that I, I'm not going to be careful here. I think the, the large institution systems, yes, are not tolerant of what the public tells them they shouldn't be tolerant of. And so if there's enough noise, if there's enough noise in the mob, I think Institutions bow out. And here I'm referring not just to univer, they, they essentially say, okay, they let the cancellation proceed. Right? Or they, and then maybe they're the, maybe they're the gavel that comes down, but, but they're not the, the lever that got the thing going. And so I'm not just thinking about universities, I'm also thinking about advertisers. I'm thinking about the big movie houses that Cancel a film that a given actor might be in because they had something in their personal life that's still getting worked out.

0 (47m 48s):
I'm thinking about people who are in a legal process that's not yet resolved, but the public has decided they're a bad person, et cetera.

2 (47m 56s):
My question is, are we really talking about the public? I agree. I agree with your question and I I'm gonna come back to it, but I I, I'm gonna, I'm gonna, I I'm gonna examine one part of your question, which is, is this really the public we're talking about? And, and I would just say exhibit A is who is the current front runner for the Republican nomination today? And the public, at least on one side of the political aisle, seems very on board, right? Number two, like, look, you know, there's a certain musician who like, you know, flew too close to the sun, blew himself to Smithery, and he's still hitting all time highs on music streams every month. The public seems fine. Like I, I think the public might, I I I would argue the public is actually more open to these things than it actually maybe ever has been.

2 (48m 39s):
And we could talk about why that's the case. I I think it's a, it's a differentiation. And this is what, what your question was aiming at, but it's a differentiation between the public and the Elites. And so, so, so I, my view is everything that you just described as an Elite phenomenon, and actually the public is very much not on board with it.

0 (48m 53s):
Hmm. Interesting. And,

2 (48m 53s):
And, and, and, and so what's actually happening is the division, what, what's happened is the public and the Elites have gapped out the, the, the, the public is more forgiving of, of, of what previously might've been considered kind of ever and extreme behavior, right? It is, oh, f Scott Fitzgerald. There are no second acts in American lives. It turns out completely wrong. Turns out they're second acts, third acts, fourth acts. Apparently you can have a limited number of acts. The public is actually up for it.

0 (49m 14s):
Yeah. I mean, I think of somebody like Mike Tyson, right? I feel like he's, every, you know, his life exemplifies everything that's amazing and great and also terrible about America.

2 (49m 25s):
If we took Mike Tyson to dinner tonight at any restaurant anywhere in the United States, what would happen?

0 (49m 29s):
He would be loved.

2 (49m 30s):
Oh, he would be like adored. He would, the the outpouring of enthusiasm and passion and love would be incredible. Like, it would be unbelievable. This, this is a great example. Like it just like the, and, and again, I'm not even gonna draw more. I'm not even gonna say I agree with that or disagree with that. I'm just like, we, I think we all intuitively know that the public is just like 100%, like absolutely. Like he's a legend. Like he's a legend. He's a living legend. People love Mike. He's like a cultural touchstone. Absolutely. And you see it when he shows up in movies, right? He shows, I don't remember the, I mean the big breakthrough, I figured this out with respect to him 'cause I don't really follow sports, but when he showed up in that, it was that first hangover movie and he shows up and the, the, you know, it was, I was in a theater and like the, the audience just goes bananas. Crazy. They're so excited to see him.

0 (50m 9s):
Yeah. He evokes delight. I always say that Mike Tyson is the only person I'm aware of that can wear a shirt with his own name on it. And, it somehow doesn't seem wrong. In fact, it just kind of makes you like him more. Yeah. It's, it, his ego feels very contoured in a way that he knows who he is and who he was. And, and yet there's a, there's a humbleness woven in maybe as a consequence of all that he's been through. I don't know. But yeah, people love Mike Public

2 (50m 36s):
Loves him now. Exactly. Now, you know, if he shows up to like, lecture at Harvard, right? Like, I think you're probably gonna get a different reaction. I don't know.

0 (50m 43s):
Well, I don't know. I mean, David Simon, you know, the, the guy who wrote the wire gave a, gave a talk at Harvard and And. it sounded to me, based on his report of that, which is very interesting in fact that people adore people who are connected to everybody in that way. Like, I feel like everybody loves Mike from, from above his status, the sides below his status. He's just sort of, he, he occupies this, this halo of, of love and adoration. Okay. Alright.

2 (51m 13s):
Yeah. Yeah. And then look, the, the other side of this is the, is is the Elites. And you kind of alluded to this at the institution. So, so basically it's like the people who are like, at least nominally in charge or feel like that they should be in

0 (51m 23s):
Charge. Yeah. I wanna make sure we define Elite. So you're not necessarily talking about people who are wealthy, you're talking about people who have authority within Institutions. So

2 (51m 29s):
The ultimate definition of an Elite is who can get who fired, Right? Like that's the, the ultimate tech who can get, who fired, boycotted, blacklisted, ostracized. Like when push prosecuted, jailed, like when push comes to shove, right? I think that's always the question who can destroy whose career? And of course you'll notice that that is heavily asymmetric when they, when these fights play out. Like there's very clear which side can get the other, the other side fired and which side, which side can't. And so yeah, so look, I think we, we live in a period of time where the Elites have gotten to be extreme in a number of dimensions. And, and I think characterized by for sure extreme Group, Think extreme, sanctimony extreme, you know, Moral, you know, I would say dungeon, you know, extreme, you know, this, this weird sort of modern puritanism and then an extreme sort of morality of like punishment and terror against their perceived enemies.

2 (52m 17s):
But, but, but I wanted to, I wanted to go through that because I actually think, I actually think that's a very different phenomenon. I think what's happening with the Elites is very different than what's happening in the, in the population at large. And, and then of course I, I think there's a feedback loop in there, which is I think the population at large is not on board with that program, right? I think the Elites are aware that the population is not on board with that program. I think they judge the population negatively as a consequence that causes the Elites to harden their own positions that causes them to be even more alienating to the population. And so they're, you know, they're in sort of an oppositional negative feedback loop. And yeah, it's gonna be, and you know, it's, and but again, it's a sort of question, okay, who, who can get who fired? And so, you know, Elites are really good at getting like normal people fired, ostracized, banned, you know, hit pieces in the press, like whatever, you know, for normal people to get Elites fired, they have to really light band together, right?

2 (53m 3s):
And really mount a serious challenge, which mostly doesn't happen, but, but, but might be starting to happen in some cases.

0 (53m 8s):
Do you think this power of the, of the Elites over stemmed from Social Media sort of going against its original purpose? I mean, when you think Social Media, you think you're giving each and every person their own little reality TV show their own voice. And yet we've seen a dramatic uptick in the number of cancellations and firings related to immoral behavior based on things that were either done or amplified on Social Media, right? It's almost as if the public is holding the wrong end of the knife.

2 (53m 41s):
Yeah. So the way I describe it, so, so I, so I use these two terms and they're somewhat interchangeable, but Elites and Institutions, and then there they're somewhat interchangeable. 'cause who runs the Institutions? The Elites, right? And so it's, it's a, it's a sort of a self self-reinforcing thing. You know, Institutions of all kinds Institutions everything from, you know, the government bureaucracies, companies, nonprofits, foundations, NGOs, you know, tech companies, you know, on and on and on. Like, you know, people who were in, people who were in charge of big complexes and that that carry a lot of basically power and influence and capability and money as a consequence of their positional authority, right? So, you know, the head of a giant foundation may never have done anything in their life that would cause somebody to have a high opinion of them as a person, but they're in charge of this, you know, gigantic multi-billion dollar complex and have all this power results.

2 (54m 24s):
And so that's just to define terms, at least Institutions. So it's actually interesting, Gallup has been doing polls on the following que on the question of, of trust in Institutions, which is sort of a therefore proxy for trust in Elites, basically since the early 1970s. And What you find, and, and they do this across all the categories of big Institutions, you know, basically every, every, everyone I just talked about a bunch of others, big business, small business banks, newspapers, broadcast television, the military police. And so they've got like 30 categories or something. And basically what you see is almost all the categories basically started in the early seventies at like 60 or 70% trust. And now they've basically almost across the board, they've just done had a complete basically linear slide down for 50 years, basically my, my whole life.

2 (55m 9s):
And you know, they're, they're, they're now bottoming out, you know, congress and journalists bottom out at like 10%, right? Like the two, the two groups everybody hates are like congress and journalists. And then it's like a lot of other big Institutions are like 20 in their twenties, thirties, forties, actually big business actually scores fairly high tech actually scores quite high. The military score is quite high, but basically everything else has really caved in. And so, so this is sort of my fundamental challenge to everybody who basically says, and you, you didn't do this, but you, you'll hear this, the simple form of this, which is Social Media caused the current trouble, and it's called this an example, collapse in faith in Institutions and, and Elites, let's call that part of the current trouble. Everybody's like, well, Social Media caused that.

2 (55m 49s):
I was like, well, no, Social, Media, Social Media is new, right? In the last, you know, Social Media is effectively new, practically speaking since 20 10, 20 12 is when it really took off. And so if the trend started in the early 1970s, right? And, it has been continuous, then we're dealing with something broader. And, and, and, and Martin Gry wrote, I think the best book on this called The Revolt of the Public, where he goes through this in detail. And, and he, he, he does, he does say that Social Media had a lot to do with what's happened in the last decade. But he says, yeah, if you go back, you look further. It was basically two things coinciding. One was just a general change in the media environment and in particular in the 1970s is when you started to, and especially in the 1980s, is when you started to get specifically talk radio, which was a new outlet.

2 (56m 30s):
And then you also start, you also got cable television. And then you also, by the way, it's actually interesting, in the fifties, sixties, you had paperback books, which was another one of these, which was an outlet. So you, you, you, you had like a fracturing in the media landscape. It started in the fifties through the eighties, and then of course, the internet like blew it wide open. Having said that, if the Elites and the Institutions were fantastic, you would know it more than ever, information is more accessible. And so the other thing that he says and I agree with is the, the public is not being tricked into thinking the Elite Institutions are bad. They're, they're, they're learning that they're bad, right? And, and, and, and the mystery, and therefore the mystery of the Gallup poll is why those numbers aren't all just zero, right? Which is ar you know, arguably, you know, in a lot of cases where they should be,

0 (57m 8s):
I think one reason that, oh,

2 (57m 9s):
And by the way, he, he thinks this is bad. So he, he and I have a different view. So here's where he and I disagree. He thinks this is bad. So he, he's, he basically says, you, you can't replace Elites with nothing. You can't replace Institutions with nothing. If, if, if, 'cause what you're just left with is just gonna be wreckage. You're gonna left with a a completely basically, you know, atomized outta control society that has no ability to marshal, you know, any sort of activity in any direction. It's just gonna be a dog eat dog awful, you know, world. I have a very different view on that, which we could, which we could talk about.

0 (57m 35s):
Yeah, I'd love to, I'd love to hear your views on that. Yeah.

Inside Tracker (57m 38s):
I'd like to take a quick break and acknowledge our Sponsor Inside Tracker Inside Tracker is a personalized nutrition platform that analyzes data from your blood and d n a to help you better understand your body and help you meet your health goals. I'm a big believer in getting regular blood work done for the simple reason that many of the factors that impact your Immediate and long-term health can only be analyzed from a quality blood test. However, with a lot of blood tests out there, you get information back about blood lipids, about Hormones and so on, but you don't know what to do with that information. With Inside Tracker, they have a personalized platform that makes it very easy to understand your data. That is to understand what those lipids, what those hormone levels, et cetera mean, and behavioral supplement nutrition and other protocols to adjust those numbers to bring them into the ranges that are ideal for your Immediate and long-term health Inside Tracker's ultimate plan now includes measures of both A P O B and of insulin, which are key indicators of cardiovascular health and energy regulation. If you'd like to try Inside Tracker, you can visit InsideTracker dot com slash Huberman to get 20% off any of inside tracker's plans. Again, that's InsideTracker dot com slash Huberman to get 20% off.

0 (58m 45s):
The, the quick question I was gonna ask before we go there is, I think that one reason that I and many other people sort of reflexively assume that Social Media caused the, the demise of our faith in Institutions is, well, first of all, I wasn't aware of this lack of correlation between the, the decline in faith in Institutions and, and the rise of Social Media, but secondarily that we've seen some movements that have essentially rooted themselves in tweets, in comments, in posts that get amplified. And those tweets and comments and posts come from everyday people.

0 (59m 24s):
In fact, I can't name one person who initiated a given cancellation or movement because it was the sort of dog piling or mob adding on to some person that was essentially anonymous. So I think that for many of us, we, we have the bottom to use Neuroscience language's, sort of a bottom up and a perspective. Oh, you know, someone sees something in their daily life or experiences something in their daily life and they tweet about it, or they comment about it, or they post about it, and then enough people dog pile on the accused that it picks up force and then the Elites feel compelled, obligated to Cancel somebody. And that tends to be the narrative.

0 (1h 0m 5s):
And so I think the logical conclusion is, oh, you know, Social, Media allows for this to happen. Whereas normally someone would just be standing on the corner shouting or calling lawyers that don't have faith in them. And you know, you've got the, like the Aren Brockovich model of a, you know, you know, that turns into a movie, but that's a rare case of this lone woman who's got this idea in mind about how big institution is, is doing wrong or somebody is doing wrong in the world and then can leverage big Institutions, excuse me. But the way that you describe it is that the Elites are, are leading this Oh yeah, this shift. Yeah. So a hundred percent. So what is the role of the public in it? I mean, I mean, just to give it a concrete example, if for instance, no one tweeted or commented a me too, or no one tweeted or commented about some ill behavior of some, I don't know, university faculty member or business person, would the Elite have come down on them anyway?

2 (1h 1m 1s):
Oh yeah. So what's happening? So would, would, would I I, based on what I've seen over the years is, is it's, it's, it, there is so much astroturfing right now. There, there are entire categories of people who are paid to do this. Some of them we call journalists, some of them we call activists, some of them we call, you know, N G O, you know, nonprofits, some of them we call university professors, like some of them we call grad students. Like whatever they're paid to do this, you know, re I dunno if you've ever looked into the misinformation industrial complex. There's this whole universe of basically these funded groups that basically do quote unquote misinformation, you know, quote unquote misinformation. And, and they're constantly mounting these kinds of attacks. They're constantly trying to gin up this kind of, basically Panic to cause somebody to get fired. Like it.

0 (1h 1m 40s):
So it's not a grassroots, it's no,

2 (1h 1m 42s):
It's the opposite of grassroots. No, it, and almost always when you trace these things back, it was a, it was a journalist, it was an activist, it was a, it was a pub, it was a public figure of some kind. Hmm. These are entrepreneur. There, there, there's, these are entrepreneurs in a, in a sort of a weird way. Like they're, they're basically, they're, they're paid their, their job mission calling in. It's all wrapped up together. Like they're true believers, but they're also getting paid to do it. And there's a giant funding, I mean, there's a very large funding complex for this coming from, you know, certain I high profile people who put huge amounts of money into this. Is

0 (1h 2m 11s):
This well known?

2 (1h 2m 12s):
Yes. Well, so I mean, it is in my world. So this is what the Social Media companies have been on the receiving end of for the last decade is it's this, it's it's basic ba it's basic, it's basically a political media activism complex with very deep pockets behind it. And you've got people who basically, literally people who sit all day and watch the TV network on the other side or watch the Twitter feeds the other side, and they wait, they, they basically wait. It's like, well, every politician, this has been the case for a long time now. Every, every politician who goes out and gives Stu speeches, you'll see there's always somebody in the crowd with a camcorder or now with a phone recording them. And that's somebody from the other campaign who's paid somebody to just be there and like record every single thing the politician says. So that, so that when AEM Mitt Romney says whatever, the 47% thing, they've got it on tape and then they clip it and they, and they try to make it viral.

2 (1h 2m 52s):
So the this stuff is, and again, like, look like these people believe what they're doing. I'm not saying it's even dishonest. Like these people believe what they're doing. They think they're fighting a holy war, they think they're protecting democracy, they think they're protecting civilization, they think they're protecting whatever it is they're protecting. But, but they, and then they know how to use the tools. And so they, they know how to, they, they know how to try to gin up the outrage. And then by the way, sometimes it works in social cascades, sometimes it works, sometimes it doesn't. Sometimes they cascade, sometimes they don't. But if you follow these people on Twitter, like this is what they do every day. They're constantly trying to, like, life is fire, right? Wow. And

0 (1h 3m 23s):
So I assume And it was really bottom up, but, but it sounds like it's sort of a mid level and then it, and then it captures the Elites and then the thing takes on a life of its own.

2 (1h 3m 31s):
By the way, it also intersects with the trust and safety groups at the Social, Media firms, right? Who are responsible for figuring out who gets promoted and who gets banned right across this. And you'll notice one large Social Media company has recently changed hands and has implemented a different kind of set of trust and safety, and all of a sudden a different kind of boycott movement has all of a sudden started to work that wasn't working before that. And in another kind of boycott movement is not working as well anymore. And so there's an inter like for sure there's an intermediation happening. Like look, the the stuff that's happening in the world today is being intermediated through Social Media. 'cause Social Media is the defining media of our time, but there are people who know how to do this and, and, and, and do this for a living. So, so no, I I very much view this as a, i I view very much the like, cancellation wave, like this whole thing.

2 (1h 4m 12s):
It's, it's a, it's a, it's, it's an, it's an Elite phenomenon. And when it appears to be a grassroots thing, it's, it's, it's, it's either grassroots among the Elites, which, you know, is, is possible. 'cause there's, you know, a fairly large number of people who are like signed up for that, you know, that particular crusade. But there's also a lot of astroturfing that's taking place inside that. The question is, okay, at what point does the population at large get pulled into this? And, and maybe that maybe there are movements at certain points in time where they do get pulled in and then maybe later they get disillusioned. And so then there's some question there, and then there's another question of like, well, if the population at large is gonna decide what these movements are, are they gonna be the same movements? Is that the Elites want? And are the Elites, how are the Elites gonna react when the population actually like fully expresses itself? Right? And so there's, and like I said, there's a feedback loop between these where the more extreme the Elites get, they tend to push the population to more extreme views on the other side and vice versa.

2 (1h 4m 58s):
So it ping ping pongs back and forth. And so yeah, this is, yeah, this is our world. Yeah,

0 (1h 5m 2s):
This explains a lot. Yeah. I wanna make sure that I, by the way,

2 (1h 5m 6s):
Matt Taibbi, so Mike Shellenberger, Matt Taibbi, a bunch of these guys have done a lot of work. The, the, just, if you just look into what's called the Misinformation industrial complex, you'll find a network of money and power that is really quite amazing.

0 (1h 5m 19s):
I've seen more and more shellenberger showing up, right?

2 (1h 5m 23s):
And he iss just looking, he's just on this stuff. He and he and Tia, and they're just, they're literally just like tracking money. I, it's just like, it's, it's very clear how the money flows, including like a remarkable amount of money out of the government, you know, which is of course, like in theory, very concerning, very interesting. The government should not be funding programs that take away people's constitutional rights. And yet somehow that is what's been happening.

0 (1h 5m 44s):
Very interesting. Yes. I wanna make sure that I hear your ideas about why the decline in confidence in Institutions and is not necessarily problematic. Is this going to be a total destruction burning down of the forest that will lead to new life? Is that your view?

2 (1h 6m 3s):
Yeah. Well, so this, this is the thing. And, and look, there's a question if you're, there's a couple questions in here, which is just like, how, how bad is it really? Like how bad are they? Right? And I, I, you know, I think they're pretty bad. A lot of 'em are pretty actually bad. And so, so, so, so that's one big question. And then, yeah, look, the other question is like, okay, if an institution has gone bad or a group of Elites have gone bad, like can, can, it's this wonderful word, Reform, right? Can, can, can they be reformed? And everybody always wants to Reform everything, and yet somehow, like nothing ever quite ever gets reformed, right? And so people have been trying to Reform, you know, housing policy in the Bay Area for decades, and, you know, we're not building, we're building fewer houses than ever before. So somehow Reform movement seemed to lead to more just, just, just more bad stuff. But anyway, yeah. So if you have an existing institution, can it be reformed?

2 (1h 6m 44s):
Can it be fixed from the inside? Like what's happening in universities? Like, there's a lot of, there are professors that Stanford as an example, who very much think that they can fix Stanford. Like, I, I don't know what you think. It doesn't seem like it's going in productive directions right now.

0 (1h 6m 56s):
Well, I mean, there are many things about Stanford that function extremely well. I mean, it's a big institution. It's certainly got its issues like any other place. They're also my employer. Mark's giving me some interesting looks. He wants me to get a little more vocal here. No, no, no. You don't need to. Yeah. Oh, no, I, I mean, I think that, I didn't mean to put you on the spot. Yeah. I mean, one of the things about being a researcher at a big institution like Stanford is, well, first of all, it meets the criteria that you described before. You know, you look to the left, you look to the right or anywhere above or below you, and you have excellence, right? I mean, I've got Nobel Prize winner below me whose daddy also won know about prize. And there is scientific offspring is likely to win. I mean, you, it, it inspires you to do bigger things than than one ordinarily would no matter what.

0 (1h 7m 36s):
So there's that, and that's great. And that persists, there's all the bureaucratic red tape about trying to get things done and how to implement decisions is very hard. And there are a lot of reasons for that. And then of course, there are the things that, you know, many people are aware of are there are public accusations about people in positions of great leadership. And that's getting played out. And it's, and the whole thing becomes kind of overwhelming and a little bit opaque when you're just trying to like run your lab or live your life. And so I think one of the reasons for this lack of Reform that you're referring to is because there's no position of reformer, right? So deans are dealing with a lot of issues. Provosts are dealing with a lot of issues.

0 (1h 8m 16s):
Presidents are dealing with a lot of issues and, and then some in some cases. And so, you know, we don't have a dedicated role of reformer someone to go in and say, listen, there, there's just a lot of fat on this and we need to trim it, or we need to create this or do that. There's, there just isn't a system to do that. And that's, I think in part because universities are, are built on old systems and they're, you know, it's like build, it's like the New York subway, like it's still, it's amazing. It still works as well as it does. And yet it's got a ton of problems also. Yeah.

2 (1h 8m 45s):
Well look, so, so, so the point, the point we could, we could debate the, the, the university specifically, but the, the point is like, look, if you do think history system going bad, and then you have to make it number one, you have to figure out if you think Institutions are going bad. The, the population largely does think that. And then at the very least, the people who run Institutions ought to really think hard about what that means. But people

0 (1h 9m 1s):
Still strive to go to these places. And I still hear from people who, like for instance, did not go to college, are talking about how a university degree is useless. They'll tell you how proud they are that their son or daughter is going to Stanford or is going to U C L A or is going to urban ban of champagne. I mean, it's almost like, to me that's always the most, you know, shocking contradiction is like, yeah, like these Institutions don't matter. But then when people wanna hold up a card that says why their kid is great, right? It's, it's not about how many pushups they can do or that they started their own business. Most of the time it's, they're going to this university and I think, well, what's going on here? So

2 (1h 9m 36s):
Do you think the median voter in the United States can have their kid go to Stanford? No.

0 (1h 9m 39s):
No, no. And, and, and, and you think

2 (1h 9m 41s):
A median voter in the United States could have their kid admitted to Stanford even with perfect s a t?

0 (1h 9m 46s):
No, no, no. In this day and age, the competition is so fierce that it requires more,

2 (1h 9m 51s):
Yeah. So like, so first of all, again, we're, we're dealing here, yes, we're dealing with there with a small number of very Elite Institutions people may admire them or not. Most people have no connectivity to them whatsoever in the statistics in the polling universities are not doing well, you know, the, the population at large, yeah, they may have fantasies about their kid gonna Stanford, but like, the reality of it is they have a very, very, I say a collapsing view of these Institutions. So, so anyway, so this, this actually goes straight to the, the question of alternatives then, right? Which is like, okay, if, if you believe that there's collapsing faith in the Institutions, if you believe that it is merit, at least in some ways, if you believe that Reform is effectively impossible, then you are faced w w And it. We could debate each of those, but like the population at large seems to believe a lot of that.

2 (1h 10m 32s):
Then there's a question of like, okay, like can, can it be replaced? And if so, like, are you better off replacing these things basically while the old things still exist? Or do you actually need to basically clear the field to be able to have the new thing exist? The universities are a great case study of this because of the student, because of how student loans work, right? And the, and the way student loans work is to, to be able to be a, to be, to be an actual competitive university compete, you need to have access to federal student lending. 'cause if you don't, everybody has to pay out of pocket And. It's completely outta reach for anybody other than, you know, a certain class of either extremely rich or foreign students. So you need access to federal student loan facility to get access to the federal student loan facility. You need to be an accredited university. Guess who runs the accreditation council?

0 (1h 11m 12s):
I

2 (1h 11m 12s):
Don't know the existing universities, right? So it's a, it's a, it's a, it's a, it's a, it's a, it's a self laundering machine. Like they, they decide who the new universities are. Guess how many new universities get accredited right? Each year to be able zero zero, right? And so as long as that system is in place, and as long as they have the government wired the way that they do, and as long as they control who gets access to federal student loan funding, like of course there's not gonna be any competition, right? Of course there can't be a new institution that's gonna be able to get to scale. Like, it's not possible. And so if you actually wanted to create a new system that was better in the, you know, I would argue dozens or hundreds of ways, it could obvious obviously be better if you were starting today, it probably can't be done as long as the existing Institutions are actually intact.

2 (1h 11m 53s):
And this is my counter against to Martin, which is like, yeah, we look, we, we, if we're gonna tear down the old, there may be a period of disruption before we get to the new, but we're never gonna get to the new if we don't tear down the old,

0 (1h 12m 3s):
When you say counter to Martin, you're talking about the author of Revolt of the Public? Yeah,

2 (1h 12m 6s):
Martin Gury. Yeah, Martin Gury says is like, look, he said basically what Martin says, what Martin says is as follows, the Elites deserve contempt, but the only thing worse than these Elites that deserve contempt would be no Elites at all. Right? And, and, and because, and he basically says on the other side, on the other side of the destruction of the Elites and the Institutions is, is nihilism. You're basically left with nothing. And then by the way, there is a nihilistic streak. I mean, there's a nihilistic streak in the, in the culture, in the politics today. There are people who basically would just say, yeah, just tear, tear the whole system down. Right? And they're there without any particular plan for, for what follows. And so you, you, I I think he, he makes a good point in that you wanna be careful that you actually have a plan on the other side that you think is actually achievable. But, but the, the, but again, the counterargument to that is if you're not willing to actually tear down the old, you're not gonna get to the new.

2 (1h 12m 51s):
Now what's interesting, of course, is this is what happens every day in business, right? So like the entire way, like how do, how do you know that the capitalist system works the way that you know, is that the old companies, when they're no longer like the best at what they do, they get torn down and then they ultimately die and they get replaced by better companies. Yeah.

0 (1h 13m 8s):
I haven't seen a Sears in a

2 (1h 13m 9s):
While. Exactly. Yeah. Right? And we know what's, what's so interesting is we know in, in capitalism, in a market economy, we know that that's the sign of health, right? That that's the sign of how the system is working properly, right? And in fact, we get actually judged by antitrust authorities in the government on that basis, right? It's like the, the best defense against antitrust charges is no, people are like coming to kill us and they're doing like a really good job of it like that, that that's how we know we're doing our job. And in fact, in business, we are specifically, it is specifically illegal for companies in the same ministry to get together and plot and conspire and plan and have things like these accreditation bureaus. Like we, we would get, if I created the equivalent in my companies of the kind of accreditation bureau that the universities have, I'd get straight to federal prison and a trust violation Sherman Act straight to prison. People have been sent to prison for that.

2 (1h 13m 50s):
So in the business world, we know that you want everything subject to market competition. We, we know that you want creative destruction, we know that you want replacement of the old with, with superior new. It's just once we get outside of business, we're like, oh, we don't want any of that. We want, we want basically stagnation and log rolling, right? And it, you know, and basically, you know, institutional, you know, incestuous like, you know, entanglements and conflicts of interest, you know, as far as the eye can see. And then we're surprised by the results.

0 (1h 14m 15s):
So let's play it out as a bit of a thought experiment. So let's say that one small banding together of people who wanna start a new university where free exchange of open ideas, where unless somebody has, you know, egregious behavior, violent behavior, you know, truly sexually inappropriate behavior against somebody, you know, they're committing a crime, right? They're allowed to be there, they're allowed to be a student or a faculty member or administrator. And let's just say this accreditation bureau allowed student loans for this one particular university. Or let's say that there was an independent source of funding for that university, such that students could just apply there. They didn't need to be part of this, you know, this Elite accredited group, which is sounds very mafia like, frankly, not necessarily violent, but certainly coercive in this, in and in the way that it walls people out.

0 (1h 15m 7s):
Let's say that then there were 20 or 30 of those, or 40 of those. Do you think that over time that model would overtake the, the existing model?

2 (1h 15m 18s):
Isn't it interesting that those don't exist, right? Remember the Sherlock Holmes, the dog that didn't bark?

0 (1h 15m 24s):
It is interesting. Those don't

2 (1h 15m 26s):
Exist. Yeah. Right. So there's, there's two possibilities. One is like nobody wants that, which I don't believe. And then the other is like, the system is wired in a way that will just simply not allow it. Right? And you, you did a hypothetical in which the system would allow it. My response to that is, no, of course the system won't allow that.

0 (1h 15m 40s):
Or the people that band together, you know, have enough money or Yeah. Or get enough resources to say, look, we can, you know, we can afford to give loans to, you know, 10,000 students per year. You know, 10,000 isn't a trivial number when thinking about the size of a university. And, and, you know, they, most of them hopefully will graduate in four years and there'll be a, a turnover. And do you think that the great future Innovators would tend to orient toward that model more than they currently do toward the traditional model? And, What, I'm trying to get back to here, is how do you think that the current model thwarts innovation as well as maybe some ways that it still supports innovation, certainly cancellation, and the, the risk of cancellation from the way that we framed it earlier is going to discourage risk takers, right?

0 (1h 16m 26s):
Of the category of risk takers that take risk in e every d every domain that really like to fly close to the sun and sometimes into the sun, right?

2 (1h 16m 33s):
Or are doing research that is just not politically

0 (1h 16m 36s):
Right. Yeah. Looking into issues that pal, right? That, that, you know, we can't even talk about on this podcast probably without, without causing a distraction of what we're actually trying to talk about.

2 (1h 16m 46s):
It gives up the whole game right there though, right?

0 (1h 16m 47s):
Exactly. So the, you know, I keep a file and it's a written file because I'm afraid to put it in a, into electronic form of all the things that I'm afraid to talk about publicly. Yeah. Because I come from a lineage of advisors who are all three died young. And I figure if nothing else, I'll die. And then, you know, I'll make it into the world and, you know, when, let's say five, 10 years, 20 years. And if not, you know, I know with certainty I'm gonna die at some point. And then we'll, we'll see where all those issues stand in any event. Is,

2 (1h 17m 15s):
Is that list getting longer over time or shorter? Oh,

0 (1h 17m 17s):
It's definitely getting longer.

2 (1h 17m 18s):
Isn't that

0 (1h 17m 18s):
Interesting? Yeah, it's get, it's getting much longer. I mean, there are just so many issues that I would love to explore on this podcast, podcast with experts and that I can't explore just be, even if I had a panel of them, because of the way that things get sound byed and segmented out and taken out of context, it's like the whole conversation is lost. And so unfortunately there are an immense number of equally interesting conversations that I'm excited to have. But it is, it is a little disturbing.

2 (1h 17m 49s):
Do you, do you remember Lysenkoism?

0 (1h 17m 52s):
No.

2 (1h 17m 52s):
Oh, so ly so famous in the Soviet history of the Soviet unionist is a famous thing. So there was a genetic geneticist named Lysenko. Yeah. That's

0 (1h 17m 58s):
Why it sounds familiar, but I'm not, I'm not calling to mind. Well, he was the guy

2 (1h 18m 1s):
Who did, he did communist genetic genetics. The, the field of genetics. The Soviets did not approve the field of genetics because of course they believed in the creation of the new man and, you know, totally quality and genetics did not support that. And so if you were doing like traditional genetics, you were, you know, gonna be, you know, at the very least fired, if not, if not killed. And so this guy, LA Sanko stood up and said, oh, I've got Marxist genetics, right? I've got like a whole new field of genetics that basically is politically compliant. And then they actually implemented that in the agriculture system of the Soviet Union. And it's, it's the origin of one of the big reasons that the Soviet Union actually fell, which was they ultimately couldn't feed themselves,

0 (1h 18m 35s):
So create a new notion of biology as it relates to genetics,

2 (1h 18m 38s):
Political, politically biology. Right. And so and so, and they, they, they not only created it, they taught it, they they mandated it, they required it, and then they implemented it in agriculture. Interesting. So it, so yeah. So it's, it's, I I never understood there, there was a bunch of things in history I never understood until the last decade. And that's one of them.

0 (1h 18m 52s):
Well, I censor myself at the level of deleting certain things. Yeah. But I don't contort what I do talk about. Yeah. So I tend to like to play on lush open fields. Yeah. Just makes my life a lot

2 (1h 19m 3s):
Easier. But this go, this goes to the rot. This goes to the rot. And I'll come back to your question, but like, this goes to the rot in the existing system, which is we, we've, and by the way, I'm no different. I'm just like you, like I'm not, I'm trying not to light myself on fire either, but like the, the rot in the existing system, and by system I mean the Institutions and the Elites, the, the rot is that the set of things that are no longer allowed. I mean, that list is like obviously expanding over time and like that's a real, like historically speaking that doesn't end in good places is

0 (1h 19m 28s):
Is this group of a particular generation that we can look forward to the time when they eventually die off.

2 (1h 19m 33s):
It's sort of the boomers plus the millennials. So

0 (1h 19m 36s):
We got a while.

2 (1h 19m 36s):
Good, good news. Bad news. Yeah. I mean, Gen, Gen X is weird, right? Gen. I'm Gen Gen X. Gen X is weird. 'cause we, we kind of slipped in the middle. We're we're kind of, we, we were kind of the, I don't like, I don't know how to describe it. We were the kind of nonpolitical generation kind of sandwich between the boomers and the millennials. Gen Z is a very o I think, open question right now, which way they go. I could imagine them being actually like much more intense than the millennials on all these issues. I could also imagine them reacting to the millennials and being far more open-minded.

0 (1h 20m 2s):
We don't know which way it's gonna go yet. We know

2 (1h 20m 3s):
Where it's gonna go. Might be different groups of them.

0 (1h 20m 5s):
I mean, I'm Gen X also. I'm 47. You're okay.

2 (1h 20m 7s):
50 52.

0 (1h 20m 8s):
Yeah. 52. Right? So more or less same. So grew up with some John Hughes films and so where the, the jocks and the hippies and the punks and the were all divided and they were all segmented. But then it all sort of mishmash together a few years later. And I think that had a lot to do with, like you said, the sort of apolitical Yeah. Aspect to our generation. I, but

2 (1h 20m 28s):
We just knew, we, the Gen X just knew the boomers were nuts, right? Like all the, I mean this is the, the, the canonical, right? The one of the great sitcoms of, of, of the era was family ties, right? With character Michael p Keaton. and he was just like, this guy is just like, yeah, the boomer, my boomer hippie parents are crazy. Like, I'm just gonna like go into business and like actually do something productive. Like there, there was something like iconic about that character in our culture. And you know, people like me were like, yeah, obviously you go into business, you don't go into like political activism. And then, you know, it's just like, man, that came whipping back around with the next generation. So just to touch real quick on the university thing. So look, there are people trying to do, and I'm actually gonna do, I think this afternoon with the University of Austin, which is, which is, which is, which is one of these. And so there are people trying to do new universities. You know, I, I like, I would say it's certainly possible.

2 (1h 21m 8s):
I hope they succeed. I'm pulling for them. I I think it'd be great. I think it'd be great if there were a lot more of them who

0 (1h 21m 12s):
Founded this university.

2 (1h 21m 14s):
This is a whole group of people. I don't want to freelance on that. 'cause I don't know originally who the idea was University

0 (1h 21m 19s):
Of Austin, not UT Austin. Yeah.

2 (1h 21m 21s):
So this is not UT Austin. Okay. It's called, it's called University of Austin, or they call it, I think it's U a t or what is it? U A T X. And so, and it's a lot of very sharp people associated with it. And, and they're, they're, they're gonna try to very, very much exactly like what you're describing. They're gonna try to do a new one. I would just tell you like the wall of opposition that they're up against is profound. Right. And part of it is economic, which is can they ever get access to federal student lending? And I, you know, I, I hope that they can. But I, I've, I, you know, it seems nearly inconceivable the way the system's rigged today. And then, you know, the other is just like they're gonna, they, they already have come under, I mean, any, any anybody, any, anybody who publicly associates with them who is in traditional academia immediately gets lit on fire.

2 (1h 22m 1s):
Right. And there's like, you know, cancellation campaigns, like, so they're up against a wall of social ostracism. Wow. They're up against a wall of, you know, press attacks. Wow. They're up against a wall of, you know, people just like doing the thing. Pou pouncing on any, anytime anybody says anything, they're gonna try to like burn the place down. This reminds

0 (1h 22m 15s):
Me of like, like Jerry Springer episodes and Geraldo Rivera episodes where, you know, it's like if a teen listened to, you know, like Danzig or Marilyn Manson type music or Metallica, that they were considered a devil worshipper. Yeah, that's right. Like we right Now, we just laugh, right? We're like, that's crazy, right? People listen to music with all sorts of lyrics and ideas and looks and, and like, that's crazy. But you know, there were people legitimately sent to prison, I think with the West Memphis three, right? These kids out in West Memphis that looked different, acted different, and were accused of murders that eventually was made clear. They clearly didn't commit, but they were imprisoned because of the music they listened to.

0 (1h 22m 58s):
I mean, this sounds very similar to that. And I remember seeing bumper stickers free the West Memphis three, and I thought this was some crazy thing. And you look into it, and this isn't, it's a little bit niche, but I mean, these were real lives and there was a active witch hunt for people that looked different and acted different. And yet now we're sort of in this inverted world where on the one hand we're all told that we can express ourselves however we want. But on the other hand, you can't get a bunch of people together to take classes where they learn biology and sociology and econ in Texas Wild.

2 (1h 23m 34s):
Yes. Well, so the, the, you know, the simple explanation is it's, this is Puritanism, right? So this is the original American Puritanism Puritanism that just like, works itself out through the system in different ways, at different times. You know, there's this phenomenon, there's a religious phenomenon in America called the, the Great Awakenings where there's just, there there'll be these periods in American history where there's basically religiosity fades, and then there'll be this snapback effect where you'll have this basically this, you know, frenzy basically of, of religion, of, you know, in the old days it would've been, you know, tent revivals and people speaking in tongues and all this stuff. And then in the modern world, it's, it's, you know, it's, it's of the form that, that we're living through right now. And so, yeah, it's just basically these waves of sort of American religious and, and you know, remember like religion in our time, religious impulses in our time don't get expressed, you know?

2 (1h 24m 14s):
'cause we live in, we live in more advanced times, right? We live in scientifically informed times. And so religious impulses in our time don't show up as overtly religious. Right. They show up in a secularized form. Right. Which of course, conveniently is, is, is therefore not subject to the First amendment separation church and state. Right. As long as the church is secular, there's no problem. Right. And so, but, but we're, we're acting out these kind of religious scripts over and over again, and we're in the middle of another religious frenzy.

0 (1h 24m 37s):
There's a phrase that I hear a lot, and I don't necessarily believe it, but I want your thoughts on it, which is the pendulum always swings back. Yeah.

2 (1h 24m 47s):
Not quite well.

0 (1h 24m 48s):
So that's how I feel too, because, you know, I'll take, boy, that'll be great. Take any number of things that we've talked about and you know, gosh, it's so crazy, you know, the, the way things have have gone with Institutions or it's so crazy the way things have gone with Social Media or it's so crazy. Fill in the blank. And people will say, well, you know, the pendulum always swings back. Yeah. Like, like the, like it's the stock market or something. Right. You know, after every crash there'll be an eventual boom. Right. And vice versa, by

2 (1h 25m 16s):
The way, that's not true either. Right, right. Most, most stock markets we have, of course, Survivorship, but it's all Survivorship, everything is Survivor. It's all, everything you just said is obviously Survivorship Bias. Right? So if you look globally, most stock markets over time, crash and burn and ever recover. The American stock market has always recovered.

0 (1h 25m 30s):
Right? I was referring to the American stock market. Yeah. Right. But

2 (1h 25m 33s):
Globally, but the reason everybody refers to the American stock market is 'cause it's the one that doesn't do that, the other 200 or whatever, that crash and burn and never recover. Like, let's go check in on the ar you know, ar on Argentina stock market right now. Like, I don't think it's coming back anytime

0 (1h 25m 45s):
Soon. Yeah. My father is Argentine and immigrated to the US in the 1960s. So he would definitely agree with that. Yeah. Like,

2 (1h 25m 51s):
That doesn't come, it doesn't come, you know, like when, when, when their stocks crash, they don't come back. So, and then, you know, like Sanco, like the Soviet Union never recovered from Lysenkoism, it never came back. It led to the end of the country. You know, literally the things that took down the Soviet, the Soviet Union were oil and, and wheat and the, the wheat thing, you can trace the crisis back to Lysenkoism. And so, yeah, no, look, I, the pendulum swings back is true. Only in the cases where the pendulum swings back. Everybody just conveniently forgets all the other circumstances where that doesn't happen. One of the things people, you see this in business also, people have a really hard time confronting really bad news. Hmm. I don't know if you've noticed that. And like every doctor who's listening right now is like, yeah, no shit. But like, like there are situa, you see it in business.

2 (1h 26m 33s):
There are situations that, it's a Star Trek, remember a Star Trek? The Khi Maru simulator. Right? So the big lesson to become a Star Trek captain is you had to go through the simulation called the Kopi Ashi Maru. And the point was there's no way to, it's a no win scenario, right? And then the, and then it turned out like Captain Kirk was the only person to ever win the scenario. And the way that he did it was he, he went in ahead of time and hacked the simulator. Right? It was the only way to actually get through. And then there was a debate whether to fire him or make him a captain. So they made him a captain. And like, you know, the problem is in real life, like we, we, you, you do get the Kobe Ashi Maru on a regular basis. Like there are actual no win situations that you can't work your way out of. And as a leader, you can't ever cop to that, right? 'cause you have to like, carry things forward and you have to look for every, every possible choice you can. But like every once in a while you do run a, into a situation where it's really not recoverable.

2 (1h 27m 16s):
And at least I found people just like cannot cope with that. And so, and so what happens is they basically then they, they basically just like exclude it from their memory that it ever happened.

0 (1h 27m 25s):
I'm glad you brought up simulators 'cause I wanna make sure that we talk about the new and emerging landscape of ai Artificial Intelligence. And I could try and smooth our conversation of a moment to go with the this one by creating some clever segue. But I'm not going to, except I'm going to ask, is there a possibility that AI is going to remedy some of what we're talking about? Let's make sure that we earmark that for discussion a little bit later. Sure. But first off, because some of the listeners of this podcast might not be as familiar with AI as perhaps they should be. We've all heard about Artificial, Intelligence, people hear about machine learning, et cetera.

0 (1h 28m 7s):
But it'd be great if you could define for us what AI is. People almost immediately hear AI and think, okay, robot's taking over. I'm gonna wake up and I'm gonna be, you know, strapped to the bed and my organs are gonna be pulled outta me. That robots are gonna be in my bank account. They're gonna kill all my children. And, and dystopia for most clearly. That's not the way it's gonna go. If you believe that machines can augment human intelligence and human intelligence is a good thing. So tell us what AI is and where you think it can take us both good and bad.

2 (1h 28m 49s):
Yeah. So, so, so there was a big debate when the computer was first invented, which was in the 1930s, 1940s. This hear people like Alan Turing and John von Neuman and these people. And the big debate at the time was, 'cause they knew they wanted to build Computers. They, they had the basic idea. And, you know, there had been like calculating machines before that. And there had been like, there had been these looms that you basically programmed to punch cards. And so there, there was like a, there was a pre-history to Computers that had to do with building sort of increasingly complex calculating machines. So they, they were kind of on a track, but they knew they were gonna be able to build, they call it general purpose computer that could basically, you could program in the way that you program Computers today. But they had a big debate early on, which is, should the fundamental architecture of the computer be based on either a, like calculating machines like cash registers and looms and right, and, and other things like that.

2 (1h 29m 34s):
Or should it be based on a model of the human brain? And they actually had this idea of Computers model in the human brain back then. And this was this concept of so-called Neural networks And. it, it's actually fairly astonishing from a, from a research standpoint. The original paper on Neural networks actually was published in 1943. Right? So, so, so they didn't have our level of Neuroscience, but like they actually knew about the neuron and they actually had a theory of like neurons interconnecting in synapsis and information processing in the brain. Even, even, even back then. And a lot of people at the time basically said, you know what, we should basically have the computer from the start be model after the human brain. 'cause like if we could, if the computer could do everything that the human brain can do, like that would be the best possible general purpose computer. And then you could have it do jobs and you could have it, you know, create art and you could have it do all kinds of things like humans can do.

2 (1h 30m 17s):
It. Turns out that didn't happen in our world. What happened instead was the industry went in the other direction. It went basically in the model of the calculating machine or the cash register. And I, I think practically speaking, that kind of had to be the case. 'cause that was actually the technology that was, was practical at the time. But, but that's the path. And so what, what we all have experiences with up to and including the iPhone in our pocket is Computers built on that basically calculating machine model, not the human brain model. And so what that means is Computers is we have come to understand them that, you know, they're basically like, you know, mathematical savants, you know, at best, right? So they're like, they're really good at like, you know, doing lots of mathematical calculations. They're really good at executing these extremely detailed computer programs.

2 (1h 30m 57s):
They're hyper literal. One of the things you learn early when you're a programmer is as a pro, as the human programmer, you have to get every single instruction you give the computer. Correct. 'cause it will do exactly what you tell it to do. And, and, and bugs and computer programs are always a mistake on the part of the programmer. Interesting. You never blame the computer, you always blame the, the, the programmer. 'cause that, that, that's the nature of the thing that you're dealing with. One

0 (1h 31m 18s):
Down score off and the whole thing.

2 (1h 31m 20s):
Yeah. Yeah. And it's the pro, it's the programmer's fault. And if you talk to any programmer, they'll agree with this. They'll be like, yeah, if there's a problem, it's, it's my fault. I I I did it. I can't blame the computer. The computer has no judgment. It has no ability to interpret, synthesize, you know, under develop an independent understanding of anything. It's just, it's literally just doing what I tell it to do step by step. So, so for 80 years we've had this just, you know, this very kind of hyper, literal, you know, kind of, kind of model Computers, these are called, technically these are what are called von Neumann machines based after the mathematician, John Van Neumann. They run in that way. And like they've been very successful and very important and our world has been shaped by them. But there was always this other idea out there, which is, okay, how about a completely different approach, which is based, based much more on how the human brain operates, or, or at least our, our our kind of best understanding of how the human brain operates, right?

2 (1h 32m 3s):
'cause those aren't the same thing. It basically says, okay, what, what if you could have a computer instead of being hyper literal, what if you could have it actually be conceptual, right? And creative and able to synthesize information, right? And able to draw judgments and able to, you know, behave in, you know, in ways that are not deterministic, but are rather, you know, creative. Right? And, and so, and, and, and, and the, and the applications for this, of course are endless. And so for example, the self-driving car, the only way that you can make a, you, you cannot program a computer with rules to make it a self-driving car. You have to do what Tesla and Waymo and these other companies have done. Now you have to use ai, right? You have to use this other architecture. And you have to basically teach them how to recognize objects in images at high speeds the same way, basically the same way the human brain does.

2 (1h 32m 47s):
And so those are, those are so-called Neural networks running inside. So

0 (1h 32m 50s):
Essentially let the machine operate based on priors. You know, we, we almost clipped a, a boulder going up this particular drive. And so therefore this shape that previously the machine didn't recognize as a boulder and now introduces to its catalog of boulders. Is that, is that sort Yeah.

2 (1h 33m 6s):
Good. A good, good example. Or let's even it make it, even even starker for self-driving car. There's something in the road. Is it a small child or a, or a plastic shopping bag being blown by the wind? Very important difference. If it's a shopping bag, you, you definitely wanna go straight through it because if you deviate off course, you might, you know, hit, you know you're gonna make a fast, you know, it's the same, it's the same challenge we have when we're driving. Like, you don't, you don't wanna swerve to avoid a shopping bag 'cause you might hit something that you didn't see on the side, but it's a small child for sure you wanna swerve, right? And so, but it's very, but like in, in that moment, and you know, small children come in different like shapes and descriptions and are wearing different kinds of clothes

0 (1h 33m 38s):
And they might tumble onto the road the same way a bag would tumble. Yeah.

2 (1h 33m 41s):
They might look like they're tumbling. And by the way, they might not be a, you know, they might be wearing a Halloween mask, right? It's the face, they might not have a recognizable human face, right? And it might, or it might be a kid with, you know, one leg, right? You definitely wanna not hit those, right? Like, so there, there, so you, you can't, this is what basically we figured out is you can't apply the rules-based approach of a vanneman machine to basically real life and expect the computer to be in any way understanding or resilient to change to, to basically things happening in real life. And this is why there's always been such a stark divide between what the machine can do, And What, the And What the human can do. And so ba basically what's happened is in the last decade, that second type of computer, the Neural Network based computer has started to actually work. It started to work actually first, interestingly in vision recognizing objects and images, which is why the self-driving car is starting to work.

2 (1h 34m 24s):
And face

0 (1h 34m 24s):
Recognition,

2 (1h 34m 25s):
Face recognition.

0 (1h 34m 26s):
I mean, when I was started off in visual Neuroscience, which is really my original home in Neuroscience, the idea that a computer or a camera could do face recognition better than a human was like a very low probability event based on the technology we had at the time, right? Based on the understanding of the face recognition cells in the fusiform gyrus. Now you would be smartest to put all your money on the machine. Yeah. You know, you wanna find faces in airports, even with masks on And, it, you know, at profile versus straight on machines can do it far better than most all people. I mean, they're the super recognizers, but even they can't match the best machines right now, 10 years ago what I just said was the exact reverse, right?

2 (1h 35m 9s):
That's right. Yeah. Right. So faces handwriting, right? And then voice, right? Being able to understand voice. Like if, if you use, just as a user, if you use Google Docs, that it has a built-in voice transcription. It's sort of the best industry leading kind of voice transcription. If you use voice transcription in Google Docs, it's breathtakingly good. You, you just speak into it And it just like types what you're, what you're saying. Well

0 (1h 35m 28s):
That's good. 'cause in my phone, every once in a while I'll say, I need to go pick up a few things and it'll say I need to pick up a few fongs. And so, well Apple needs to get on board. Whatever the voice recognition is that Google's using, maybe

2 (1h 35m 40s):
It knows you better than you think. So

0 (1h 35m 43s):
That was not the topic I was avoiding discussing, you know?

2 (1h 35m 46s):
No. So that's on the list, right? That's on your, your list. So, so look, there, there's actually this, there's your, there's a reason actually why Google's so good. And Apple is not right now at that kind of thing. And, it actually goes to actually the, it's actually an ideological thing of all of all things. Apple does not permit pooling of data for any purpose, including training ai. Whereas Google does. And, and Apple's just like stake their brand on privacy. And among that is sort of a pledge that they don't like pool your data. And so all of Apple's AI is like AI that has to happen like locally on your phone. Whereas Google's AI can happen in the cloud, right? It can happen across pool data. Now, by the way, some people think that that's bad 'cause they think pooling data's bad. But, but that, that's an example of the shift that's happening in the industry right now, which is you have this separation between the people who are embracing the new way of training AI and the people who basically, for whatever reason are not

0 (1h 36m 32s):
You, excuse me, you, you say that some people think it's bad because of privacy issues or they think it's bad because of the reduced functionality of that ai.

2 (1h 36m 40s):
Oh no. So you're, you're definitely gonna get, so, so, so there's three reasons AI have started to work. One of them is just simply larger data sets, larger amounts of data. So, so, so specifically the reason why objects and images are now, the reason machines are now better than humans at recognizing objects, images, or recognizing faces is because modern facial recognition, ais are trained across all photos on the internet of people billions and billions and billions of photos, right? Unlimited number of photos of people on the internet attempts to train facial recognition systems 10 or 20 years ago, they'd be trained on, you know, thousands or tens of thousands of photos.

0 (1h 37m 13s):
So the Input data is simply much more vast, much

2 (1h 37m 16s):
Larger. And this is the reason to get to the conclusion on this. The, this is the reason why ChatGPT works so well is ChatGPT, one of the reasons ChatGPT works so well is it's trained on the entire internet of text. And the entire internet of text was not something that was available for you to train an AI on until it came to actually exist itself, which is new in the last, you know, basically decade.

0 (1h 37m 33s):
So in the case of face recognition, I could see how having a much larger Input, Data Set would be beneficial if the goal is to recognize Mark Andreessen's face because you are looking for signal to noise against everything else, right? Right. But in the case of chat, G p t, when you're pooling all text on the internet and you ask chat g p t to say, construct a paragraph about Mark Andreessen's prediction of the future of human beings over the next 10 years, and the likely to be most successful industries give ChatGPT that if it's pooling across all text, how does it know what is authentically Mark Andreessen's text? Because in the case of face recognition, you have a, you've got a standard to work from a a verified image versus everything else in the case of text, you have to make sure that what you're starting with is verified text from your mouth.

0 (1h 38m 28s):
So, which makes sense if it's coming from video. But then if that video is deep faked, all of a sudden what's true, your, your your valid Marc Andreessen is of question. And then everything chat G p t is producing that is then of question, right?

2 (1h 38m 46s):
So I would say there's a before and after thing here. There's like a, before there's like a, before ChatGPT and after G P T question, right? Because the existence of G P T itself change changes, changes the, the answer. So before ChatGPT, so the reason the version you're using today is trained on data up till September, 2021, is they're, they're cut off of their training, their training set up till September, 2021, almost all texts on the internet was written by a human being. And then most of that was written by people under their own names. Some of it wasn't, but a lot. But a lot of it was, and and why, you know, this for me is 'cause it was published in a magazine under my name or it's a podcast transcript and it's, it's under my name. And, and generally speaking, if you just did a search on like what are things Marc Andreessen has written and said, 90 plus percent of that would be, would be correct. And there look, somebody might have written a fake, you know, parody article or something like that, but like, not that many people were spending that much time writing like fake articles about like, things that I said, right?

0 (1h 39m 32s):
So many people can pretend to be you. Exactly

2 (1h 39m 34s):
Right. And so generally speaking, you could, you can kind of get your arms around the idea that there's a corpus of material associated with me. Or by the way, same thing with you. There's a corpus of of YouTube transcripts and other your, your academic papers and talks you've given and you can kind of get your hands around that. And that's, that's how these systems are trained. They, they, they take all that data collectively, they put it in there and, and that's why this works as well as it does. And that's why if you ask chat g p t to speak or write like me or like you or like, you know, somebody else, it, it will actually generally do a really good job. 'cause it, it has that, it has all of our prior text in, its in its training data. That's it from here on out, this gets harder. And of course the reason this gets harder is because Now, we have AI that can create text and we have AI that can create text at industrial scale, right?

0 (1h 40m 12s):
Is it watermarked as AI generated text? No, no, no, no. Not at all. How hard would it be to do that?

2 (1h 40m 16s):
Yeah, I think it's impossible. I I think it's impossible. There are people who are trying to do that. This is a hot topic in the classroom. I was just talking to a friend who's got like a 14 year old kid in a, in a, in a class. And there's like these recurring scandals. Like every kid in the class is using ChatGPT to like write their essays or to help them write their essays. And then the, the, the teacher is using one of, there's a tool that you can use that it purports to be able to tell you, you know, whether something was written by ChatGPT, but it's like only write like 60% of the time. And so there was this case where this student wrote an essay where their parent sat and watched them write the essay and then they submitted it and this tool got the conclusion incorrect. And then the student feels outraged 'cause he got unfairly cheated. But the teacher is like, well, you're all using the tool. Then it turns out there's another tool that basically you feed in text And.

2 (1h 40m 58s):
it actually is sort of, it's, it's a, it's called, they call it a summarizer. But what it really is, is it's a cheating mechanism to, to basically just shuffle the words around enough so that it sheds whatever characteristics were associated with ai. So there's like a, an arms race going on in educational settings right now around this exact question. I, I don't, I don't think it's possible to do. There are people working on the watermarking. I don't think it's possible to do the watermarking. And I think it's just kind of obvious why it's not possible to do that, which is you can just read the output for yourself. It's, it's really good. How are you actually gonna tell the difference between that and something that a real person wrote? And then by the way, you can also ask Jet g p T to write in different styles, right? So you can tell it like, you know, write in the style of a 15 year old, right? You can tell it to write in the style of, you know, a non-native English speaker, right?

2 (1h 41m 40s):
Or if you're a non-native English speaker, you can tell it to write in the style of an English speaker, native English speaker, right? And so it, it, the tool itself will help you evade. So I I I, I don't think that, I think there's a lot of people who are gonna want to distinguish right? Quote real versus fake. I, I think those days are, I think those days are over

0 (1h 41m 56s):
Genie's outta the bottle.

2 (1h 41m 57s):
I genie's completely outta the bottle. And, and by the, and by the way, I actually think this is good. This doesn't map to, this doesn't map to my worldview of how we use this technology anyway, which, which we can come back to. So there's that. So, so, so there's that. So, and, and then there's the problem, therefore of like the so-called deepfake problem. So then there's the problem of like deliberate, basically manipulation. And that's like, you know, one of your one, one of your many enemies, one of your increasingly long list of enemies, thank you. Like mine, who basically is like, wow, I know how I'm gonna get him, right? I'm gonna, I'm gonna, I'm gonna create, so I'm gonna, I'm gonna use it to create something that looks like a Hebrew man transcript and I'm gonna have him say all these bad things and I'm, or a video or a video or a video.

0 (1h 42m 33s):
I mean, I mean, Joe Rogan and I were deep faked in a video. I don't want to flag people to it. I won't. So I won't talk about what it was about, but where it, for all the world look like a conversation that we were having and we never had that specific conversation.

2 (1h 42m 48s):
Yeah, that's right. So that's gonna happen for sure. And so there, there's gonna, so what there's gonna need to be is there's need to be basically Registries where basically you, you, you in, like, in your case, you will, you will submit your legitimate content into a registry under your unique cryptographic key, right? And then basically there will be a way to check against that registry to see whether that was the real thing. And I think this needs to be done for, for sure, for public figures. It needs to be done for politicians. It needs to be done for, you know, music.

0 (1h 43m 12s):
What about taking what's already out there and being able to authenticate it or not? In the same way that many times per week I get asked, is this your account about some, a direct message that somebody got on Instagram? And I always tell them, look, I only have the one account, this one verified account. Although now with the advent of pay-to-play verification makes it a little less potent as a, you know, security blanket for knowing if it's not this account, you know, then it's not me. But in any case, these accounts pop up all the time pretending to be me and I'm, I'm, you know, relatively low on the, on the scale, not, not, not low, but relatively low on the scale to say like a, you know, like a Beyonce or something like that who has, you know, hundreds of millions of followers.

0 (1h 43m 54s):
So is there a system in mind where people could go in and, and verify text, you know, click yes or no, this is me, this is not me. And even there, there's the opportunity for people to fudge to eliminate things about themselves that they don't want to out there by saying, no, that's not me. It wasn't, I didn't actually say that or create that.

2 (1h 44m 12s):
Yeah, no, that's right. And so, yeah, so technologically it's actually pretty straightforward. So the, the way to implement this technologically is with public key. It's called public Key Cryptography, which is the basis for how, you know, Cryptography information is secured in the world today. And, and so basically what you would do the implementation form of this would be, you would like, you would pick whatever is your most trusted channel. And let's say, let's say it's your YouTube channel as an example where just everybody just knows that it's you on your YouTube channel 'cause you've been doing it for 10 years or whatever, And, it's just obvious. And you would just publish, like in the About Me page on YouTube, you would just publish your, your, your public cryptographic key that's unique to you, right? And then anytime anybody wants to check to see whether any piece of content is actually you, they go to a registry in the cloud somewhere and they basically submit, they basically say, okay, is this him? And then they can basically you to see whether somebody with your public key, you had actually certified that this was something that, that, that you made.

2 (1h 44m 58s):
Now who runs that registry is an interesting question. If that registry is run by the government, we will call that the m ministry of truth. I think that's probably a bad idea. If that registry is run by a company, we would, you know, call that basically the equivalent of like a credit bureau or something like that. Maybe that's how it happens. The problem with that is that company now becomes hacking target number one Right? Of every bad person on earth, right? 'cause you can, you know, if anybody breaks into that company, you know, they can, they can, they can fake all kinds of things. Yeah.

0 (1h 45m 23s):
They own the truth, right?

2 (1h 45m 24s):
They own the truth, right? And, and by the way, insider threat also, their employees can monkey with it, right? So you have to really trust that company. The third way to do it is with a blockchain, right? And so this with a, the crypto blockchain technology, you could have a distributed system, basically distributed database in the cloud that is run through a blockchain and then it, it, it, it implements this Cryptography and this certification process. What,

0 (1h 45m 43s):
What about Quantum Internet, yeah. Is that another way to encrypt these things? I know most of our listeners are probably not familiar with Quantum Internet, but put simply, it's a way to secure communications on the internet. Let's just leave it at that. It's sophisticated and we'll probably do a whole episode about this at some point, but maybe you have a, a succinct way of describing Quantum Internet, but that would be better. And if so, please, please offer it up. But is Quantum Internet going to be one way to secure these kinds of data and, and resources? It it

2 (1h 46m 10s):
Maybe in in in the, in the future years? In the future? We don't yet have working Quantum Computers in practice. So it's not, it's not currently something you could do, but maybe, maybe in a decade or two,

0 (1h 46m 18s):
Tell me, I'm gonna take a stab at defining Quantum Internet one sentence. It's a way in which if anyone were to try and peer in on a conversation on the internet, it essentially would be futile because of the way that Quantum Internet changes the way that the communication is happening so fast and so many times in any one conversation, it's essentially changing the translation or the language so fast that there's just no way to keep up with it. Is that more or less accurate? Yeah.

2 (1h 46m 43s):
Con conceivably okay. Yeah. Not, yeah, not yet. But yeah, someday.

0 (1h 46m 46s):
So going back to ai, you know, most people who hear about AI are afraid of ai. Well, well, I I think most people who aren't informed, this

2 (1h 46m 55s):
Goes back to our Elites versus masses thing.

0 (1h 46m 57s):
Oh, interesting. Well, I heard you say that, you know, and this is from a, a really wonderful tweet thread that we will link in the show note captions that, that you put out not long ago, and that I've read now several times and that everyone really should take the time to read it probably takes about 20 minutes to read it carefully and to think about each piece. And it's, I highly recommend it. But you said, and I'm quoting here, let's address the fifth, the one thing I actually agree with, which is AI will make it easier for bad people to do bad things. Yep. Yeah.

2 (1h 47m 33s):
Yeah. Well, so, so yes. So well, so, so, so, so, yeah. So, so first of all, there is a general freakout happening around ai. I think it's primarily, it's one of these, again, it's an Elite driven freakout. I don't think the man on the street knows, knows, cares, or feels one way or the other, think it's just not a relevant concept. And it's probably just sounds like science fiction. So there, there's, there's, I think there's, there's an Elite driven freak out that's happening right now. I think that Elite driven freakout has many aspects to it that I think are incorrect, which is not surprising. I would think that given that I think the Elites are incorrect about a lot of things, but I think they're very wrong about a number of things that're saying about ai. But that said, look, it's a, it's a, this is a very powerful New Technology, right? This is like a new general purpose like thinking technology, right? So like, what if machines could think, right? And, What, if you could use machines that think And What, if you could have them think for you, there's obviously a lot of good that could come from that.

2 (1h 48m 19s):
But also people, you know, look criminals could use them to plan better, you know, crimes, you know, terrorists could use 'em to plan better terror attacks and so forth. And so these are going to be tools that bad people can use to do bad things, for sure.

0 (1h 48m 31s):
I can think of some ways that AI could be leveraged to do fantastic things. Like in the realm of Medicine. An AI pathologist perhaps can scan 10,000 slides of histology and find the one micro tumor cellular aberration that would turn into a full-blown tumor. Whereas the even mildly fatigued or well-rested human pathologist as great as they come, might miss that, right? And perhaps the best solution is for both of them to do it, and then for the human to verify what the AI has found and vice versa. Right? That's right. Right. Yeah. And that's just one example. I mean, I can come up with thousands of examples where this would be wonderful.

0 (1h 49m 15s):
So I'll, I'll

2 (1h 49m 16s):
Give you another one by the way, of Medicine. So you're talking about some, an analytic result, which is good and important. The other is like, the machines are gonna be much better bedside manner. Hmm. They're gonna be much better at dealing with the patient. And we already know there's already been a study, there's already been a study on this. So there was already a study done on this where there was a, a, a study team that scraped thousands of medical questions off of an internet forum. And then they had real doctors answer the questions, and then they had basically G P T four answer the questions, and then they had another panel of doctors score the responses, right? So th there, there were no patients experimented on here. This was a test contained within the, within the medical world. But, and then they, the, the, the, the panel of the judges, the panel of doctors, who are the judges, scored the answers on both factual accuracy and on bedside manner on empathy.

2 (1h 49m 59s):
And the G P T four was, was, was equal or better on most of the factual questions analytically already. And it's not even a specifically trained me medical ai, but it was overwhelmingly better on empathy. Amazing. Right? And so, and, and, you know, yeah, I don't think, yeah, I don't, do you, do you treat patient, do you treat patients directly in your work? No. Don't. You don't. Yeah. So I don't, we

0 (1h 50m 20s):
Do, we run clinical trials, right? But I don't, I don't do any direct clinical. So I,

2 (1h 50m 25s):
I, you know, I've, I, I've no direct experience with this, but from the surgeons, I froms, like if you talk to surgeons or you talk to people who train surgeons, what they'll tell you is like, surgeons need to have an emotional remove from their patients in order to do a good job with the surgery. The side effect of that. And by the way, look, it's a hell of a job to have to go in and tell somebody that they're gonna die, right? Or that they have, they're never gonna recover, they're never gonna walk again or whatever it is. And so there's a, there's sort of something inherent in that job where they need to keep an emotional reserve from the patient, right? To be able to do the job. And it's expected of them as professionals. The machine has no such limitation. Like the machine can be as sympathetic as you want it to be for as long as you want it to be. It could be infinitely sympathetic. It's happy to talk to you at four in the morning. It's happy to sympathize with you. It's, and, and by the way, it's not just sympathizing you in, in the way that, oh, it's just lying.

2 (1h 51m 8s):
You know? It's just making up words to lie to you to make you feel good. It can also sympathize with you in terms of helping you through all the things that you can actually do to improve your situation. Right? And so, you know, boy, like if you'd be, you know, can you keep a patient actually on track with a physical therapy program? Can you keep a patient on track with a nutritional program? Can you keep a patient off of drugs or alcohol? Right? And if they have a machine medical companion that's with them all the time that they're talking to, all the time, that's infinitely patient, infinitely wise, right? Infinitely loving, right? And, and it's just gonna be there all the time. And it's gonna be encouraging and it's gonna be saying, you know, you did such a great job yesterday. I know you can do this again today. Cognitive behavioral therapy is an obvious fit here. These things are gonna be great at, at CCB t. And that's, that's already starting. But you can already use ChatGPT G P T as a, as a C B T therapist if you want.

2 (1h 51m 51s):
It's actually quite good at it. And so, so there's, there's a universe here that's, it goes to what you said, there's a universe here that's opening up, which is what, what I believe is it's it's partnership between man and machine, right? It's, it's a symbiotic relationship, not an adversarial relationship. And so the, the exactly, the doctor is going to pair with the AI to do all the things that you described, but the patient is also going to pair with the ai. And I, and I think it's, I think, I think this, this partnership that's gonna emerge is gonna lead among other things to actually much better health outcomes.

0 (1h 52m 18s):
I mean, I've relied for so much of my life on excellent mentors, right? From a very young age, and still now in order to make best decisions possible with the information I had. And rarely were they available at four in the morning sometimes, but not on a frequent basis. And they fatigue like anybody else. And they have their own stuff like anybody else, baggage events in their life, et cetera. What you're describing is a sort of AI coach or therapist of sorts. Yes. That hopefully would learn to identify our best self and encourage us to be our best self. And when I say best self, I don't mean that in any kind of pop psychology way.

0 (1h 52m 60s):
I mean, I could imagine AI very easily knowing, you know, how well I slept the night before And, What types of good or bad decisions I tend to make at two o'clock in the afternoon when I've only had five hours of sleep, or maybe just less rem sleep the night before. It might encourage me to take a little more time to think about something. Yeah. Might gimme a little tap on the wrist through a device that no one else would detect too. You know, refrain from something, you know,

2 (1h 53m 23s):
It's never gonna judge you. It's never gonna be resentful. It's never gonna be upset that you didn't listen to it. Right. It's never gonna go on vacation. It's gonna be there for you. Like, I, I think this is the, this is the way people are going to live. It's gonna start with kids, and then over time it's gonna be adults. And the way people are gonna live is they're gonna have a exactly, friend, therapist, companion, mentor, coach, teacher, right. Assistant, and that, that, that, that, or, or by the way, maybe multiple of those And, it may be that we're actually talking about six, like different personas interacting, which is a whole nother another possibility. But they're gonna have a committee.

0 (1h 53m 52s):
A committee,

2 (1h 53m 52s):
Yeah, yeah. Committee, committee, yeah. Yeah, exactly. Actually different personas and, and may by the way, when there are difficult decisions to be made in your life, maybe what you want to hear is the argument among the different personas. And so you're just gonna get, you're just gonna grow up. You're just gonna have this in your life, and you're gonna always be able to talk to it and always be able to learn from it and always be able to help it make, you know, and like, it's just, it's going to, it's, it's, it's gonna be a symbiotic relationship. It's gonna, I think it's be a much better way to live. I think people are gonna get a lot out of it.

0 (1h 54m 17s):
What Modalities will it include? So I can imagine my phone has this engine in it, this AI companion, and I'm listening in headphones as I walk into work. And it's giving me some, not just encouragement, some warnings, some thoughts that, things that I might ask Marc, Andreessen today that I might not have thought of and so on. I could also imagine it having a more human form. I could imagine it being tactile, having some haptics, so tapping to remind me so that it's not gonna enter our conversation in a way that, that it interferes or distracts you. But I would be aware, oh, right. You know, things of that sort. I mean, how many different Modalities are we going to allow these AI coaches to approach us with?

0 (1h 54m 60s):
And is anyone actually thinking about the hardware piece right now? Because I'm hearing a lot about the software piece. What does the hardware piece look like?

2 (1h 55m 7s):
Yeah. So the entrepreneurs is where Silicon Valley is gonna kick in. So the entrepreneurial community is gonna try all of those, right? And by the way, the big companies and and and startups are gonna try all those. And so there's, obviously, there's big companies that are working, you know, the, the big companies that have talked about a variety of these, including, you know, heads up displays, ar, vr, you know, kinds of things. You know, there's lots of people doing voice. You know, the, the voice thing is, voice is a real possibility. It may just be an, an ear piece. There's a new startup that just unveiled a, a, a new thing where it's actually, they actually project, so you'll have like a pendant you wear on like a necklace, And, it actually like projects. Like, literally it will like project images, like on your hand or like on the table or on the wall in front of you. So like, maybe, maybe that's how it shows up. Yeah. There are people working on so-called haptic or touch-based kinds of things.

2 (1h 55m 49s):
There are people working on actually picking up nerve signals, like out of your arm, right. To be able to, to be able to do, there's people, there's, you know, there's some science for being able to do basically like sub vocalization. So maybe you could pick up, you could pick up that way. Bone conduction, you know, so yeah, the, the, these are all going to be tried. So, so, so that's one question is the physical form of it. And then, and then the other question is the software version of it, which is like, okay, what's the level of abstraction that you want to deal with these things in? Like, do you, like right now it's like a question answer or paradigm, right? In so-called chat bot, like ask a question, get an answer, ask a question, get an answer. Like, well, you want that to go for sure to more of a fluid conversation. You want it to build up more knowledge of who you are, and you don't wanna have to explain yourself a second time and so forth.

2 (1h 56m 31s):
And then you wanna be able to tell it things like, well, remind me this, that, or, you know, be sure to tell me when X. But then maybe over time, more and more, you want it actually deciding when it's gonna talk to you. Right. And when it thinks it has something to say, it says it, and otherwise it stays silent.

0 (1h 56m 45s):
And, and normally, at least in my head, unless I make a concerted effort to do otherwise, I don't think in complete sentences. So presumably this, these ai, these machines could learn my style of fragmented internal dialogue. And maybe I have an earpiece and I'm walking in and, and I start hearing something. But at some, you know, advice, et cetera, encouragement, discouragement. But at some point, those sounds that I hear in an earphone are very different than seeing something or hearing something in the room. We know this based on the Neuroscience of musical perception and language perception, hearing something in your head is very different.

0 (1h 57m 28s):
And I could imagine at some point that the AI will cross a precipice where if it has inline wiring to actually control Neural activity in specific brain areas, and I don't mean very precisely, even just stimulating a little more prefrontal cortical activity, for instance, through the earpiece. You know, a little ultrasound wave now can stimulate prefrontal cortex in a non-invasive way that's being used clinically and experimentally that the AI could decide that I need to be a little bit more context aware. Right? Right. This is something that is very beneficial for those listening that are trying to figure out how to navigate through life. It's like, you know, know the context you're in and know the catalog of behaviors and words that are appropriate for that situation and not others.

0 (1h 58m 7s):
And, you know, this would go along with agreeableness perhaps, but strategic agreeableness, right? Context is important. There's nothing diabolical about that Context is important, but I could imagine the AI recognizing, ah, we're entering a particular environment, I'm now actually gonna ramp up activity in prefrontal cortex a little bit in a certain way that allows you to be more situationally aware of yourself and others, which is great. Unless I can't necessarily short circuit that influence because, you know, at some point the AI is actually then controlling my brain activity and my decision making and my speech. Yeah. Right? I think that's what people fear, is that once we cross that precipice, that we are giving up control to the Artificial versions of our human intelligence.

0 (1h 58m 52s):
Yeah.

2 (1h 58m 53s):
And look, I think we have to decide, I, you know, we, we collectively, and we as individuals, I think have to decide exactly how to do that. And, and this is the big thing that I believe about ai that's just a much more, I would say, practical view of the world than a lot of the Panic that you hear. It's just like, these are machines. They, they, they're able to do things that increasingly are like the things that people can do in some circumstances. But these are machines. We build the machines, we decide how to use the machines when we want the machines turned on, they're turned on, and we want them turned off. They're turned off. And so, yeah. So I, I think that's absolutely the kind of thing that the individual person should always be in charge of.

0 (1h 59m 20s):
I mean, everyone was, and I have to imagine some people are still afraid of crispr of gene editing, right? But gene editing stands to revolutionize our treatment of all sorts of diseases, you know, inserting and deleting particular genes in adulthood, right? Not having to recombine in the womb a new, a new organism is an immensely powerful tool, right? And yet, the Chinese scientist who did CRISPR on humans, this has been done, actually did his postdoc at Stanford with Steve Quake, then went to China, did CRISPR on babies, mutated something. I believe it was the h i v one of the h i V receptors. I'm told it was with the intention of augmenting human memory. It had very little to do, in fact, with limiting susceptibility to H I v per se, right?

0 (2h 0m 3s):
To do with the way that that receptor is involved in human memory. The world demonized to that person. We actually don't know what happened to them, whether or not they have a laboratory now or they're sitting in jail. It's unclear. But in China and elsewhere, people are doing CRISPR on humans. We know this. It's not legal in the US and other countries, but it's happening. Do you think it's a mistake for us to fear these technologies so much that we back away from them and end up 10, 20 years behind other countries that could use it for both benevolent or malevolent reasons?

2 (2h 0m 38s):
Yeah, so there's always, and, you know, look, the, the details matter. So it, it, it's technology by technology. But I would say there's, there's two things. There's, you always have to think in, in these questions, I think in terms of counterfactuals and opportunity cost, right? And so, so CRISPR's an interesting one. Crispr, you manipulate the human genome. Nature manipulates the human genome. Yeah. Like in all kinds of ways. Yeah.

0 (2h 0m 59s):
When you pick a spouse and you have a child with that spouse, oh, you're doing genetic recombination. You

2 (2h 1m 3s):
Are really, yes. Yeah, you are quite possibly, you know, if you're Genghis Khan, you know, you're determining the future of, you know, humanity, right? By those like, like, yeah. Nature. I mean, look, mut mutations, like, so this is the old, this is the old question of like, basically state, you know, this is all state of nature, state of grace, like, you know, basically is nature good? And then therefore Artificial things are bad, you know, which is kind of shot. A lot of people have like ethical views like that. I, I, I'm always at the view that like, nature's a bitch and wants us dead, like nature's out to get us, man. Like, nature wants to kill us, right? Like, nature wants to like evolve all kinds of like horrible viruses. Nature wants to do plagues, nature wants to like, do, you know, you know, weather, you know, like nature, nature wants to do all kinds of stuff. I mean, look, the, the original nature of religion was the original religion, right?

2 (2h 1m 45s):
Like, that was the original thing. People worshiped. And the reason was because nature was the thing that was out to get you right before you had scientific and technological methods to be able to, to be able to deal with it. So, so, so the idea of not doing these things to me is just saying, oh, we're just gonna turn over the future of everything to nature. And I don't think that, that, there's no reason to believe that that leads in a particularly good direction or bad, you know, that, that, that, that that's not a value neutral decision. And then the related thing that comes from that is this always this question around what's called the Precautionary Principle, which shows up in all these conversations on things like crispr, which basically is this, it's this Principle that basically says the inventors of a New Technology should be required to prove that it will not have negative effects before they roll it out.

2 (2h 2m 25s):
This, of course, is a very new idea. This is actually a new idea. In the 1970s, it was actually invented by the German greens, the 1970s. Before that, people didn't think in those terms. People just invented things and rolled them out. And we got all of modern civilization by people inventing things and, and, and rolling them out. The German greens came up with the Precautionary Principle for one specific purpose. I'll bet you can guess what it is. It was to prevent

0 (2h 2m 49s):
Famine,

2 (2h 2m 50s):
Nuclear Power. Oh, it was to shut down attempts to do civilian Nuclear Power. And if you fast forward 50 years later, you're like, wow, that was a big mistake, right? So with, with what they said at the time was, you have to prove that nuclear reactors are not gonna melt down and cause all kinds of problems. And of course, as an engineer, can you prove that that will never happen? Like, you can't, like, you, you can't rule out like things that might happen in the future. And so that, that philosophy was used to stop Nuclear Power, by the way, not just in Europe, but also in the US and around much of the rest of the world. If you're somebody who's concerned about carbon emissions, of course, this is the worst thing that happened in the last 50 years in terms of energy. We, we actually have the silver bullet answer to unlimited energy with zero carbon emissions, Nuclear Power. We, we choose not to do it.

2 (2h 3m 30s):
Not only do we choose not to do it, we're actually shutting down the plants that we have now, right. In California. And we just shut down, you know, the, the, the big plant Germany just shut down their plants, Germany's in the middle of an energy war with Russia, right? That we are informed as existential for the future of Europe. But

0 (2h 3m 44s):
Unless the risk of nuclear power plant meltdown has increased. And I have to imagine it's gone the other way. Yeah. What is the rationale behind shutting down these plants and not expanding?

2 (2h 3m 55s):
Because nuclear is bad, right? Nuclear is icky. Nuclear is nuclear. Nuclear has been tagged.

0 (2h 3m 59s):
It just sounds bad. Yeah. Nuclear. Yeah.

2 (2h 4m 1s):
Yeah, yeah.

0 (2h 4m 2s):
Go nuclear. Yeah,

2 (2h 4m 3s):
Nuclear. Well, so what, what happened? We

0 (2h 4m 4s):
Didn't shut down postal offices and you hear it go postal. So what,

2 (2h 4m 6s):
What happened was, so, so nuclear technology arrived on planet Earth as a weapon, right? So it arrived in, in the form of the, the first thing they did was in the middle of World War ii. So the first thing they did was the atomic bomb. They dropped on Japan. And then, and then there were all the debates that followed around nuclear weapons and disarmament. And there's a whole conversation to be had, by the way, about that. 'cause there's different views you could have on that. And then it was in the, like fifties and sixties where they started to roll out civilian Nuclear Power. And then there were accident, there were melt, there were accidents. There was like, you know, three mile island melted down and you know, and then Chernobyl, you know, melted down in the Soviet Union. And then even recently, you know, Fukushima melted down. And so, you know, there have been meltdowns. And so I think it was a combination of, it's a weapon. It is sort of icky, a scientist sometimes of the ick factor, right?

2 (2h 4m 46s):
It's sort of, it's, you know, it's radioactive, it glows green, you know, and by the way, it becomes a, you know, becomes like a mythical fictional thing. And so you have all these movies of like, horrible, you know, super villains powered by like nuclear energy and all this, all this stuff.

0 (2h 4m 58s):
Well, the intro to the Simpsons, right? Is the Nuclear Power plant and the three eyed fish and the, you know, all the, all the negative implications of this Nuclear Power plant run by at least on the Simpsons idiots. And that is the, you know, the dystopia where people are unaware of just how bad it is and who

2 (2h 5m 18s):
Owns the Nuclear Power plant, right? This is like evil, you know, this evil capitalist, right? Like, so it's like connected to like, you know, capitalism, right? And then,

0 (2h 5m 25s):
So we're blaming Matt groaning for the demise of a particular,

2 (2h 5m 28s):
He certainly didn't help.

0 (2h 5m 30s):
No, he didn't. Right.

2 (2h 5m 31s):
But it's literally this amazing thing where if you're just like thinking, if you're just thinking like rationally, scientifically, you're like, okay, we wanna get rid of carbon. This is the obvious way to do it. So, so, okay. Fun fact, Richard Nixon did two, two things that really mattered on this. So one is he defined in 1971 something called Project Independence, which was to create a thousand new state of the art nuclear plants, civilian nuclear plants in the US by 1980. And to get the US completely off of oil and cut the entire US energy grid over to Nuclear Power electricity, cut over to electric cars, the whole thing like detached from carbon. You'll notice that didn't happen. Why did that not happen? 'cause he also created the E P A and the Nuclear Regulatory Commission, which then prevented that from happening, right? And then Nuclear Regulatory Commission did not authorize a new nuclear plant in the US for 40 years.

0 (2h 6m 13s):
Why would he hamstringing himself like that?

2 (2h 6m 16s):
You know, he got distracted by Yeah.

0 (2h 6m 18s):
Yeah. He had

2 (2h 6m 19s):
By wa by Watergate. Yeah. In Vietnam.

0 (2h 6m 22s):
Yeah. I think Ellsberg just died recently, right? He did. The guy who released the Pentagon papers. Yeah. So, you know, it's this thing, it's complicated, but it's Yeah,

2 (2h 6m 28s):
Yeah, yeah. Exactly. It's this thing. Yeah. He didn't, you know, like he left office shortly thereafter. He didn't have time to, you know, fully figure this out. I don't know whether he would've figured it out or not. You know? Look, Ford could have figured it out. Carter could have figured it out. Reagan could figured it out. Any of these guys could have figured it out. It's like the most obvious, knowing what we know today. It's the most obvious thing in the world. The Russia thing is the amazing thing. It's like Europe is literally funding Russia's invasion of Ukraine by paying them for oil, right? And they can't shut off the oil 'cause they won't cut over to nuclear, right? And, and, and then of course, what happens, okay, so then here's the other kicker of what happens, right? Which is they won't do nuclear, but they wanna do re renewables, right? Sustainable energy. And so what they do is they do solar and wind. Solar and wind are not reliable because it sometimes gets dark out and sometimes the wind doesn't blow.

2 (2h 7m 8s):
And so then what happens is they fire up the coal plants, right? And so the actual consequence of the Precautionary Principle for the purpose it was invented, is a massive spike in use of coal

0 (2h 7m 18s):
That's taking us back over a hundred years. Yes.

2 (2h 7m 20s):
Correct. That is the consequence of the Precautionary Principle. Like, that's the consequence of that mentality. And so it's a failure of a Principle on its own merits for the thing it was designed for. And then, you know, there, there is a whole movement of people who want to apply it to every new thing and, and, and, and And this is the hot topic on AI right now in, in, in Washington, which is like, oh my God, these people have to prove that this can never get used for bad things.

0 (2h 7m 40s):
Sorry, I'm hung up on this nuclear thing and and I wonder, can it just be renamed? I mean, seriously, I mean there is something about the naming of things. We know this in biology. Yeah, right? I mean, you know, Lamar and evolution and things like that are bad. These are bad words in biology. But we had a guest on this podcast, ODed Avi, who's over in Israel, who's shown, you know, inherited traits. But if you talk about his Lamar, and then it has all sorts of negative implications, but he's, the, his discoveries have important implications for everything from inherited trauma to treatment of disease. I mean, there's all sorts of positives that await us if we are able to reframe our thinking around something that yes, indeed could be used for evil, but that has enormous potential.

0 (2h 8m 24s):
And that is an agreement with nature, right? This fundamental truth that at least to my knowledge, no one is revising in any significant way anytime soon. So what if it were called something else?

2 (2h 8m 35s):
It

0 (2h 8m 35s):
Could be, instead of nuclear, it's, it's called, you know, sustainable. Right? I mean, it's amazing how marketing can sh can shift our perspective of robots, for instance, or, anyway, I'm sure you can come up with better examples than I can, but is there a good solid PR firm working from the nuclear side?

2 (2h 8m 55s):
Thunberg Greta Thunberg. So,

0 (2h 8m 59s):
Oh, Thunberg

2 (2h 8m 60s):
Thunberg. Like if she got, if she was in favor of it Yeah. Which by the way, she's not, she's dead set against it.

0 (2h 9m 4s):
She said that a

2 (2h 9m 5s):
Hundred percent. Yeah.

0 (2h 9m 6s):
Ba based on, based

2 (2h 9m 8s):
On, I

0 (2h 9m 9s):
Mean the Ian principles, the

2 (2h 9m 10s):
Prevailing, the, the prevailing ethic in Environmentalism 50 years is that nuclear is evil. Like they, they won't consider it. There are, by the way, certain environmentalists who disagree with this. And so Stewart Brand is the one that's been the most public, and he has impeccable credentials in this space. and he wrote this Poller

0 (2h 9m 22s):
Catalog, puller's

2 (2h 9m 22s):
Catalog guy. Yeah. And he's written a whole bunch of really interesting book since, and he, he wrote a recent book that goes through in detail. He's like, yes, obviously the correct environmental thing to do is Nuclear Power. And we should be, we should be implementing Project Independence. We should be building a thousand. We should specifically, we should, he didn't say this, but this is what I always say. We should hire Charles Koch. We should hire Koch Industries. Right. And they should build us a thousand Nuclear Power plants. Right. And then we should give 'em the presidential medal of freedom for saving the environment. Right.

0 (2h 9m 48s):
And that would put us independent of our reliance on oil.

2 (2h 9m 51s):
Yeah. Then we're done with oil. Like we're just think about what happens. We're done with oil, zero emissions. We're done with the Middle East. We're done, we're done. We, we're not drilling, we're not drilling on American land anymore. We're not drilling on foreign land. Like we have no military entanglements in places where we're drilling. We're not, you know, despoiling in Alaska, we're not nothing, no offshore rigs, no nothing. We're done. And you basically just, you build state-of-the-art plants engineered properly. You have them just completely contained when there's nuclear waste, you just entomb the waste. Right. And concrete. And so it, it, it just sits there, you know, forever. It's just very small, you know, footprint, you know, kind of thing. And you're just done. And so this is like the mo to me it's like scientifically, technologically, this is just like the most obvious thing in the world. It's a massive tell on the part of the people who claim to be pro-environment, that they're not in favor of this.

0 (2h 10m 34s):
And if I were to say tweet that I'm pro-nuclear power because it's the more sustainable form of power. If I, if I hypothetically did that today, yeah. What would happen to me in this, in the No,

2 (2h 10m 44s):
You'd be a crypto fascist, you know, interesting, dirty, evil capitalist, you know, monster. How dare you. Right? Yeah.

0 (2h 10m 51s):
I'm unlikely to run that experiment. I was just curious. That was what we call a ga donkin experiment. You're experiment in our

2 (2h 10m 56s):
Own. You're a terrible Andrew. You're a terrible human being. Wow. We, we were looking for evidence that you're a terrible human being and now Now, we know it. Right? And so this is a great example of, of the, I I gave Andrew a book on the way in here with this, my favorite new book. It's like title of is when Reason Ghost on Holiday. And this is a great example of it, is the, the people who are, who, who, who, the people who simultaneously say they're environmentalists and say they're anti-nuclear power, like the, the positions just simply don't reconcile. But that doesn't bother them, like at all. So be clear, I predict none, none of this will happen.

0 (2h 11m 24s):
Amazing. I need to learn more about Nuclear Power,

2 (2h 11m 27s):
Long coal,

0 (2h 11m 28s):
Long coal,

2 (2h 11m 29s):
Long coal, invest in coal.

0 (2h 11m 31s):
Because you think we're just gonna

2 (2h 11m 32s):
Revert it's the energy source of the future. Well, 'cause it can't be solar on wind 'cause they're not reliable. So you need something. And if it's not nuclear, it's gonna be either like oil, natural gas or coal.

0 (2h 11m 42s):
And you're unwilling to say bet on nuclear because you don't think that the sociopolitical elitist trends that are driving against nuclear are likely to dissipate anytime soon.

2 (2h 11m 53s):
No, not a chance. I, I can't imagine It would be great if they did, but the, the, that, that it's, they, they are the, the, the, the powers that be are very locked in on this as a position. And look, they've been saying this for 50 years, and so they'd have to reverse themselves off of a bad position they've had for 50 years. And people really don't like to do that.

0 (2h 12m 10s):
One thing that's good about this and other Podcasts is that young people listen and they eventually will take over. Yeah.

2 (2h 12m 15s):
And by the way, I will say also there, there are nuclear entrepreneurs. So there are actually a, there are a bunch plenty of young kids. There are a bunch of young entrepreneurs who are basically not taking no for an answer. And they're trying to develop, and particularly there's people trying to develop new very small form factor Nuclear Power plants with a variety of possible use cases. So, you know, look, maybe, you know, maybe they show up with a better mousetrap and people take a second look, but we'll see.

0 (2h 12m 39s):
Well just rename it. So my understanding is that you think we should go all in on AI with the constraints that we discover we need in order to reign in safety and things of that sort. Not unlike Social Media. Yeah. Not unlike the internet, not

2 (2h 12m 56s):
Unlike what we should have done with Nuclear Power. Yeah.

0 (2h 13m 1s):
And in terms of the near infinite number of ways that AI can be envisioned to harm us, how, how do you think we should cope with thats psychologically, you know, because I can imagine a lot of people listening to this conversation are thinking, okay, that all sounds great, but there are just too many what ifs that are terrible. Right. You know, what if the machines take over? What if, you know, all the silly example I gave earlier, but you know what, if one day I get log into my, you know, hard earned bank account and it's all gone. Right. You know, my, the AI version of myself like ran off with someone else and with all my money. Right, right, right. My AI coach abandoned me for somebody else after it learned all the stuff that I taught it.

0 (2h 13m 42s):
Yeah. It took off with somebody else. Yeah. Stranded, you know, And, it has my bank account numbers, like this kind of thing. Right. You

2 (2h 13m 49s):
Could really make this scenario horrible, right? If you

0 (2h 13m 51s):
Kept going. Yeah. Well we can throw in a benevolent example as well to counter it, but it's kind of fun to think about where the human mind goes, right?

2 (2h 14m 1s):
Yeah. So first I, I would say we gotta separate the real problems from the fake problems. And so there's a lot, there are a lot of science fiction scenarios I think are just not real. And the ones that you decided as an example, like it's not, that's not what's gonna happen. And I can explain why that's not what's gonna happen. So you, you should, there there's a set of, there's a set of fake ones. The fake ones are are ones, or the ones that just aren't, I think, technologically grounded that aren't rational. It's the, is gonna like wake up and decide to kill us all. It's gonna like, yeah, it's gonna develop the kind of agency where it's gonna steal our money, you know, money and our spouse and everything else, or kids. Like that's just, that's not how it works. And then there's also all these concerns, you know, destruction of society concerns. And this is, you know, misinformation, hate speech, Deep, Fakes, like all that stuff, which I, I don't think is a real, is a real, is actually a real problem. And then there's a, people have a bunch of economic concerns around, you know, what's gonna take all the jobs, right?

2 (2h 14m 45s):
All of those kinds of things. We could talk about that. I don't think those are, those are, those are, I don't think that's actually the thing that happens. But then there are two actual real concerns that I actually do very much agree with. And one of them is, is what you said, which is bad people doing bad things. And there's, there's a whole set of things to be done inside there. The big one is we should use AI to build defenses against all the bad things, right? And so for, for example, there's a concern, AI's gonna make it easier for, for bad people to build pathogens, right? Design pathogens in labs, which, you know, bad people, bad scientists can do today. But this is gonna make it easier, easier to do. Well obviously we should have the equivalent of an operation warp speed operating, you know, in perpetuity anyway, right? But then we should use AI to build much better bio defenses, right? And we should be using AI today to design, like, for example, full spectrum vaccines against every possible form of pathogen, right?

2 (2h 15m 30s):
And so, and so defensive mechanism hacking, you can use AI to build better defense tools, right? And so you should, you should have a whole new kind of security suite wrapped around you, wrapped around your data, wrapped around your money, where you're, you, you're having AI repel attacks, disinformation hate speech, Deep, Fakes, all that stuff. You should have an AI filter when you use the internet where it, you know, you shouldn't have to figure out whether it's really me or whether it's, it's a made up thing. You should have an AI assistant that's doing that for you.

0 (2h 15m 55s):
Oh yeah. I mean these, these little banners and cloaks that you see on Social Media, like this has been deemed misinformation, right? You know, if you're me, you always click, right, right, right, right. 'cause you're like, what's behind the scrim? Right? And then, or this is a, I don't always look at the, the, this image is is gruesome type thing. Sometimes I just pass on that. But if it's something that if seems debatable, of course you

2 (2h 16m 17s):
Look well and you should have, you should have an AI assistant with you when you're on the internet and you should be able to tell that AI assistant what you want, right? So yes, I want the full, I want the full experience to show me everything. I want it from a particular point of view. And I don't want to hear from these other people, people who I don't like. I, by the way, it's gonna be my eight year old is using this. I don't want anything that's gonna co cause a problem. And I want everything filtered. And, and, and AI-based filters like that, that you program and control are gonna work much better and, and be much more honest and straightforward and clear and so forth than what we have today. So, so anyway, so, so basically what I want people to do is think every time you think of like a risk of how it can be used, just think of like, okay, we can use it to build a countermeasure. And the great thing about the countermeasures is they can not only offset AI Risks, they can offset other Risks, right? Because we already live in a world where pathogens are a problem, right?

2 (2h 16m 59s):
We ought to have better vaccines anyway, right? We already live in a world where there's cyber hacking and cyber-terrorism. They already live in a world where there's bad content on the internet. And we have the ability now to build much better AI powered tools to deal with all those things.

0 (2h 17m 12s):
I also love the idea that AI physicians, you know, getting decent healthcare in this country is so difficult, even for people who have means or insurance. I mean, the number of phone calls and weights that you have to go through to get a referral to see a specialist. I mean, it's, it's absurd. Like, I mean, the process is, is absurd. I mean, it makes one partially, or frankly Ill just to go through the process of having to do all that. I don't know how anyone does it. And granted, I don't have the highest degree of patient, but I'm pretty patient, And, it drives me insane to even just get a remedial care. But, so I can think of a lot of benevolent uses of ai and I'm, I'm grateful that you're bringing this up and here and that you've tweeted about it in that thread.

0 (2h 17m 57s):
Again, we'll refer people to that and that you're thinking about this. I have to imagine that in your role as investor nowadays, that you're also thinking about AI quite often in terms of all these roles. And so does that mean that there are a lot of young people who are really bullish on AI and are going for it? Yeah. Okay. This is here to stay. Yeah. Okay. Yeah. Oh

2 (2h 18m 18s):
Yeah. Big time. Okay.

0 (2h 18m 19s):
Unlike crispr, which is sort of in this liminal place where biotech companies aren't sure if they should invest or not, right? In, in, in crispr, because it's unclear whether or not the governing bodies are going to allow gene editing, right? Just like it was unclear 15 years ago if they were gonna allow gene therapy. But Now, we know they do allow gene therapy and immunotherapy, right? Okay.

2 (2h 18m 37s):
So there is a fight. Now, having said that, there's a fight, there's a fight happening in Washington right now over exactly what should be legal or not legal. And there's quite a bit of risk, I think, attached to that fight right now. 'cause there are some people in there that are being telling a very effective story to try to get people to either outlaw AI or specifically limited to a small number of big companies, which I think has potentially disastrous. By the way, the EU also is like super negative. You know, the EU has turned super negative on basically all New Technology. So they're moving to try to outlaw ai, which if they succeed

0 (2h 19m 5s):
Outlaw

2 (2h 19m 6s):
Ai Yeah. Just like flat out don't want it.

0 (2h 19m 8s):
But that's like saying you're gonna outlaw the internet. I don't see how you can stop this train. And

2 (2h 19m 11s):
Frankly, they're not a big fan of the internet either. So like they're, I I think they regret the EU has a very, especially the EU bureaucrats, the people who run the EU in Brussels have a very negative view on a lot of modernity.

0 (2h 19m 24s):
But what I'm hearing here, you know, calls to mind things that, that I've heard people like David Goggins say, which is, you know, there's so many lazy, undisciplined people out there that nowadays it's easier and easier to become exceptional. I've heard him say something to that extent. It almost sounds like there's so many countries that are just backing off of particular technologies because it just sounds bad from the PR perspective that, you know, it's creating great kind of low hanging fruit opportunities for people to barge forward and countries to barge forward if they're willing to embrace this stuff.

2 (2h 19m 54s):
It is, but number one, you have to have a country that wants to do that. And, and that those exist. And there, there are countries like that. But, and then the other is, look, they, they need to be able to withstand the attack from stronger countries that don't want them to do it, right? So like e eu, like the EU has, you know, nominal control over like whatev whatever it is, 27 or whatever member countries. So like, even if you're like, whatever, the Germans get all fired up about whatever, like Brussels can still in a lot of cases, just like flat out basically control them and tell them not to do it. Right? And then the US, I mean, look, we have, you know, we have a lot of control over a lot of the world,

0 (2h 20m 24s):
But it sounds like we sit somewhere sort of in between, like right now people are developing AI technologies in US companies, right? So it is happening, yeah,

2 (2h 20m 34s):
Today, today it's happening. But like I said, there's a set of people who are very focused in Washington right now about trying to either ban it outright or trying to, as I said, limit it to a small number of big companies. And then, look, China's got a whole, the, the other part on this is China's got a whole different kind of take on this right? Than we do. And so they're of course going to allow it for sure, but they're gonna allow it in the ways that, that their, their system wants it to happen, right? Which is which much more for population control and to implement authoritarianism. And then of course they, they, they are going to spread their technology and their vision of how society should run across the world, right? So we're, we're, we're back in a cold war dynamic like we were with the Soviet Union, where there are two different systems that have fundamentally different views on, on issue, you know, concepts like freedom and, and individual choice and freedom of speech and so on.

2 (2h 21m 17s):
And, you know, we know where the Chinese stand. We're still figuring out where we stand, right? There are a lot of, so I'm having a lot of schizophrenic, I'm having specifically a lot of schizophrenic conversations with people in DC right now where if I talk to them and China doesn't come up, they just like hate tech. They hate American tech companies. They hate ai, they hate Social Media, they hate this, they hate that. They hate crypto, they hate everything and they just wanna like punish and like ban and like, they're just like very, very negative. But then if we have a conversation half hour later and we talk about China, then the conversation totally different. Now, we need a partnership between the US government and American tech companies to defeat China. It's, it's like the exact opposite discussion, right?

0 (2h 21m 51s):
Is that fear or competitiveness on

2 (2h 21m 53s):
China specifically

0 (2h 21m 54s):
On, on in terms of the, the US response in Washington when, you know, you bring up these technologies like, you know, I'll lump crisper in there. Things like crisper Nuclear, Power ai, it all sounds very cold. Yeah. Very dystopian to a lot of people. Yeah. And yet there are all these benevolent uses as we've been talking about. And then you say you raise the issue of China, and then it sounds like this big, you know, dark cloud emerging, and then all of a sudden, you know, it's, we need to galvanize and, and develop these technologies to counter their effort. So is it fear of them or is it competitiveness or both?

2 (2h 22m 28s):
Well, so without them in the picture, you just have this, and basically it's, it's, there's an old bedroom in was a saying as me against my brother, me and my brother against my cousin, me and my brother, my cousin against, you know, the world, right? Like, so, so with it's e it's actually, it's evolution in action. If is, I think would think about it, is that there's no external threat. Then the conflict turns inward. And then, and then at that point, there's a big fight between specifically tech and then I was just say, generally politics. And, and my interpretation of that fight is it's a fight for status. It's, it's fundamentally a fight for status and for power, which is like, if you're in politics, you like the, you like the status quo of how power and status work in our society. You don't want these new technologies to show up and change things. 'cause change is bad, right?

2 (2h 23m 8s):
Change, change threatens your position, it threatens your, you know, the respect that people have for you and your control over, over things. And so I think it's primarily a status fight, which, which we could talk about. But the, the China thing is just like a straight up geopolitical us versus them, you know, like I said, it's like a cold war scenario. And you know, look, 20 years ago, the prevailing view in Washington was we need to be friends with China, right? And we're gonna be trading partners with China. And yes, they're a totalitarian dictatorship, but like if we trade with them over time, they'll become more democratic. In the last five to 10 years, it's become more and more clear that that's just not true. And now there's a lot of people in both political parties in DC who very much regret that and want to change too much more of a sort of a cold war footing.

0 (2h 23m 47s):
Are you willing to comment on TikTok and technologies that emerge from China that are in widespread use within the us? Like how much you trust them or don't trust them? I can go on record myself by saying that early on when TikTok was released, we were told to Stanford faculty that we should not and could not have TikTok accounts nor WeChat accounts, right?

2 (2h 24m 9s):
So start with, there are a lot of really bright Chinese tech entrepreneurs and engineers who are trying to do good things. I'm totally no, absolutely positive about that. So I, I think the, the, the many of the people it mean mean very well, but the, the Chinese have a specific system and the system is very clear and unambiguous. And the system is, everything in China is owned by the party. It's not, it's not even owned by the state, it's owned by the party, it's owned by the Chinese Communist Party. So the Chinese Communist Party owns everything. They control everything. By the way, it's actually illegal to this day. It's illegal for a foreign investor to buy equity in a Chinese company. There's all these like basically legal mac machinations that people do to try to do something that's like economically equivalent to that. But it's actually still illegal to do that. The chi Chinese have no intention. The Chinese Communist Party has no intention of letting foreigners own any of China, like zero intention of that.

2 (2h 24m 52s):
And they, they, they regularly move to make sure that that doesn't happen. So, so, so, so they own everything. They control everything. So,

0 (2h 24m 58s):
Sorry to interrupt you, but people in China can invest in American companies. Oh yeah. They send you all the time.

2 (2h 25m 3s):
Well, they can subject to US government constraints. There, there is a, there is a US government system that attempts to mediate that called cfius. And there are more and more, there are more and more limitations being put on that. But if you can get through that approval process, then legally you can do that. Whereas the same is not true with respect to China. And so, so, so they just have a system. And so if you're, if you're the c e O of a Chinese company, it's not optional. If you're the c e o of by dancer or c e o of Tencent, it's not option. Your, your relationship with the Chinese Communist Party is not optional. It's required. And what's required is you are a unit of the party and you and your company do what the party says. And when the party says we get full access to all user data in America, you say yes.

2 (2h 25m 43s):
When the party says you change the algorithm to optimize to a certain social result, you say yes. Right? So, so it's just, it's whatever, it's whatever, it's whatever Xi Jinping and his party cadres decide, and that's what's gets implemented. If you're the c e o of a Chinese tech company, there is a political officer assigned to you who has an office down the hall. And at any given time, he can come down the hall, he can grab you out of your staff meeting or board meeting, and he can take you on the hall and he can make you sit for hours and study Marxism and Xi Jinping thought and quiz you on it and test you on it. Wow. And you'd better pass the test, right? Like, so, so it's like a straight political control thing. And then by the way, if you get crosswise with them, like, you know,

0 (2h 26m 22s):
So when we see tech founders getting called up to Congress for, you know, what looks like interrogation, but it's probably pretty light interrogation compared to what happens in other countries. Yeah.

2 (2h 26m 34s):
It's, it's a, it's a, it's state, it's state power. They, they just, they, they just have this view of top down state power, and they view it's that their system and they view that it's necessary for lots of historical and Moral reasons that they've defined and that's how they run. And then they've got a view that says how they wanna propagate that vision outside the country. And they have these programs like Belt and Road, right, that basically are intended to propagate kind of their vision worldwide. And so they are who they are. Like I will say that they don't lie about it, right? They're, they're very straightforward. They give speeches, they write books. You can buy Xi Jinping speeches. He goes through the whole thing. They have, they have their tech 2025 plan talked about, you know, it's like 10 years ago, their whole AI agenda. It's all in there.

0 (2h 27m 8s):
And is there goal that, you know, in 200 years, 300 years that China is the superpower controlling everything, but

2 (2h 27m 15s):
Yeah. Or 20 years, 30 years or two years? Three years. Yeah. Oh,

0 (2h 27m 17s):
They got a shorter horizon

2 (2h 27m 18s):
Still. I still, I mean they're, they're, yeah, they're, they're, I mean if, if you're, it's like, you know, and I don't know everybody's a little bit like this I guess, but yeah, if you're, they wanna win.

0 (2h 27m 26s):
Well the, the crispr in humans example that I gave earlier was interesting to me because first of all, I'm a neuroscientist and they edited any genes, but they chose to edit the genes involved in the attempt to create super memory babies, right? Which presumably would grow into super memory adults. And whether or not they succeeded in that isn't clear. Those babies are alive and presumably by now walking, talking, as far as I know, whether or not they have super memories isn't clear. But China is clearly unafraid to augment biology in that way. And I believe that that's inevitable.

0 (2h 28m 8s):
That's gonna happen elsewhere, probably first for the treatment of disease. But at some point, I'm assuming people are gonna augment biology to make smarter kids. I mean, people not always, but often will select mates based on the traits they would like their children to inherit. So this happens far more frequently than could be deemed bad because either that or people are bad because people do this all the time. Selecting mates that have physical and psychological and cognitive traits that you would like your offspring to have. CRISPR is a more targeted approach. Of course. You know, the reason I'm kinda giving this example and, and examples like it is that I feel like so much of the way that governments and the, and the public react to technologies is to just, you know, take that first glimpse, And, it just feels scary.

0 (2h 28m 53s):
Yeah. You think about the old apple ad of the, you know, 1984 ad, I mean, there was one very scary version of the personal computer and Computers and robots taking over and everyone like automatons. And then there was the Apple version where it's all about creativity, love, and peace. And, it had the pseudos psychedelic California thing going for it. Again, great marketing seems to convert people's thinking about technology such that what was once viewed as very scary and dangerous and dystopian is like an oasis of opportunity, right? So why are people so afraid of new technologies?

2 (2h 29m 30s):
So this is the thing I've tried to understand for a long time because the history is so clear and the, the history basically is every New Technology is greeted by what's called a Moral Panic. And so it's basically this like historical freakout of some kind that causes people to basically predict the end of the world. And you, you go back in time and actually this historical sort of effect, it, it happens even in things now where you just go back in, its ludicrous, right? And so you mentioned earlier the satanic Panic of the, of the eighties and you know, the concern around like heavy metal music, right? Before that there was like a freakout around comic books. You know, in the fifties there was a freakout around jazz music in the twenties and thirties, you know, it was devil music. You know, there was a freakout, the arrival of bicycles caused a Moral Panic in the like 1860s 1870s.

2 (2h 30m 10s):
Bicycles. Bicycles, yeah. So there was this thing at the time, so bicycles were the first, they were the first very easy to use personal transportation thing that basically let kids travel between towns, you know, quickly without any overhead. You know, you have to take care of hos, just jump on a bike and go. And so it, it was, there was a historical Panic specifically around at the time, young women who the first time were able to venture outside the confines of the town to maybe go have a boyfriend in another town. And so the magazines at the time ran all these stories on this phenomenon, medical phenomenon called bicycle face. And the idea of bicycle face was the exertion caused by pedaling a bicycle would cause your face, your, your face would grimace. And then if you were on the bicycle for too long, your face would lock into place, right?

2 (2h 30m 51s):
And then, sorry, I was just Right. And then you would be, you would be unattractive. And therefore, of course unable to then, you know, get married cars. There was a Moral Panic around car, red flag laws. There were all these laws that created the, the automobile, automobile freaked people out. So there were all these laws, if in the early days of the automobile in a lot of places you had to, you would take a ride at automobile and automobiles, they broke down all the time. So it would be you only rich people had automobiles. So it'd be you and your mechanic in in the car, right? For when it broke down. And then you had to hire another guy to walk 200 yards in front of the car with a red flag. and he had to wave the red flag. And so you could only drive as fast as he could walk. 'cause the red flag was to warn people that the, that the car was coming.

2 (2h 31m 33s):
And, and then, and then, and I think it was Pennsylvania, they had the most draconian version, which was, they were very worried about the cars scaring the horses. And so there was a law that said if you saw a horse coming, you need to stop the car. You, you had to disassemble the car and you had to hide the pieces of the car behind the nearest hay bale, wait for the horse to go by. Oh. And, and then you could put your car back together. So, so anyways, just example is a electric lighting. There was a Panic around like where this was gonna like completely ruin, you know, this is gonna completely ruin like the romance of the dark And. it was gonna cause, you know, you know, a whole new kind of like terrible civilization where everything is always brightly lit. So there's just like all these examples. And so it's like, okay, what on earth is happening that this is always what happens? And so I finally found this book that I think has a, has a good model for it.

2 (2h 32m 16s):
A book is called Men Machines in Modern Times. And it's written by this m i t professor like 60 years ago. So it, it predates the internet, but it uses a lot of historical examples. And What, he says basically is, he says there's, there's actually a three stage response. There's a three stage societal response to new technologies. It's very predictable. He said, stage one is basically just denial. Just ignore, like we just like don't pay any attention to this. Nobody takes it seriously. We just like, there's just a blackout on the whole topic. He says stage, that's stage one. Stage two is rational counter-argument. So stage two is where you line up all the different reasons why this can't possibly work. It can't possibly ever get, you know, cheap or you know, this, that, you know, not fast enough or whatever the thing is. And then he says, stage three he says, is when the name calling begins.

2 (2h 32m 58s):
So he says stage three is like when, right? Right. So when they fail to ignore it and they failed to argue society out of it, I love it. They move to the name calling, right? Yeah. And what's the name calling? The name calling is, this is evil. This is Moral Panic, this is evil, this is terrible, this is awful. This is gonna destroy everything. Like, don't you understand? Like, you know, all this, you know, it's just, it's just like this is, this is horrifying. And, and you, you know, the person working on it are being reckless and evil and you know, all all this stuff and you must be stopped. And, and he said the reason for that is because basically fundamentally what these things are is they're a war over status. It's a, it's a, it's a war over status and therefore a war over power. And, and then of course ul ultimately money, but, but status, human status is the thing. And so, and, and 'cause what he says is what, what is the societal impact of a New Technology, the societal impact of a New Technology is it reorder status in the society?

2 (2h 33m 43s):
So the people who are specialists in that technology become high status and the people who are specialists in the previous way of doing things become low status. And generally people don't adapt, right? Generally, if you're the kind of person who is high status because you're an, you're, you're an evolved adaptation to an existing technology, you're probably not the kind of person that's gonna enthusiastically try to replant yourself onto a New Technology. And so, and so, this is like every politician who's just like in a complete state of Panic about Social Media, like why are they so freaked out about Social Media is 'cause they all know that the whole nature of modern politics has changed the entire battery of techniques that you use to get elected before Social Media are now obsolete. Obviously the best new politicians of the future are gonna be a hundred percent creations of Social, Media

0 (2h 34m 22s):
And Podcasts and

2 (2h 34m 23s):
Podcasts.

0 (2h 34m 23s):
And we're seeing this now as we head towards the next presidential election, that Podcasts clearly are going to be featured very heavily in that next election because long form content is a whole different landscape. So

2 (2h 34m 35s):
This is exact, so this is the Rogan, this is so, so funny. So Rogan Rogan's had like, what, like, he's had like Bernie, he's had like tulsi, he's had like a whole series of

0 (2h 34m 41s):
This r k most recently. And that's created a lot of controversy. A

2 (2h 34m 44s):
Lot of controversy. But also my understanding, I it's like he's, I'm sure he is invited everybody. I'm sure he, I'm sure he'd love to have Biden on. I'm sure he'd love to have Trump on. I'm sure he'd, you'd

0 (2h 34m 50s):
Have to ask him. I mean, I think that, you know, every podcaster has their own ethos around who they invite on and why and how. Yeah. So I certainly can't speak for him. Okay, fair enough. But, but I have to imagine that any opportunity to have true long form discourse that would allow people to really understand people's positions on things, I have to imagine that he would be in favor of that sort thing.

2 (2h 35m 14s):
Yeah. Or somebody else would. Right. You know, some, some other top, top Podcasts undoubtedly would. Right? And so, so there's a, if my point, my exactly, I totally agree with you. But my point is, if, if you're a politician, if you're, if you're a, let's say a legacy politician, right? You have the option of embracing the New Technology, you can do it anytime you want. Right? You, but you, you don't, they're not, they won't, like they won't do it. And why won't they do it? Well, okay, it's, first of all, they wanna ignore it, right? They wanna pretend that things aren't changing, you know. Second is they want, like, they have rational counterarguments for like why the existing campaign system works the way that it does and this and that and the existing media networks and like, here's how you like do things and here's how you give speeches and here's the clothes you wear and the tie and the thing and the pocket. And like you've got your whole system. It's how you succeeded was coming up through that system. So you've got all your arguments as to why that won't work anymore.

2 (2h 35m 55s):
And then, and then we've now proceeded to the, the, the name calling phase, which is now is evil. Right now. It's evil for somebody to show up in po you know, on a, on, on a stream, God forbid, for three hours and actually say what they think, right? It's gonna destroy society, right? So it it it's exactly right. It's like, it's, it's a, it's a classic example o of this pattern. And anyway, so Mor Morrison says in the book, basically this is the forever pattern. Like this will never change. This will, this is one of those things where you can learn about it and still not the entire world could learn about this. And still nothing changes because at the end of the day, it's not, it has, it's not the tech that's the question. It's the, it's the reordering of status.

0 (2h 36m 31s):
I have a lot of thoughts about the podcast component. I'll just say this because I want to get back to the topic of innovation of technology. But on a long form podcast, there's no safe zone. You know, the person can get up and walk out. But if the person interviewing them, and certainly Joe is the best of the very best, if not the most skilled podcaster in the, the entire universe at continuing to press people on specific topics when they're, you know, trying to bob and weave and wriggle riggle out. He'll just keep, you know, either drilling or alter the question somewhat in a way that forces them to finally come up with an answer of some sort.

0 (2h 37m 12s):
And I think that probably puts certain people's cortisol levels through the roof such that they just would never go on there. Well,

2 (2h 37m 20s):
I think there's another deeper question also, or another, another question along with that, which is how many people actually have something to say

0 (2h 37m 27s):
You have real substance, right? Yeah.

2 (2h 37m 29s):
Like how many people can actually talk in a way that's actually interesting to anybody else for any length of time? Like how much substance is there really? And like a lot of historical politics was to be able to manufacture a facade where you honestly, as far as I, you can't tell like how, how deep the thoughts are. Like even if they have deep thoughts, like it's, it's kept, it's kept away from you, they would certainly never cop to it.

0 (2h 37m 48s):
It's going to be an interesting next, what is it about, you know, 20 months or so? Yeah. Panic leading into the next selection. Yeah.

2 (2h 37m 54s):
Panic, the Panic and the name calling have already started. So yeah, it's

0 (2h 37m 56s):
Gonna be, yeah, I was gonna say this list of three things, denial, you know, the counter argument and name calling. It seems like with AI it's already just jumped to numbers two and three. Yes, correct. Right? We're already at two and three and it's kind of leaning three. Yes. Yeah,

2 (2h 38m 10s):
That's correct. Because they're, well, so AI is unusual just because it had so tech new technologies that take off, they almost always have a prehistory. They almost always have a 30 or 40 year history where people tried and failed to get them to work before they took off. AI has an 80 year prehistory, so it has a very long one. And then it, it, it just, it all of a sudden started to work dramatically well, like seemingly overnight. And so it, it went from basically it, as far as most people were concerned, it went from, it doesn't work at all to, it works incredibly well in one step and that almost never happens. And so, and that's why I actually think that's exactly what's happening. I think it's actually speed running this progression just, just because if you use mid journey or you use G P T or any of these things for five minutes, you're just like, wow. Like obviously this thing is gonna be like the, obviously in my life, this is gonna be the best thing ever.

2 (2h 38m 52s):
Like this is amazing. There's all these ways that I can use it. And then, and then therefore immediately you're like, oh my God, this is gonna Transform everything. Therefore step three,

0 (2h 39m 1s):
Right. Straight to the name calling. Exactly. In the face of all this, there are Innovators out there, maybe they are aware, they are Innovators, maybe they are already starting companies or maybe they are just some young or older person who has these five traits in abundance or doesn't, but knows somebody who does and is partnering with them in some sort of idea. And you have an amazing track record at identifying these people. I think in part because you have those same traits yourself. I've heard you say the following, the world is a very malleable place. If you know what you want and you go for it with maximum energy and drive and passion, the world will often reconfigure itself around you much more quickly and easily than you would think.

0 (2h 39m 50s):
That's a remarkable quote because it says at least two things to me. One is that you have a very clear understanding of the inner workings of these great Innovators. We talked a little bit about that earlier, these five traits, et cetera. But that also, you have an intense understanding of the world landscape. And the way that we've been talking about it for the, the last hour or so is that it is a really intense and kind of oppressive landscape. You've got countries and organizations and the Elites and journalists that you know, that are trying to, not necessarily trying, but are suppressing the innovation process. I mean, that's sort of the, the picture that I'm getting.

0 (2h 40m 30s):
So you, it's like it, we're trying to innovate inside of a vice that's getting progressively tighter. And yet this quote argues that it is the, the person, the boy or girl, man or woman who says, well, you know what, that all might be true, but my view of the world is the way the world's gonna bend. Right? Or I'm gonna create a dent in that vice that allows me to exist the way that I want. Or you know what, I'm actually gonna uncurl the vice the other direction. And so I'm at once picking up a sort of pessimistic glass half empty view of the world as well as a glass half full view. And so tell me about that and, and tell if you would, could you tell us about that from the perspective of someone listening who is thinking, you know, I've got an idea and I know it's a really good one.

0 (2h 41m 22s):
'cause I just know I might not have the confidence of Extrinsic reward yet, but I just know right. There's a seed of something, right? What does it take to foster that and how do we foster real innovation in the landscape that we're talking about?

2 (2h 41m 36s):
Yeah. So part is I think you just, I think one of the ways to square it is I think you, you as the innovator need to be signed up to fight the fight, right? So, and, and again, this is where like the fictional portrayals of startups, I think take people off course or even scientists or whatever, 'cause they, the, when, when, when there's great success stories, they get kind of prettified after the fact and they may, they get made to be like cute and fun. And it's like, yeah, no, like if you talk to anybody who actually did any of these things, like no, there was, it was just like the, these things are always just like brutal exercises and just like sheer willpower and fighting, you know, fighting, fighting forces that are trying to get you. So, so, so part of it's you just, you have to be signed up for the fight. And this kind of goes to the conscientiousness thing. We're talking about it. We also, my partner Ben uses the term Courage a lot, right? Which is some combination of like, just stubbornness, but coupled with like a willingness to take pain and not stop and, you know, have people think very bad things of you for a long time until it turns out, you know, you hopefully prove yourself.

2 (2h 42m 27s):
Prove yourself, correct. And so you have to be willing to do that. Like it's a, it's a con. These are, it is a, it's a contact sport. Like it's, these aren't easy roads, right? It's a contact sport. So you have to be signed up for the fight. The advantage that you have is an innovator. If is the, is at the end of the day, the truth actually matters. And all the arguments in the world, classic Victor Hugo quote is, there's nothing more powerful in the world than an idea whose time has come, right? Like if, if it's real, right? And And it, so just pure substance, if the thing is real, if the idea is real, like if it's a legitimately good scientific discovery and you know about how the nature works, if it's a new invention, if it's a new work of art, and if it's real, you know, then you, you, you do at the end of the day, you have that on your side and all of the people who are fighting you and arguing with you and telling you no, they don't have that on their side, right?

2 (2h 43m 17s):
It's, it's not like they're, they're showing up with some other thing and they're like, my thing is better than your thing. Like, that's not the main problem, right? The main problem is like, I have a thing, I'm convinced everybody else is telling me it's stupid wrong, it should be illegal, whatever the thing is. But at the end of the day, I still have the thing, right? And so and so, so at the end of the day, like yeah, the, the, the truth really matters. The substance really matters if it's real, it's real. I'll give you an example. It's really hard historically to find an example of a New Technology that came into the world that was then pulled back and, you know, and we could, you know, nuclear is maybe a, maybe an example of that. But even still, there are still, you know, nuclear, there are still nuclear plants like running today. You know, that, that, that still exists. You know, I would say the same thing as scientific.

2 (2h 43m 57s):
Like at least I may ask you this, I don't know, I don't know of any scientific discovery that was made. And then people, like, I, I, I know, I know there areas of science that are not politically correct to talk about today, but every scientist knows the truth, right? Like the, the truth is still the truth. I mean, even the geneticists in the Soviet Union who were forced to buy into lassen egoism, like knew the whole time that it was wrong. Like I, that that I'm completely convinced of.

0 (2h 44m 18s):
Yeah. They couldn't delude themselves. Especially because the basic training that one gets in any field established some core truths upon which even the crazy ideas have to rest. And if they don't, as you pointed out, things fall to pieces, I would say that even the technologies that did not pan out and in some cases were disastrous, but that were great ideas at the beginning are starting to pan out. So the example I'll give is that most people are aware of the Elizabeth Holmes Theranos debacle to put it lightly, you know, analyzing what's in a single drop of blood as a way to analyze Hormones and diseases and antibodies, et cetera. I mean, that's a great idea.

0 (2h 44m 59s):
I mean, it's a terrific idea as opposed to having a phlebotomist come to your house, or you have to go in and you get tapped with a, you know, and the pulling vials and the whole thing. There's now a company born out of Stanford that is doing exactly what she sought to do, except that at least the courts ruled that she fudged the thing and is, that's why she's in jail right now. But the idea of getting a wide array of markers from a single drop of blood is an absolutely spectacular idea. The biggest challenge that company is going to confront is the idea that it's just the next Theranos. But if they've got the thing Yeah. And they're not fudging it Yeah. As it apparently Theranos was. I think everything will work out ala Victor, Hugo.

2 (2h 45m 43s):
Yeah, exactly. Yeah. Yeah. 'cause who wants to go back? Like when, when, if they, if they, if they, if if they get to the work, if it's real, it's gonna be like, this is the thing, the the opponents. The opponents, they're, they're not bringing their own ideas. Like they're not bringing their Oh my ideas better than yours. Like that's not what's happening. They're bringing the silence or counterargument. Right. Or name calling. Right.

0 (2h 46m 5s):
Well, this is why I think people who need to be loved probably stand a reduced chance of success. Oh yeah. Yeah. And maybe that's also why having people close to you that do love you and allowing that to be sufficient right. Can be very beneficial. This gets back to the idea of partnership and family right. Around Innovators. Because if you feel filled up by those people local to you Yep. You know, in your home Yep. Then you don't need people on the internet saying nice things about you or your ideas because you're good. Yeah. And you can forge forward. Yeah. Another question about innovation is the teams that you assemble around you, and you've talked before about the sort of small squadron model, you know, sort of David and Goliath examples as well, where, you know, a small group of individuals can create a technology that frankly outdoes what a, you know, a giant like Facebook might be doing or what any other large company might be doing.

0 (2h 47m 1s):
There are a lot of theories as to why that would happen, but I know you have some unique theories. Why do you think small groups can defeat large organizations?

2 (2h 47m 11s):
So the conventional explanation is, I think correct. And it's just that large organizations have a lot of advantages, but they just have a very hard time actually executing anything because of all the overhead. So large organizations have combinatorial communication overhead, right? The, the number of people who have to be consulted, who have to agree on things, gets to be staggering. The amount of time it takes to schedule the meeting gets to be staggering. You know, you get these really big companies and they have some issue, they're dealing with And, it takes like a month to schedule the pre-meeting, to like plan for the meeting, which is gonna happen two months later, which is then gonna result in a post meeting, which will then result in a board presentation, which will then result in a planning offsite. Right? So they're like, I thought,

0 (2h 47m 49s):
I thought academia was bad, but what you're describing is giving me hives. Kaf

2 (2h 47m 52s):
Kafka was, Kafka was a documentary. Yeah. Like the, this is, this is, yeah. So it, it's just like these, these are, I mean, look, you'd have these organizations, a hundred thousand people or more, like you're in, you're more of a nation state than a co than a than a than a company. And you've got all these competing internal, you know, it's the bedwin thing. I was saying before, you've got all these internal, like at most big companies, your internal enemies are like way more dangerous to you than anybody on the outside. Hmm.

0 (2h 48m 13s):
Can you elaborate on that? Oh, yeah, yeah. Your,

2 (2h 48m 14s):
Your big competition, the big competi at a big company, the big competition is for the, for the next promotion. Right. And, and, and the enemy for the next promotion is the next executive over in your company. Like that's your enemy. The other, the competitor on the outside is like an abstraction. Like maybe they'll matter someday, whatever. I gotta beat that guy inside my own company. Right? And so the, the internal warfare is at least as intense as the external warfare. And so, yeah. So it's just, I mean this is just all the, you know, iron law of all these big, big bureaucracies and how they function. So if, if a big bureaucracy ever does anything productive, I think it's like a miracle. Like it's like a miracle to the point where there should be like a celebration. There should be parties, there should be like ticker tape parades for like big large organizations that actually do things like that. That's great. 'cause it's like, so, it's so rare.

2 (2h 48m 55s):
It doesn't happen very often. So anyway, so that's the conventional explanation. Whereas look, small co small companies, small teams, you know, there's a lot that they can't do. 'cause they can't, you know, they're not operating at scale and they don't have global coverage and, you know, all these kind of, you know, they don't have, have the resources and so forth, but at least they can move quickly, right? They can organize fast, they can have a, you know, if there's an issue today, they can have a meeting today, they can solve the issue today, right. And everybody, they need to solve the issue is in the room today, right. So they can just move a lot faster. I think that's part of it. But I think there's another deeper thing underneath that, that people really don't like to talk about. It takes us back full circle to where we started, which is just the sheer number of people in the world who are capable of doing new things is just a very small set of people. And so you're not gonna have a hundred of them in a company or a thousand or 10,000, you're gonna have three, eight or 10 maybe.

2 (2h 49m 43s):
And

0 (2h 49m 43s):
Some of them are flying too close to

2 (2h 49m 44s):
The sun. Some of them are blowing themselves up. Right? Right. Some of them are. Right. So I B m I actually first learned this at I, so my first actual job job was at I B M when it was, and when, when I b m was still on top of the world right before it caved down in the early nineties. And so when I was there, it was 440,000 employees, which, and again, if you inflation adjust like today for that same size of business, inflation adjusted, market size adjusted, it would be it's equivalent today of like a two or 3 million person organization. It was like a, it was a nation state. There were 6,000 people in my division, you know, and we were next door to another building that had another 6,000 people in another division. So you just, you could work there for years and never meet anybody who didn't work for I B M. The first half of every meeting was just I IBMers introducing themselves to each other. Like, it, it was just mind boggling and the, the level of, of complexity.

2 (2h 50m 25s):
But they were so powerful that they had at, at, at four years before I got there in 1985, they were 80% of the market capitalization of the entire tech industry. Right. So, so they were at a level of dominance that even, you know, Google or Apple today is not even close to right. At the time. So that, that's how powerful they were. And so they had a system And, it worked really well for like 50 years. They had a system which was, if most of the employees in the company were expected to basically rigid follow rules. So they dressed the same, they acted the same, they did everything out of the playbook. You know, they, they were trained very specifically, but they had this category of people they called Wild Ducks. And this was an idea that the founder Thomas Watson's come up with Wild Ducks. And the Wild Ducks were, they often had the formal title of an I B M fellow.

2 (2h 51m 7s):
And they were the people who could make new things. And there were eight of them. And they got to break all the rules and they got to invent new products. They got to go off and work on something new. They didn't have to report back. They got to pull people off of other projects to work with them. They got, you know, budget when they needed it. They, they reported directly to the c e o. They got whatever they needed, he supported them in doing it. And they were glass breakers and, you know, and they showed the, the one in Austin at the time was this guy Andy Heller. and he would show up and, you know, jeans and cowboy boots and, you know, amongst an ocean of men in, you know, blue suits, white shirts, red ties, and put his cowboy boots up on the table. And. it was fine for Andy Hall to do that. And, it was not fine for you to do that. Right. And so they very specifically identified, we, we have an, we have an, we have like a, an like, almost like an aristocratic class within our, our company that gets to play by different rules.

2 (2h 51m 55s):
Now the expectation is they deliver, right? They, they, their job is to invent the next breakthrough product. But we, I B m management know that the 6,000 person division's not gonna invent the next product. We know, know it's gonna be crazy. Andy Heller and his, and his cowboy boots. And so I, I was always like very impressed. Like, and again, like ultimately I B m had its issues, but like that model worked for 50 years, right? Like worked incredibly well. And I, I think that's basically the model that works. And so it is, but it's a paradox, right? Which is like, how do you have a large bureaucratic, regimented organization, whether it's academia or government or business or anything that has all these rule followers in it and all these people who are jealous of their status and don't want things to change, but then still have that spark of, of, of creativity.

2 (2h 52m 37s):
I would say mostly it's impossible. Mostly it just doesn't happen. Those people get driven out, right? And, and in tech what happens is those people get driven out. 'cause we will fund them. These are the people we fund, right?

0 (2h 52m 48s):
I was gonna say, I, I these are gather that you are in the business of finding and funding the Wild Ducks.

2 (2h 52m 53s):
The Wild Ducks. That's exactly right. And, and actually this is actually the close, the close the loop. This is actually I think the simplest explanation for why I b m ultimately caved in. And then HP sort of in the eighties also ca you know, these I B m HP kind of were monolith. There were these incredible monolithic, incredible companies for 40 or 50 years. And then they kind of both caved in, in the, in the eighties and nineties. And I, and I actually think it was the emergence of venture capital, it was the emergence of a parallel funding system where the Wild Ducks, or in HP's case, their, their superstar technical people could actually leave and start their own companies. And, and again, it goes back to the university discussion we're having is like, this is what doesn't exist at the university level. This certainly does not exist at the government level.

0 (2h 53m 28s):
And until recently in media, it didn't exist until there's this thing that we call Podcasts. Exactly

2 (2h 53m 33s):
Right, exactly right.

0 (2h 53m 34s):
Which clearly have picked up some, some Momentus and I, I would hope that these other Wild duck models will, will move quickly. Yeah.

2 (2h 53m 43s):
But the one thing you know, right, and you, you know this like the one thing you know is the people on the other side are gonna be mad as hell.

0 (2h 53m 47s):
Yeah. They're going to, well I think they're past denial. The counter arguments continue. Yeah. The name calling is prolific name

2 (2h 53m 55s):
Calling is fully underway. Yeah.

0 (2h 53m 57s):
Yes. Well, mark, we've covered a lot of topics, but as with every time I talk to you, I learn, oh, so very much so. I'm so grateful for you taking the time outta your schedule to talk about all of these topics in depth with us. You know, I'd be remiss if I didn't say that. It is clear to me now that you are hyper realistic about the landscape, but you are also intensely optimistic about the existence of Wild Ducks. Yes. And those around them that support them and that are necessary for the implementation of their ideas at some point. Yeah. And that also you have a real rebel inside you. So that is Oso welcome on this podcast and it's Oso needed in these times and every time.

0 (2h 54m 42s):
So on behalf of myself and the rest of us here at the podcast and especially the listeners, thank you so much. Thanks

2 (2h 54m 49s):
For having me.

0 (2h 54m 50s):
Thank you for joining me for today's discussion with Marc Andreessen. If you're learning from and or enjoying this podcast, please subscribe to our YouTube channel. That's a terrific zero-cost way to support us. In addition, please subscribe to the podcast on both Spotify and Apple and on both Spotify and Apple. You can leave us up to a five star review. If you have questions for me or comments about the podcast or guests that you'd like me to consider hosting on the Huberman Lab podcast, please put those in the comment section on YouTube. I do read all the comments.

2 (2h 55m 18s):
Please also check out the Sponsors mentioned at the beginning and throughout today's episode. That's the best way to support this podcast.

Momentous (2h 55m 25s):
Not on today's podcast, but on many previous episodes of the Huberman Lab podcast, we discuss supplements. While supplements aren't necessary for everybody, many people derive tremendous benefit from them for things like improving sleep hormone support and focus. The Huberman Lab podcast has partnered with Momentous supplements. If you'd like to access the supplements discussed on the Huberman Lab podcast, you can go to live Momentous spelled o u s, so it's live Momentous dot com slash Huberman and you can also receive 20% off. Again, that's live Momentous spelled O u s.com/ Huberman.

2 (2h 55m 56s):
If you haven't to already subscribed to our Neural Network Newsletter, our Neural Network Newsletter is a completely zero cost monthly Newsletter. That includes summaries of podcast episodes as well as protocols that is short PDFs describing, for instance, tools to improve sleep tools to improve Neuroplasticity.

0 (2h 56m 13s):
We talk about deliberate cold exposure, fitness, various aspects of mental health. Again, all completely zero cost. And to sign up, you simply go to Huberman Lab dot com, go over to the menu in the corner, scroll down to Newsletter and provide your email. We do not share your email with anybody if you're not already following me on Social Media. I am Huberman Lab on all platforms. So that's Instagram, Twitter threads, LinkedIn and Facebook. And at all of those places, I talk about science and science related tools, some of which overlaps with the content of the Huberman Lab podcast, but much of which is distinct from the content of the Huberman Lab podcast. Again, it's Huberman Lab on all Social Media platforms. Thank you once again for joining me for today's discussion with Marc Andreessen.

0 (2h 56m 53s):
And last but certainly not least, thank you for your interest in science.