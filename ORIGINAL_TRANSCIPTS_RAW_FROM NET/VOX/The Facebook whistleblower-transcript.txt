Stripe (1s):
Support for the shortage. It comes from Stripe. Stripe is a technology company that helps businesses place their economic infrastructure. Online partnering with Stripe allows companies to focus on other projects. As the tech company handles more than 250 million API requests per day, Stripe wants to support you in building the business you desire. So visit stripe.com to learn more. As you assume, that's S T R I P e.com to get started today.

Hbo Max (36s):
The new HBO max original documentary series, the way down God greed, and the cult of Gwen Shamblin explores the legacy of the remnant churches, infamous leader. After rising to fame with her way down workshop Christian based diet program, Gwen Shamblin, Laura founded the remnant fellowship church to further her message, despite a carefully curated image, Laura and the church soon fielded accusations of abuse and exploitation for their alleged cult-like practices. The way down God greed and the cult of Glen Shamblin is now streaming on HBO. Max,

0 (1m 19s):
You might've noticed your Facebook went out on Monday and your messenger and your Instagram and your WhatsApp. Everything at the company went pretty haywire for five-ish hours, which meant almost 3 billion people around the world. Weren't able to rely on their social media. Mark Zuckerberg lost something like $6 billion in personal wealth yesterday. And that might not even be the worst thing that happens to Facebook this week.

3 (1m 49s):
Good afternoon, chairman Blumenthal, ranking member, Blackburn, and members of the subcommittee. Thank you for the competent

0 (1m 56s):
Facebook whistleblower named Francis. How Gunn testified on Capitol hill.

3 (2m 2s):
I joined Facebook because I think Facebook has the potential to bring out the best in us, but I'm here today because I believe Facebook's products, harm children, stoke division, and weak in our democracy. The company's leadership knows how to make Facebook and Instagram saver, but won't make the necessary changes because they have put their astronomical profits before people.

0 (2m 26s):
How can recently leaked thousands of pages of research from inside the company to the wall street journal?

4 (2m 33s):
Look, there's been a lot of people wondering, is this product good for us on any number of different levels, right? From mental health to safety, to political discs

0 (2m 42s):
Specifically, she leaked them to this reporter, Jeff Horwitz.

4 (2m 46s):
I think the thing that this body of work shows is how Facebook understands itself and how Facebook understands its own platform has gone off the rails in ways that it hasn't disclosed to the public.

0 (3m 1s):
We asked Jeff to walk us through the thousands of pages of internal Facebook files, because there's just a ton of stuff we started with Instagram and what Facebook's researchers discovered when they started asking users about how it made them feel.

4 (3m 17s):
And the operational assumption inside Facebook is that this stuff really isn't good for teenagers who are in a vulnerable place in particular girls.

0 (3m 26s):
What did the research say?

4 (3m 28s):
So the research says that for most users, people who are in a good place, Instagram's fine. You know, there's some negative social comparison for most people, you know, and that's just, you look at other people and you see that they are more attracted than, than you have better jokes than you, you know, go on better vacations than you, but that's okay. You know, it's about a life. We all live with it, but for a minority of users and not necessarily even a small minority of users, Instagram can really make some problems worse in particular body image with something the research focused on. And so they found that for people who were in a vulnerable place, this could be bad. The headline kind of quote from the article probably was we make body image issues worse than one in three teenage girls, one in three.

4 (4m 12s):
And now that's that's for people who already have body image issues, but it's a pretty high number. If that makes sense.

0 (4m 18s):
Did they do any research on where bad body image issues amongst teenage girls leads?

4 (4m 24s):
They have found that for some users. And again, this is among people who had thoughts of hurting themselves. They trace that directly back to the platform. So in the U S it was, I think 6% of users who thought about killing themselves in the last month, trace that idea to Instagram directly

0 (4m 44s):
And do people who have this really negative relationship with Instagram, use it less. Once they establish that this is bad for me,

4 (4m 52s):
This is a problem that the platform has, which is that the people who seem to be most vulnerable and most effected by these negative effects are also the ones who have the least self control in terms of their usage of the product. It makes some sense to me intuitively, right? Like for me, it's, I'm a reporter, so it's Twitter, but you know, it's not uncommon sort of just to be in a bit of a funk and just keep scrolling. And what Facebook found is that particularly content in certain categories like beauty and fitness and fashion could end up basically triggering these negative spirals, which is

0 (5m 28s):
To say what people end up using it more. Yeah.

4 (5m 31s):
They basically don't take a break at the point when really it would be good for them to take a break and they keep coming back to the platform, even if they are aware that it's not good for them. So

0 (5m 42s):
From dark to even darker, tell me what you discovered about human trafficking. Yeah.

4 (5m 48s):
It turns out that that Facebook spending on safety issues is just heavily concentrated on English and European languages. Outside of that, it's a very steep drop-off and just expenditures are very little. And so one of the things we found was that Facebook was for a long time, pretty aware that had a human trafficking problem on its platform. This was mostly people selling themselves or being sold into indentured servitude, usually in Gulf states. So they were be buyers and sellers groups for jobs. And, you know, sometimes those jobs would be rail and sometimes they would involve sex trafficking, for example, you know, coming from the Philippines or from Africa.

4 (6m 32s):
And they basically would be giving away their passports and their rights, and they could be resoled without their consent. It's a pretty rough system. Facebook knew this was happening and hadn't really done anything about it. They had a very small team focused on this sort of thing, and nothing really happened until late 2019. When the BBC wrote a story about human trafficking on Facebook

5 (6m 59s):
In Saudi Arabia, we found hundreds of women being sold on Haraj another popular commodity app. And on Facebook and Instagram, we found hundreds more

4 (7m 8s):
And apple took notice of it, basically told Facebook that if they didn't get the problem under control, immediately, apple might remove Instagram and Facebook from the app store And, you know, knowledge that people were being sold on the platform. Hadn't been enough to get the company to act, but you know, the threat of getting removed from the app store, it damn sure was. So Facebook just sort of convulsively acted at that point. They took down a ton of content related to human trafficking groups, pages, posts all that, but they didn't really fix it. They kind of took care of the immediate problem. And then things went back to normal.

4 (7m 49s):
And, you know, one of my colleagues actually ended up speaking to a woman who was trafficked just within the last year.

6 (7m 56s):
I think there should be some sort of verification on every job adverts on Facebook

0 (8m 6s):
And maybe the human trafficking thing sounds shocking if you're like here in north America somewhere, but we've heard stories about how Facebook is being used abroad for like, even like genocide in Myanmar, right?

7 (8m 19s):
Facebook, incited violence against Rohingya. We are not saying this. This was the assessment of United nations investigators to

4 (8m 28s):
It turns out that this is actually kind of a pretty standard weakness. The company has just not invested in many of the safety tools, right? Algorithmic screening tools that look for bad or inciting content or things of that nature. They literally do not exist in many languages. I mean, so for example, in Arabic, they literally don't have people who can moderate content in most dialects of Arabic. And, you know, you can tell that, you know, from, let's say the Israeli Palestinian violence earlier this year,

8 (9m 1s):
Palestinian activists are accusing Facebook of censorship and targeting the social media giant with one star reviews and the apple and Google app.

4 (9m 10s):
They were just making errors right. And left. I mean, they're their own employees. We could see this in some of the internal documents, their own employees had to step in and just say like, guys, you're, you're string over some of the largest news outlets in the middle east right now. And you're calling the name of one of the holiest sites in Islam, a terrorist organization. Like you got to fix this immediately. And so it's kind of this like crazy thing where things are always on fire and they haven't really ever invested in what we even think of as the sort of baseline level of safety efforts that we expect from Facebook in developed markets.

0 (9m 45s):
You're talking about a lack of resources in one of the wealthiest companies in the world. Why aren't they investing in like core safety teams in some of these countries, if they're offering the platforms there?

4 (9m 58s):
I think a question of where pressure comes from something that Francis noted to me is that every additional market Facebook enters and every sort of new language, it services is almost definitionally going to be smaller and poorer than the last country or, you know, last market. And so it just simply is a question of priorities and Facebook has traditionally responded to bad press and government attention from powerful governments. And there just simply isn't the force required to even first of all, detect what's going on in the platform and to complain about it in those other markets.

4 (10m 38s):
So

0 (10m 39s):
It's in their financial interests to offer the platforms in these poor countries, but it's not in their financial interests to moderate or regulate the platforms in those country.

4 (10m 50s):
Well, Facebook's not making meaningful revenue in me and Mar, but they really do not like the idea that there could be sort of other social media players serving the same markets. And they kind of have tried to be everywhere and do everything all at once. And so I think it's like less straight money than it is just like the presence is something that the company really, really is invested in. Let's say,

0 (11m 12s):
Talk about the algorithm. The algorithm seems to sort of be the roots of everything Facebook does, and Francis is out there saying it makes people angry.

3 (11m 22s):
The result has been more division, more harm, more lies, more threats, and more combat. In some cases, this, this dangerous online talk has led to actual violence that harms and even kills people.

4 (11m 37s):
So Facebook has made a lot of tweaks to its algorithms over the years. It's like literally testing out dozens of them at any given time, but a really big one came in 2018 was called meaningful social interaction. And when we ask people

9 (11m 52s):
What they want out of the, the number one answer, we get across different countries. And in different ways we ask if they want to connect with friends and family specifically, they want to keep in touch with people who live far away.

4 (12m 3s):
The idea was that Facebook was going to prioritize content that came from friends and family, either reshared or original posts, and that it was going to really kind of try to avoid passive Sperling and the way it was going to do that was by prioritizing content that made people engage, right? So like, like reshare emoji or comment,

9 (12m 29s):
Which is why this year we're really focused on what we're talking about as meaningful social interactions. We're trying to make sure that the time spent on the platform is time. People say is well spent.

4 (12m 41s):
And so what Facebook's researchers realized is that this algorithm change they did in, in 2018, it turned out to really favor things that were angry and things that were incendiary and things that broke Facebook's rules. And so basically Facebook ended up turning up the heat on political discourse worldwide

10 (13m 3s):
At dynamic lead to a complaint to Facebook by major political parties across Europe, this 2019 internal report obtained by Houchin says that the parties feel strongly, that the change to the algorithm has forced them to skew negative in their communications on Facebook, leading them into more extreme policy positions.

0 (13m 31s):
So Facebook's sort of trying to balance this idea that, you know, we want to be like a shiny, happy place with, we want to keep you on these platforms as much as humanly possible. And the thing that actually keeps you there, it turns out is having emotional reactions, even though a lot of them end up being negative.

4 (13m 49s):
Yeah. And I mean, I think the company came to realize that they were creating. I mean, they did come to realize, we could see this in the documents. They were creating perverse incentives for people to create angrier content and they didn't change it as you know, they kind of made some tweaks around the edges, but they weren't really willing to give up on the benefits for usage that heavy engagement based ranking allowed for.

0 (14m 16s):
It sounds like a lot of what you discovered, Jeff, and I don't mean this as any sort of like disparaging analysis of, of your work here. It sounds like a lot of what you're discovering is like what people may be suspected was true of how Facebook was running its business is now just being confirmed by these leaks and by your reporting. Is that fair?

4 (14m 39s):
Yeah. And I think it's, it's also look like, I don't think we knew that human trafficking was a problem on Facebook. We knew that teen mental health was something that a lot of people were worried about. You know, we knew that Facebook did tend to rile people up. I think what really matters about this first of all, is that Facebook is the only one who's ever able to be sure of these things or even get anywhere close to shore. Right? Otherwise it's just us debating social media in a bar. That's one of the big points here is that the company itself is the only one who can even fully consider much less answer these questions. And right now there, isn't a way in, and I think that's one of the things that Francis is really concerned about and that motivated her to talk to me originally and to eventually do what she did in terms of collecting documents.

4 (15m 31s):
She's hopeful that if everybody is aware of what Facebook is actually doing, that may be, they will demand sources of information about the company that don't require someone like her to take a lot of risk and go public

BetterHelp (16m 17s):
Support for the journey comes from better help. Here's how it works. Better help. Online therapy offers professional counseling that happens entirely online, sign up and let them know what's going on with you. And you'll be set up with one of their licensed professional therapists. From that point forward, they will be your point person. You can log into your account any time, send them a message, set up a weekly video or phone session with them from the comfort of your home or wherever. Maybe you have a walk. You like, I have a walk. I like, and if you ever decide, you'd like to switch to a different counselor. That's not a problem either today, explain listeners can get 10% off their first month of online therapy@betterhelp.com slash explained. You can join the over two plus million people who have taken charge of their mental health with the help of an experienced professional, according to better help, that is better aglp.com/explained, get matched with a better health therapist and get started today.

Hbo Max (17m 22s):
The new HBO max original documentary series, the way down God greed, and the cult of Gwen Shamblin explores the legacy of the remnant churches, infamous leader. After rising to fame with her way down workshop, Christian based diet program, Gwen Shamblin, Laura founded the remnant fellowship church to further her message, despite a carefully curated image, Laura and the church soon fielded accusations of abuse and exploitation for their alleged cult-like practices. The way God greed and the cult of Glen Shamblin is now streaming on HBO. Max,

0 (18m 1s):
Jeff you're reporting on the Facebook files, anonymized the whistleblower, but since she's made her identity known with a 60 minutes interview on Sunday and congressional testimony today, what's her story.

4 (18m 14s):
She's 37 grew up in Iowa daughter of a doctor and a college professor who eventually turned into an Episcopal priest. And she got her start at Google, I think came up doing product management and algorithm management.

3 (18m 31s):
I I've worked as a product manager at large tech companies since 2006, including Google Pinterest, Yelp, and Facebook. My job has largely focused on algorithmic products like Google plus search and recommendation systems. Like the one that powers the Facebook newsfeed.

4 (18m 47s):
She ended up getting really, really sick kind of derailed her career. She was at Google then and basically had to resign and she hired a family friend to help her with her recovery because she could barely walk at that point. You know, this young man was a really close friend and, you know, it was a really meaningful connection to her at a time when she was largely homebound. And then he got radicalized on the internet. Now it wasn't Facebook. It was like Reddit for Chan, but basically someone she knew and was one of her only points of social contact doing a really rough time, ended up going down kind of a white nationalist conspiracy rabbit hole.

4 (19m 39s):
And she tried to intervene. She failed and the guy basically left the bay area where she lived and kind of the friendship just disappeared for a number of years. He's actually since recovered and projected the really crazy beliefs, but that really left a mark. And so she had been ignoring Facebook job recruitment offers for years, but in 2018, on 2018, she decided that she was going to try to go join Facebook and do work that would actually help address this stuff. And I've seen her cover letter to the company. It's very clear. She wants the job because a friend's been radicalized and she thinks it's important to try to help people avoid that.

4 (20m 19s):
So she really came into the job with like a pretty personal motivation. And I think what happened is once she got inside, she kind of became convinced that Facebook either couldn't or wouldn't solve the problems. She had worked at a lot of companies. And I think one of the things that really surprised or was just kind of how bare bones a lot of Facebook's integrity operation was. So, I mean, she was given a team of brand new engineers and told to study and essentially fix the problem of narrowcast misinformation.

4 (21m 0s):
That's kind of something that like the Russians did in 2016, you target specific demographics, whether it's black lives matter or thin blue line people, and really try to kind of spread misinformation among particular communities, that's tailored to them. And you know, she, by her own acknowledgement did not manage to do the job at the time Facebook had given her and basically was told, look, we're Facebook. We do impossible things with minimal resources, you know, get used to it. And that was like supposed to be motivational, but in her mind, she just looked around and she came to believe that the company simply wasn't investing what it needed to invest in safety work and even beyond the investing issue was the question of where they actually following their own advice when their experts did come to conclusions about where problems were and how they could be fixed.

0 (21m 52s):
Do you know when she makes the decision to sort of give up, trying to fix these things internally and make what she knows

4 (22m 1s):
Public? I think our turning point came on December 2nd of last year when, after this kind of like intense and heroic effort by this kind of understaffed civic integrity team to try to prevent total disaster from happening in the U S election. And they did have a lot of successes. In addition to some major defeats Facebook decided like a few weeks after the election, it was going to disband the entire thing. And, you know, it was framed as kind of a thing that was in everyone's best interest and everyone's going to get jobs and get spread throughout the organization. But I think a lot of people on civic, including her kind of took this as a sign that Facebook truly just consider the civic team to be a thorn in his side, rather than an entity that could help guide it to a better place I'd gotten in touch with her weeks before then.

4 (22m 48s):
I only heard from her that evening for the first time.

0 (22m 51s):
Does Facebook dispute any of what she has shared with you or, or any of what she's saying now in public?

4 (22m 58s):
Not the legitimacy of it. I think Facebook has disputed whether the characterization is correct and they've taken issue with whether the wall street journal has over-hyped things, but I mean, the documents are the documents. And I think that's, you know, one of the things that is really powerful about this stuff is that when Facebook wants to argue about teen mental health, they're not arguing with the wall street journal, they're arguing with their own researchers.

0 (23m 22s):
And is Facebook currently arguing with its own researchers?

4 (23m 25s):
Oh yeah. There's been a lot of that. Facebook actually released a couple of the slide decks related to teen mental health that we wrote about. And they annotated them with just some like pretty harsh criticism of their own people. And, you know, whether the research was justified or the researchers kind of understood the objectives they were going for, or whether things to be considered causal. And like this is UX research. It's not supposed to be publishable in like science and, you know, be a pre-review process. So, I mean, you know, from my perspective, it looked like people doing what was basically solid, you know, rough and ready UX work, but you know, you have Facebook kind of both publicly and privately kind of suggesting that it's researchers caused a problem.

4 (24m 9s):
Hmm.

0 (24m 10s):
Is there anything Facebook's doing here actually illegal or is it simply ill-advised bad for society

4 (24m 21s):
Hurried? Is it some of these things are investor issue that Facebook hasn't been straightforward with the world in general and yes, that includes its investors. That's, you know, kind of the basis for filing a claim with the sec and seeking whistleblower protection status there, but per read is more with it, whether it's illegal or not, you know, maybe our laws aren't ready for it right now, but that's just kind of a sign that, you know, we got to get to work rather than that, you know, there's nothing wrong with it whatsoever. Like, I mean, it does make some sense, right? Like most of our laws from the internet predate social networks. So you wouldn't even expect that they could have gotten it right,

3 (24m 55s):
Because demonstrated they cannot act independently. Facebook over and over again has shown it uses profit over safety. It is subsidizing is paying for its profits with our safety. I'm hoping that this will have had a big enough impact on the world that they get the fortitude and the motivation to actually go put those regulations into place.

0 (25m 16s):
Is Congress equipped to deal with Facebook or is Facebook sort of unknowable to them and changing too quickly

4 (25m 24s):
For them? I think Francis like a lot of people inside Facebook and former employees is a little distraught with the state of the public debate outside the company, right. People are talking about, you know, is there bias political bias? You know, like in the sense of like, is mark Zuckerberg, squelching conservatives, or should section two 30, be repealed or should Facebook be broken up? And like, these are kind of her, her the wrong questions. They don't really get to the things that she's seen that she thinks are problematic, which is the issue of engagement based ranking and the issue of information access from outside the company.

4 (26m 6s):
So I think, you know, one of her goals and her reasons for going public, I think is that she did want to see if she could have some influence and perhaps have members of Congress and people in the press as well asking somewhat different questions. And they have been historically in terms of, you know, not, you know, should we ban Facebook or break it up or, you know, cause it to get sued into oblivion by repealing section two 30, but should we simplify it and force more information to be produced about it,

0 (26m 34s):
But an important distinction here is that she doesn't think Facebook can regulate itself and that someone needs to step in. Is that right? Yeah.

4 (26m 44s):
I mean, that's, that is look, I mean, this is someone who joined the company with the goal of being part of the solution and helping it fix itself. And she gave up on that idea and she gave up on it, candidly, I think pretty quickly and pretty thoroughly in the end. She does believe that not just outside oversight, but a smarter type of outside oversight is going to be necessary.

3 (27m 17s):
We now know the truth about Facebook's destructive impact. I came forward at great personal risk because I believe we still have time to act, but we must act. Now I'm asking you our elected representatives to act. Thank you.

0 (27m 53s):
Jeff Horwitz is the lead reporter on the Facebook files over at the wall street journal. There's a lot in there and we certainly didn't cover all of it. Read more@wsj.com or you can listen. The wall street journal has a daily news podcast. It's called the journal today's episode of this daily news podcast was produced by will read I'm Sean Rama's firm it's today explained

Hbo Max (29m 7s):
The new HBO max original documentary series, the way down God greed, and the cult of Gwen Shamblin explores the legacy of the remnant church's infamous leader after rising to fame with her way down workshop, Christian based diet program, Gwen Shamblin, Laura founded the remnant fellowship church to further her message, despite a carefully curated image, Laura and the church soon fielded accusations of abuse and exploitation for their alleged cult-like practices. The way down God greed and the cult of Glen Shamblin is now streaming on HBO max.