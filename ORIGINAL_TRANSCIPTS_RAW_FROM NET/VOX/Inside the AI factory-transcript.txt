1 (3s):
We are used to thinking of artificial intelligence as knowledge generated by machines. You can get chat g p t to write an email for you. I

2 (15s):
Hope this email finds you well, blah, blah, blah, blah, blah.

1 (19s):
You can ask mid journey what Pope Francis would look like in a puffer jacket. Can

3 (24s):
I say something without you guys getting mad?

1 (26s):
But it turns out there's a vast network of human labor powering ai. There are people training AI every day, sometimes all day, just clicking away on images on pixels so that the AI can get better at identifying things the way we humans do. We're going Inside the AI factory on today. Explain.

5 (1m 1s):
I am so sorry ma'am. I know you need this medicine, but it looks like it's not covered by your insurance.

6 (1m 7s):
Yeah, unfortunately I had to deny that one.

7 (1m 10s):
Wait, who are you? I'm

6 (1m 12s):
Your insurance company's pharmacy benefit manager. I get paid based on the price of a medicine and I don't make as much money off this one.

7 (1m 20s):
No one should stand between you and your medicine. Visit phrma.org/middleman to learn more paid for by Pharma

8 (1m 31s):
Barbie has been around for almost 65 years, but Barbie's aesthetic, you could argue it's been around even longer

9 (1m 38s):
When you kind of put up like a timeline of the history of fashion Barbie coy has always kind of been there. We just didn't call it that. Mm. And I think by the time you hit like clueless superstars like Paris Hilton and Nicole Richie, it, it became more of a a, a present

8 (1m 57s):
The enduring appeal of Barbie fashion. This week on Intuit Vultures pop culture podcast,

2 (2m 6s):
You are listening to today explain.

1 (2m 8s):
I'm Sean Ramas from, and I'm joined by Josh Jezza from The Verge who just wrote a big piece about the people behind artificial intelligence.

10 (2m 18s):
It is about the human labor behind artificial intelligence. you know, it's often said that AI learns from data, it finds patterns and data, but that data has to be curated, sorted, labeled, sometimes made by humans. So I wrote about those humans. It's often called data annotation, sometimes data labeling. The work is pretty weird. And there's a huge range in what you might be doing. Like let's say you log onto your platform and you might be labeling clothes and social media photos. You might be sorting TikTok videos based on whether they're fast paced or slow paced or, or something.

10 (3m 4s):
Or you might be like labeling food and saying like, yes, that's, that's Diet Coke. Or you might be looking at chatbot responses and saying, you know, this is incorrect or this is profane or, you know, too long or, or totally off the wall. So there's a huge range in the types of, of jobs you might be doing. What they have in common is they tend to be sort of small. Like there's one thing you're doing over and over and over and also have extremely high quality standards. Like let's say you are outlining vehicles or, or something like that. You have to outline it to the pixel.

1 (3m 44s):
Is this like the kind of thing that I do when I'm trying to log into a website and it's like how many of

2 (3m 48s):
These pictures have cars in them?

10 (3m 50s):
Exactly. It's a lot like capture. That was actually a, a method and still is a method of kind of getting this work for free. you know, by definition it's something AI can't do yet.

1 (4m 1s):
So when I do a capture, I'm helping the backend of some website trade ai.

10 (4m 7s):
Exactly. And, and if you may, you may have noticed over the years that capture's I've gotten harder. That's because the AI has gotten better. So you need blurry or weirder images ah, to sort of raise the bar and also to improve ai.

1 (4m 19s):
So in a way, you and I and all of us are AI annotators.

10 (4m 23s):
Yes. Yeah. And annotators are just people who do it, you know, full-time for pay.

1 (4m 31s):
Did you see people doing this kind of work?

10 (4m 34s):
So I did this kind of work.

1 (4m 36s):
You did it yourself?

10 (4m 37s):
Yes. I I, I did it myself as a way to meet people who are doing this kind of work. It's all online for the most part. So

1 (4m 44s):
Did you like apply for a job? Did you cheat on The Verge?

10 (4m 49s):
I made all of a dollar 50, I think, but yes, it was my second job for a couple months. But the application is very easy. You just have to speak English and you know, have an email address and you fill out some basic information and then, and then you get a, you know, a welcome email and you're invited into a Slack channel. And, and then you have to start training to actually work. You have to learn what data annotation is and then do kind of a training module for each task as sort of a video game.

11 (5m 17s):
So these courses are like instructions. So you have to read them carefully and understand each and every, each of each. And those instructions can come with scenarios and they can come with some questions or quizzes. A project has, let's say three or four courses. You have to start the first one, you finish it, go to the next one and so forth.

10 (5m 44s):
Now you can start working on this thing for money.

1 (5m 47s):
Gimme like a day. What did, like, what was your shift like?

10 (5m 50s):
So it was extremely difficult. you know, I thought I was gonna kind of log in and see what kind of jobs were out there and, you know, get invited to these channels and move past this fairly quickly. But I, I kept flunking the training for the first task I would try to do. Huh. I can give you an example. Like one of the early ones I was doing was just labeling clothing and the instructions were something like label the items of clothing that are real clothes that can be worn by real people or something like that. It was just like, like seemingly quite self-explanatory. So I just sort of clicked, proceed past the instructions and, and got started and failed immediately. Huh.

10 (6m 30s):
One of the, one of the things that tripped me up at first was like there was a magazine that had some photos of clothes in it. It was like, well that's not, you know, you can't wear a magazine but like to an ai, these systems are really literal, they're not very smart. And so it's all just pixels. It doesn't understand what a magazine is or what a reflection is. And so you need to label images of clothes and reflections of clothes and mirrors and things like that. And so that was sort of the first curve ball, but then it just goes on from there. It's like label costumes but not suits of armor. And where you draw that line is the difference between, you know, having a job and getting fired. These sorts of weird distinctions that get drawn. The full instructions were over 40 pages and you have to kind of keep referring back to those as you, you do your work,

1 (7m 19s):
You talk about failing, do you still get paid if you fail?

10 (7m 23s):
No. I mean you, you'll get paid for the tasks that you completed, but then you know, you just get booted out, says you're a low quality has, you know, gotten you suspended from this task and you have to go back and start training again on some new thing and try to qualify.

1 (7m 36s):
Wow. So it's really in your interest to read the instructions it sounds like.

10 (7m 41s):
Yeah. And workers, I found they end because the instructions are, you know, they're not well written. They're just in humanly complex and so they end up teaching each other, you know, doing a lot of free labor, honestly doing YouTube tutorials or, or or Google meets where they try to teach each other what these instructions actually mean. Is

1 (7m 59s):
It steady work Josh? Do you get as many tasks as you want? Is it like dependable income?

10 (8m 5s):
No. So this is one of the things that surprised me. I mean it's, it's obviously unsteady at the level of like if you don't read the instructions really carefully and you do something wrong, you're gonna get banned. And so that is very precarious but it's also just unsteady. Even if you're the best annotator in the world, there's like a really spiky demand for this sort of work. you know, there'll be a period where there's a bunch of well-paying tasks on there and you can work as much as you want and then they'll disappear and you don't know why and you have no work or you can only do tasks for a penny or something like that and then they'll come back. So I spoke to a lot of people and this was people were frustrated at the, at the low pay and even more than that, people were frustrated that it's like steady enough that you can almost depend on it, but not enough that you aren't constantly without work.

10 (8m 56s):
And so they would, you know, I talked to people who developed these habits of waking up every three hours in case something well paying appeared and then if there was staying up for 36 hours straight, just sort of labeling. Oh, talked to one guy who was just labeling elbows and knees, he didn't know why but it was paying well and you just wanted to do it while it lasted. 'cause then you might be out of work for a

1 (9m 15s):
Week elbows and knees.

10 (9m 17s):
Yeah, it was, that was, there's a lot of stuff on the, you just have no idea what it's for and and that was one of them where it was just like photos of crowds and it was like label all the elbows and knees. So

1 (9m 26s):
Okay, so you're just sitting there labeling elbows and knees for 36 hours straight for how much money?

10 (9m 34s):
It's, it's super variable. Each task pays some amount of money but the workers I talked to, they were getting paid for something like that. Like a couple bucks an hour as low as $1 an hour.

11 (9m 45s):
It cannot pay all the bills. It's a side hustle. Maybe just one bill, maybe it's for the internet bill and then that's it.

1 (9m 55s):
Wow. And do they have any idea why they are labeling elbows and knees for a dollar an hour potentially 36 hours straight?

10 (10m 7s):
No. Well they know, they know they're training AI and they know it's for some company but they don't know whose AI or what they're training it to do unless they can kind of guess that, you know, it's a self-driving car or something. But the elbows and knees know they don't know because there's just layers and layers of anonymity in the system. So like each project, like all they know about the platform is that it's called remo tasks and then each project is named something totally cryptic like pillbox bratwurst or, or something just like non-sequitur code names. And so they have no idea what it's for really

1 (10m 52s):
In a minute on today. Explained what and who all this labeling is for really?

5 (11m 17s):
I am so sorry ma'am. I know you need this medicine but it looks like it's not covered by your insurance.

6 (11m 23s):
Yeah, unfortunately I had to deny that one.

7 (11m 26s):
Wait, who are you?

6 (11m 27s):
I'm your insurance company's pharmacy benefit manager. I get paid based on the price of a medicine and I don't make as much money off this one.

7 (11m 36s):
No one should stand between you and your medicine. Visit pma.org/middleman to learn more paid for by pharma.

13 (11m 45s):
Can you describe what he-man looks like?

14 (11m 50s):
Okay, he-man muscle biking ish. White guy

13 (11m 56s):
Wiley Easton's son loved playing with his he-man toys As a little kid in the eighties,

14 (12m 2s):
I think I said something like, oh you could, you could be a superhero too. And he said No, I can't be a superhero because I'm not white.

13 (12m 13s):
And that's when Wiley Easton decided to make a black superhero toy for her son and other kids like him.

15 (12m 20s):
I just remember my mom running rushing to me showing me a little article about a figure that looked like he man, but he was black

13 (12m 30s):
Here. The story of Sunman and his fans on the latest episode of this is Love, listen wherever you get your podcasts,

2 (12m 42s):
Why did the elbow cross the road? Let me tell you, it's quite humorous

1 (12m 49s):
Today explained we are back with Josh Jezza Josh. You just told us that these people who are slaving away training AI 36 hours straight, a dollar an hour, whatever it is, they don't know exactly what they are doing it for. Do you know what they're doing it for?

10 (13m 5s):
So AI needs tons of examples to learn from. And so autonomous vehicles is a great example of something where this thing is out in the world steering around a multi ton piece of metal. The stakes are really high, you can't have it get confused. It's super dangerous. There was a case a couple years ago where an uber self-driving car killed a woman in Arizona could recognize pedestrians that could recognize bikes, but it struggled to figure out what was happening with a person walking a bike along a street not near a crosswalk. It like didn't have enough data on it. And so the demand for data for self-driving cars is super high.

10 (13m 46s):
If you think about how many times you're driving and you go past construction or just something you know unexpected happens, you need to have data on it. So there's thousands and thousands of people whose job it is to get data from these cars and and go through and say, you know, here's a pedestrian, here's a traffic cone, here's a pothole. That's basically how it works with any machine learning system. you know, whether it's language or image recognition. You need training data and you need someone to make sure it's the right training data and and to put tags on it and provide that human input. And

1 (14m 19s):
Where are these data annotators based typically?

10 (14m 24s):
They're all over the world. 'cause you need so much of this data, the pay tends to be fairly low. And so you have a lot of people in India, the Philippines, Kenya is a big hub, Venezuela 'cause you often get paid in US dollars. And so if there's a place where the currency is crashing and people can do the work and there's fast internet, the work tends to go there.

11 (14m 49s):
Since I'm in Kenya, Africa. So we get paid I think one to $2 an hour, which is pretty low. You can see it's just as it has to be because cannot cater for your basic needs whether it's a phone bill or the the rent. Yeah.

1 (15m 7s):
How long have we been outsourcing our data training?

10 (15m 12s):
It's been at least a decade. you know, probably more. One of the turning points happened in kind of the late two thousands. you know, you've always needed some form of data curation but before that it was often done by, you know, a researcher and their grad students or something. But with increasing computational power became possible to train on more data. And so in the late two thousands you have people start to use labeled data sets of millions of images instead of, you know, a couple thousands. We

16 (15m 43s):
Downloaded nearly a billion images. I used the crowdsourcing technology like Amazon Mechanical Turk platform to help us to label these images. At its

10 (15m 55s):
Peak, when you reach that scale, people start going overseas because you need people who will work for less workers

16 (16m 2s):
Together. Almost 50,000 workers from 167 countries around the world helped us to cling sort and label nearly a billion candidate images.

1 (16m 18s):
Will the need for these data annotators eventually dry up? Is this job sort of a finite experiment?

10 (16m 29s):
There are different views on that. There's certainly people in the AI industry who think, you know, we're gonna reach a breakthrough where the AI is gonna be so smart that it doesn't need human input anymore. It's gonna be become super intelligent. There's a lot of other people who disagree with that. And certainly historically what has happened is annotation is always kind of getting automated. Like if you look at those early image recognition systems, like that's automated. AI can tell the difference between a image of a cat and a dog. But it enables new technologies like self-driving cars. And now you need even more people doing even more and more complicated forms of meditation. And that has been the way it's gone.

10 (17m 9s):
And you know, you can certainly see a world where these language models are out in the world and they're all the things they're supposed to be doing like giving health advice or or legal advice are complex, changing high stakes fields and you're gonna need even more human annotation there.

1 (17m 25s):
So is this future of like, you know, perpetual human collaboration with AI gonna lead us to some ideal where the cars will drive themselves perfectly or the don don't know, the robot doctors will know my knee from my elbow.

10 (17m 41s):
I guess I should talk about sort of how brittle these systems are. That's the word that's used to describe their knowledge, the state of their knowledge when you're training something to be accurate. For example, you have people who are rating it for accuracy but one maybe they're not rating it correctly 'cause it's very time consuming and often impossible to fact check every written response. Often responses are open to interpretation or or just too complicated. And two, you don't know that it's learning the right patterns as opposed to learning to talk like whatever text people have labeled as as accurate sounds like. So one of the risks that I think we're seeing now is it's become, these language models particularly have become extremely good bullshitters.

10 (18m 29s):
Huh? Like you may have seen the case of the chat G P T lawyer who submitted some legal filings, citing cases that he asked chat G P T for

17 (18m 40s):
The lawyer cited more than half a dozen relevant court decisions to make his case for why the lawsuit had precedent. The only problem, none of those decisions were real. The program even reportedly told him Yes when he asked it to verify that the cases were legitimate.

1 (18m 57s):
Sounds like a trash lawyer though honestly, right? Yes,

10 (19m 0s):
I would, I would certainly not consult Chad g p t for legal advice. And the question is will will it ever get there? If you just throw enough annotation at it, enough data at it, is there gonna be a point where it learns what is true or false or what the legal reasoning or something like that? Or is it gonna continue to just sort of be a better and better mimic And you're always gonna have that possibility that it's gonna make some catastrophic error. That's an open question. And also as an open question of how you're gonna have people who can continue to oversee these models as they get so good at mimicking people. Yeah, right. Like you need a, a very good lawyer all of a sudden who can critique an AI model that is good at, you know, making up legal advice.

1 (19m 48s):
And what about the other side of this? Just like the treatment of workers. I mean you mentioned people working 36 hours straight. If, if Google might be behind the contract job that someone in Kenya has, that's paying them a dollar an hour to annotate elbows, are they cool with working people like that 36 hours straight for like a dollar an hour?

10 (20m 9s):
That is a question for Google, but I can say that some of their annotators in the us the people who are reading search results and YouTube results through the platform app and have been protesting their conditions saying that you know, they're underpaid, that they don't have health benefits.

18 (20m 28s):
Raiders are why Google search results are so good. They make sure that people like you and me get the information we need every single time and no one working for Google should be struggling to pay their rent.

10 (20m 41s):
Google's defense has been that they are paid fairly, but there tends to be in the industry not a lot of attention on this kind of work. Part of it I think stems from the sense that it won't be needed for long but you, you know, the AI will get good enough that you don't need annotators anymore. And so it's not really a job so much as just like some temporary work that you're calling on someone to do and what happens after that is not really your concern. And so I, I think there's a a sense where companies just sort of don't even really think of it as a labor issue that they're just kind of buying, buying a bunch of data that that may be changing.

10 (21m 21s):
I've seen just sort of in papers people say, you know, these annotators were paid the median wage wherever they're based or things like that. I think there is, you know, when attention is brought to this situation, there often is a push to do better but it's pretty uneven and there's just not a lot of transparency in the data pipeline. And so even if you wanna do better, it's hard.

1 (21m 46s):
You know what it sounds like Josh, it sounds like it might just be easier to pay people to do jobs. Did that occur to you at any point while you were clicking through whatever data that you were annotating?

10 (21m 58s):
That did occur to me many times while I was annotating. There's one where it was quite acute, where I was tracing pallets in like a warehouse for some kind of self-driving forklift. And just the amount of really kind of excruciatingly detailed labor that was going into figuring out how to drive a forklift around to automate, you know, one job, a forklift driver. It was pretty staggering. I mean there must've been hundreds if not thousands of people working on this thing around the world just tracing pixel by pixel, each pallet in each pallet hole and these dark warehouses. I guess that, that the hope of these companies is that once you've done all that work, you have this thing that can do it forever but don don't know that that's true.

10 (22m 43s):
'cause you know, the world keeps changing and throwing up new, new edge cases

1 (22m 48s):
And somewhere in this world that used to be a good union job. Right,

10 (22m 52s):
Exactly.

1 (22m 54s):
What were you hoping people would take away from your piece? What were you hoping people would learn by going inside this AI factory?

10 (23m 3s):
I think there are a couple different things and a couple different reasons why it's important to, to look at this work. I mean, the first is just kind of a, the labor issues that it raises. You have these potentially extremely profitable technologies that rely on often low paid and labor around the world that is often not discussed. And kinda the second thing that I, that I wasn't expecting to find but found is that the work, it's kind of structurally precarious in a way that a lot of, even for gig work, like gig work is notoriously precarious. But the way AI development works where you need a ton of data to train your model and then you need like a bit of more specific data to fill in some edge case and then nothing for a while.

10 (23m 46s):
And then a ton more data means that if this is gonna be a fixture in an AI economy, there's gonna be a lot of time people are not working and there's gonna be times when lots and lots of people need to work. And the way it's set up right now, the workers pay the cost of that. They're the ones who are unemployed whenever they're not needed and then they're expected to be kind of on demand when they are needed. I think also just a better understanding of the way these systems work. I think it's easy to, especially with something like chat G B T when it can tell you that it's an AI trained by open AI using reinforcement learning and, and all about itself that you know, acting in these very human-like ways that there's a tendency to think it can reason like a human, but it's important to think about the fact that a lot of that stuff was written there manually by humans and then reinforced by humans.

10 (24m 37s):
And there's a sense in which seeing the humans in the system kind of makes you realize how inhuman these machines are and that they have some pretty glaring weaknesses.

1 (24m 53s):
I don't trust 'em Josh.

10 (24m 55s):
I think that's, I think that's wise for the time being.

1 (25m 6s):
Josh Jezza does investigations at The Verge. You can read his work at The Verge dot com. His piece that inspired our episode today was titled Inside the AI factory. And it also ran on the cover of a recent issue of New York Magazine. The show today was produced by Amanda Lewellyn. It was edited by Amina Ade and fact checked by Laura Bullard We were engineered by Patrick Boyd. I'm Sean Ramis for and this is today explained.

2 (25m 44s):
Goodbye.